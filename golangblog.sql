-- MySQL dump 10.13  Distrib 5.7.14, for osx10.11 (x86_64)
--
-- Host: localhost    Database: life_assistant
-- ------------------------------------------------------
-- Server version	5.7.14

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `SpiderDoc_golangblog`
--

DROP TABLE IF EXISTS `SpiderDoc_golangblog`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `SpiderDoc_golangblog` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `title` longtext NOT NULL,
  `author` varchar(80) NOT NULL,
  `article` longtext NOT NULL,
  `href` varchar(200) NOT NULL,
  `publish_time` date DEFAULT NULL,
  `create` datetime(6) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `SpiderDoc_golangblog_href_13f9b150_uniq` (`href`),
  KEY `SpiderDoc_golangblog_author_2a988f44` (`author`),
  KEY `SpiderDoc_golangblog_publish_time_2f703917` (`publish_time`)
) ENGINE=InnoDB AUTO_INCREMENT=581 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `SpiderDoc_golangblog`
--

LOCK TABLES `SpiderDoc_golangblog` WRITE;
/*!40000 ALTER TABLE `SpiderDoc_golangblog` DISABLE KEYS */;
INSERT INTO `SpiderDoc_golangblog` VALUES (468,'Introducing the Developer Experience Working Group','The Developer Experience Working Group','		Introducing the Developer Experience Working Group\n		10 April 2017\n		\n    Over the last several years, Go\'s audience has shifted from early\n    adopters to mainstream users. Today, our users come from a wide\n    variety of backgrounds, experiences, and expectations. The needs\n    of users are growing faster than the Go project can currently address\n    them. To streamline the experience for first-time Go users,\n    we\'ve created the Developer eXperience Working Group (DXWG).\n    For the next three months, this group will work together on delivering:\n    improvements to the Go installation experience\n    better guidelines to help new users\n    guides on tooling and developer environments (editors and IDEs)\n    running user studies to systematically analyze and measure friction points\n    improvements to the Go Tour and Go Playground\n    A secondary goal of the working group is to better understand how to\n    involve the Go community in charting Go’s future. We hope that working\n    groups – Go team members working alongside community members – will\n    help Go scale its leadership and address user needs.  We’ll learn\n    from this experience and iterate.\n    The initial members of the working group are:\n    Carmen Andoh, Chris Broadfoot, Francesc Campoy, Jaana Burcu Dogan,\n    Steve Francia, Jess Frazelle, Bill Kennedy, Katrina Owen, Natalie\n    Pistunovich, Mat Ryer, Dmitry Shuralyov.\n    We are looking for additional people to help with contributing\n    code, writing documentation, sharing feedback and experiences\n    (user stories), reviewing contributions, and more. If you are\n    interested in any of our current areas of focus, please join\n    us on the #devexp channel on the\n    Gophers Slack and subscribe to the\n    golang-devexp\n    mailing list.\n		\n			By The Developer Experience Working Group\n		\n	','https://blog.golang.org/developer-experience','2017-04-10','2017-05-07 13:49:58.546145'),(469,'Smaller Go 1.7 binaries','David Crawshaw','		Smaller Go 1.7 binaries\n		18 August 2016\n		\n  Introduction\n    Go was designed for writing servers.\n    That is how it is most widely used today, and as a result a lot of\n    work on the runtime and compiler is focused on issues that matter to\n    servers: latency, ease of deployment, precise garbage collection,\n    fast startup time, performance.\n    As Go gets used for a wider variety of programs, there are new issues that must be considered.\n    One of these is binary size.\n    It has been on the radar for a long time\n    (issue #6853 was filed over two\n    years ago), but the growing interest in using Go for\n    deploying binaries on smaller devices — such as the Raspberry Pi or\n    mobile devices — means it received some attention for the Go 1.7\n    release.\n  Work done in Go 1.7\n    Three significant changes in Go 1.7 affect binary size.\n    The first is the new SSA backend that was enabled for AMD64 in this release.\n    While the primary motivation for SSA was improved performance, the\n    better generated code is also smaller.\n    The SSA backend shrinks Go binaries by ~5%.\n    We expect larger gains for the more RISC-like architectures\n    like ARM and MIPS when those backends have been converted to SSA in Go 1.8.\n    The second change is method pruning.\n    Until 1.6, all methods on all used types were kept, even if some of\n    the methods were never called.\n    This is because they might be called through an interface, or called\n    dynamically using the reflect package.\n    Now the compiler discards any unexported methods that do not match an\n    interface.\n    Similarly the linker can discard other exported methods, those that are only\n    accessible through reflection, if the corresponding\n    reflection features\n    are not used anywhere in the program.\n    That change shrinks binaries by 5–20%.\n    The third change is a more compact format for run-time type\n    information used by the reflect package.\n    The encoding format was originally designed to make the decoder in\n    the runtime and reflect packages as simple as possible. By making this\n    code a bit harder to read we can compress the format without affecting\n    the run-time performance of Go programs.\n    The new format shrinks Go binaries by a further 5–15%.\n    Libraries built for Android and archives built for iOS shrink further\n    as the new format contains fewer pointers, each of which requires\n    dynamic relocations in position independent code.\n    In addition, there were many small improvements such as improved\n    interface data layout, better static data layout, and simplified\n    dependencies. For example, the HTTP client no longer links in the entire HTTP\n    server.\n    The full list of changes can be found in issue\n    #6853.\n  Results\n    Typical programs, ranging from tiny toys to large production programs,\n    are about 30% smaller when built with Go 1.7.\n    The canonical hello world program goes from 2.3MB to 1.6MB:\n  package main\nimport \"fmt\"\nfunc main() {\n    fmt.Println(\"Hello, World!\")\n}\n    When compiled without debugging information the statically\n    linked binary is now under a megabyte.\n    A large production program used for testing this cycle, jujud, went from 94MB\n    to 67MB.\n    Position-independent binaries are 50% smaller.\n    In a position-independent executable (PIE), a pointer in a read-only\n    data section requires a dynamic relocation.\n    Because the new format for type information replaces pointers by\n    section offsets, it saves 28 bytes per pointer.\n    Position-independent executables with debugging information removed\n    are particularly important to mobile developers, as this is the kind\n    of program shipped to phones.\n    Big downloads make for a poor user experience, so the reduction here\n    is good news.\n  Future Work\n    Several changes to the run-time type information were too late for the\n    Go 1.7 freeze, but will hopefully make it into 1.8, further shrinking\n    programs, especially position-independent ones.\n    These changes are all conservative, reducing binary size without increasing\n    build time, startup time, overall execution time, or memory usage.\n    We could take more radical steps to reduce binary size: the\n    upx tool for compressing executables\n    shrinks binaries by another 50% at the cost of increased startup time\n    and potentially increased memory use.\n    For extremely small systems (the kind that might live on a keychain)\n    we could build a version of Go without reflection, though it is\n    unclear whether such a restricted language would be sufficiently\n    useful.\n    For some algorithms in the runtime we could use slower but more\n    compact implementions when every kilobyte counts.\n    All of these call for more research in later development cycles.\n    To the many contributors who helped make Go 1.7 binaries smaller,\n    thank you!\n		\n			By David Crawshaw\n		\n	','https://blog.golang.org/go1.7-binary-size','2016-08-18','2017-05-07 13:49:58.574213'),(470,'Using Subtests and Sub-benchmarks','Marcel van Lohuizen','		Using Subtests and Sub-benchmarks\n		3 October 2016\n		\n  Introduction\n    In Go 1.7, the testing package introduces a Run method on the\n    T and\n    B types\n    that allows for the creation of subtests and sub-benchmarks.\n    The introduction of subtests and sub-benchmarks enables better handling of\n    failures, fine-grained control of which tests to run from the command line,\n    control of parallelism, and often results in simpler and more maintainable code.\n  Table-driven tests basics\n    Before digging into the details, let\'s first discuss a common\n    way of writing tests in Go.\n    A series of related checks can be implemented by looping over a slice of test\n    cases:\n  func TestTime(t *testing.T) {\n    testCases := []struct {\n        gmt  string\n        loc  string\n        want string\n    }{\n        {\"12:31\", \"Europe/Zuri\", \"13:31\"},     // incorrect location name\n        {\"12:31\", \"America/New_York\", \"7:31\"}, // should be 07:31\n        {\"08:08\", \"Australia/Sydney\", \"18:08\"},\n    }\n    for _, tc := range testCases {\n        loc, err := time.LoadLocation(tc.loc)\n        if err != nil {\n            t.Fatalf(\"could not load location %q\", tc.loc)\n        }\n        gmt, _ := time.Parse(\"15:04\", tc.gmt)\n        if got := gmt.In(loc).Format(\"15:04\"); got != tc.want {\n            t.Errorf(\"In(%s, %s) = %s; want %s\", tc.gmt, tc.loc, got, tc.want)\n        }\n    }\n}\n    This approach, commonly referred to as table-driven tests, reduces the amount\n    of repetitive code compared to repeating the same code for each test\n    and makes it straightforward to add more test cases.\n  Table-driven benchmarks\n    Before Go 1.7 it was not possible to use the same table-driven approach for\n    benchmarks.\n    A benchmark tests the performance of an entire function, so iterating over\n    benchmarks would just measure all of them as a single benchmark.\n    A common workaround was to define separate top-level benchmarks\n    that each call a common function with different parameters.\n    For instance, before 1.7 the strconv package\'s benchmarks for AppendFloat\n    looked something like this:\n  func benchmarkAppendFloat(b *testing.B, f float64, fmt byte, prec, bitSize int) {\n    dst := make([]byte, 30)\n    b.ResetTimer() // Overkill here, but for illustrative purposes.\n    for i := 0; i &lt; b.N; i++ {\n        AppendFloat(dst[:0], f, fmt, prec, bitSize)\n    }\n}\nfunc BenchmarkAppendFloatDecimal(b *testing.B) { benchmarkAppendFloat(b, 33909, \'g\', -1, 64) }\nfunc BenchmarkAppendFloat(b *testing.B)        { benchmarkAppendFloat(b, 339.7784, \'g\', -1, 64) }\nfunc BenchmarkAppendFloatExp(b *testing.B)     { benchmarkAppendFloat(b, -5.09e75, \'g\', -1, 64) }\nfunc BenchmarkAppendFloatNegExp(b *testing.B)  { benchmarkAppendFloat(b, -5.11e-95, \'g\', -1, 64) }\nfunc BenchmarkAppendFloatBig(b *testing.B)     { benchmarkAppendFloat(b, 123456789123456789123456789, \'g\', -1, 64) }\n...\n    Using the Run method available in Go 1.7, the same set of benchmarks is now\n    expressed as a single top-level benchmark:\n  func BenchmarkAppendFloat(b *testing.B) {\n    benchmarks := []struct{\n        name    string\n        float   float64\n        fmt     byte\n        prec    int\n        bitSize int\n    }{\n        {\"Decimal\", 33909, \'g\', -1, 64},\n        {\"Float\", 339.7784, \'g\', -1, 64},\n        {\"Exp\", -5.09e75, \'g\', -1, 64},\n        {\"NegExp\", -5.11e-95, \'g\', -1, 64},\n        {\"Big\", 123456789123456789123456789, \'g\', -1, 64},\n        ...\n    }\n    dst := make([]byte, 30)\n    for _, bm := range benchmarks {\n        b.Run(bm.name, func(b *testing.B) {\n            for i := 0; i &lt; b.N; i++ {\n                AppendFloat(dst[:0], bm.float, bm.fmt, bm.prec, bm.bitSize)\n            }\n        })\n    }\n}\n    Each invocation of the Run method creates a separate benchmark.\n    An enclosing benchmark function that calls a Run method is only run once and\n    is not measured.\n    The new code has more lines of code, but is more maintainable, more readable,\n    and consistent with the table-driven approach commonly used for testing.\n    Moreover, common setup code is now shared between runs while eliminating the\n    need to reset the timer.\n  Table-driven tests using subtests\n    Go 1.7 also introduces a Run method for creating subtests.\n    This test is a rewritten version of our earlier example using subtests:\n  func TestTime(t *testing.T) {\n    testCases := []struct {\n        gmt  string\n        loc  string\n        want string\n    }{\n        {\"12:31\", \"Europe/Zuri\", \"13:31\"},\n        {\"12:31\", \"America/New_York\", \"7:31\"},\n        {\"08:08\", \"Australia/Sydney\", \"18:08\"},\n    }\n    for _, tc := range testCases {\n        t.Run(fmt.Sprintf(\"%s in %s\", tc.gmt, tc.loc), func(t *testing.T) {\n            loc, err := time.LoadLocation(tc.loc)\n            if err != nil {\n                t.Fatal(\"could not load location\")\n            }\n            gmt, _ := time.Parse(\"15:04\", tc.gmt)\n            if got := gmt.In(loc).Format(\"15:04\"); got != tc.want {\n                t.Errorf(\"got %s; want %s\", got, tc.want)\n            }\n        })\n    }\n}\n    The first thing to note is the difference in output from the two implementations.\n    The original implementation prints:\n  --- FAIL: TestTime (0.00s)\n    time_test.go:62: could not load location \"Europe/Zuri\"\n    Even though there are two errors, execution of the test halts on the call to\n    Fatalf and the second test never runs.\n    The implementation using Run prints both:\n  --- FAIL: TestTime (0.00s)\n    --- FAIL: TestTime/12:31_in_Europe/Zuri (0.00s)\n        time_test.go:84: could not load location\n    --- FAIL: TestTime/12:31_in_America/New_York (0.00s)\n        time_test.go:88: got 07:31; want 7:31\n    Fatal and its siblings causes a subtest to be skipped but not its parent or\n    subsequent subtests.\n    Another thing to note is the shorter error messages in the new implementation.\n    Since the subtest name uniquely identifies the subtest there is no need to\n    identify the test again within the error messages.\n    There are several other benefits to using subtests or sub-benchmarks,\n    as clarified by the following sections.\n  Running specific tests or benchmarks\n    Both subtests and sub-benchmarks can be singled out on the command line using\n    the -run or -bench flag.\n    Both flags take a slash-separated list of regular expressions that match the\n    corresponding parts of the full name of the subtest or sub-benchmark.\n    The full name of a subtest or sub-benchmark is a slash-separated list of\n    its name and the names of all of its parents, starting with the top-level.\n    The name is the corresponding function name for top-level tests and benchmarks,\n    and the first argument to Run otherwise.\n    To avoid display and parsing issues, a name is sanitized by replacing spaces\n    with underscores and escaping non-printable characters.\n    The same sanitizing is applied to the regular expressions passed to\n    the -run or -bench flags.\n    A few examples:\n    Run tests that use a timezone in Europe:\n  $ go test -run=TestTime/\"in Europe\"\n--- FAIL: TestTime (0.00s)\n    --- FAIL: TestTime/12:31_in_Europe/Zuri (0.00s)\n        time_test.go:85: could not load location\n    Run only tests for times after noon:\n  $ go test -run=Time/12:[0-9] -v\n=== RUN   TestTime\n=== RUN   TestTime/12:31_in_Europe/Zuri\n=== RUN   TestTime/12:31_in_America/New_York\n--- FAIL: TestTime (0.00s)\n    --- FAIL: TestTime/12:31_in_Europe/Zuri (0.00s)\n        time_test.go:85: could not load location\n    --- FAIL: TestTime/12:31_in_America/New_York (0.00s)\n        time_test.go:89: got 07:31; want 7:31\n    Perhaps a bit surprising, using -run=TestTime/New_York won\'t match any tests.\n    This is because the slash present in the location names is treated as\n    a separator as well.\n    Instead use:\n  $ go test -run=Time//New_York\n--- FAIL: TestTime (0.00s)\n    --- FAIL: TestTime/12:31_in_America/New_York (0.00s)\n        time_test.go:88: got 07:31; want 7:31\n    Note the // in the string passed to -run.\n    The / in time zone name America/New_York is handled as if it were\n    a separator resulting from a subtest.\n    The first regular expression of the pattern (TestTime) matches the top-level\n    test.\n    The second regular expression (the empty string) matches anything, in this case\n    the time and the continent part of the location.\n    The third regular expression (New_York) matches the city part of the location.\n    Treating slashes in names as separators allows the user to refactor\n    hierarchies of tests without the need to change the naming.\n    It also simplifies the escaping rules.\n    The user should escape slashes in names, for instance by replacing them with\n    backslashes, if this poses a problem.\n    A unique sequence number is appended to test names that are not unique.\n    So one could just pass an empty string to Run\n    if there is no obvious naming scheme for subtests and the subtests\n    can easily be identified by their sequence number.\n  Setup and Tear-down\n    Subtests and sub-benchmarks can be used to manage common setup and tear-down code:\n  func TestFoo(t *testing.T) {\n    // &lt;setup code&gt;\n    t.Run(\"A=1\", func(t *testing.T) { ... })\n    t.Run(\"A=2\", func(t *testing.T) { ... })\n    t.Run(\"B=1\", func(t *testing.T) {\n        if !test(foo{B:1}) {\n            t.Fail()\n        }\n    })\n    // &lt;tear-down code&gt;\n}\n    The setup and tear-down code will run if any of the enclosed subtests are run\n    and will run at most once.\n    This applies even if any of the subtests calls Skip, Fail, or Fatal.\n  Control of Parallelism\n    Subtests allow fine-grained control over parallelism.\n    To understand how to use subtests in the way\n    it is important to understand the semantics of parallel tests.\n    Each test is associated with a test function.\n    A test is called a parallel test if its test function calls the Parallel\n    method on its instance of testing.T.\n    A parallel test never runs concurrently with a sequential test and its execution\n    is suspended until its calling test function, that of the parent test,\n    has returned.\n    The -parallel flag defines the maximum number of parallel tests that can run\n    in parallel.\n    A test blocks until its test function returns and all of its subtests\n    have completed.\n    This means that the parallel tests that are run by a sequential test will\n    complete before any other consecutive sequential test is run.\n    This behavior is identical for tests created by Run and top-level tests.\n    In fact, under the hood top-level tests are implemented as subtests of\n    a hidden master test.\n  Run a group of tests in parallel\n    The above semantics allows for running a group of tests in parallel with\n    each other but not with other parallel tests:\n  func TestGroupedParallel(t *testing.T) {\n    for _, tc := range testCases {\n        tc := tc // capture range variable\n        t.Run(tc.Name, func(t *testing.T) {\n            t.Parallel()\n            if got := foo(tc.in); got != tc.out {\n                t.Errorf(\"got %v; want %v\", got, tc.out)\n            }\n            ...\n        })\n    }\n}\n    The outer test will not complete until all parallel tests started by Run\n    have completed.\n    As a result, no other parallel tests can run in parallel to these parallel tests.\n    Note that we need to capture the range variable to ensure that tc gets bound to\n    the correct instance.\n  Cleaning up after a group of parallel tests\n    In the previous example we used the semantics to wait on a group of parallel\n    tests to complete before commencing other tests.\n    The same technique can be used to clean up after a group of parallel tests\n    that share common resources:\n  func TestTeardownParallel(t *testing.T) {\n    // &lt;setup code&gt;\n    // This Run will not return until its parallel subtests complete.\n    t.Run(\"group\", func(t *testing.T) {\n        t.Run(\"Test1\", parallelTest1)\n        t.Run(\"Test2\", parallelTest2)\n        t.Run(\"Test3\", parallelTest3)\n    })\n    // &lt;tear-down code&gt;\n}\n    The behavior of waiting on a group of parallel tests is identical to that\n    of the previous example.\n  Conclusion\n    Go 1.7\'s addition of subtests and sub-benchmarks allows you to write structured\n    tests and benchmarks in a natural way that blends nicely into the existing\n    tools.\n    One way to think about this is that earlier versions of the testing package had\n    a 1-level hierarchy: the package-level test was structured as a set of\n    individual tests and benchmarks.\n    Now that structure has been extended to those individual tests and benchmarks,\n    recursively.\n    In fact, in the implementation, the top-level tests and benchmarks are tracked\n    as if they were subtests and sub-benchmarks of an implicit master test and\n    benchmark: the treatment really is the same at all levels.\n    The ability for tests to define this structure enables fine-grained execution of\n    specific test cases, shared setup and teardown, and better control over test\n    parallelism.\n    We are excited to see what other uses people find. Enjoy.\n		\n			By Marcel van Lohuizen\n		\n	','https://blog.golang.org/subtests','2016-10-03','2017-05-07 13:49:58.591853'),(471,'Seven years of Go','The Go Team','		Seven years of Go\n		10 November 2016\n		\n    Today marks seven years since we open-sourced our preliminary sketch of Go.\n    With the help of the open source community, including more than a thousand\n    individual contributors to the Go source repositories,\n    Go has matured into a language used all over the world.\n    The most significant user-facing changes to Go over the past year are the\n    addition of built-in support for\n    HTTP/2 in\n    Go 1.6 and the integration of the\n    context package into the standard library in Go 1.7.\n    But we’ve been making many less visible improvements.\n    Go 1.7 changed the x86-64 compiler to use a new SSA-based back end,\n    improving the performance of most Go programs by 10–20%.\n    For Go 1.8, planned for release next February,\n    we have changed the compilers for the other architectures to use the new back end too.\n    We’ve also added new ports, to Android on 32-bit x86, Linux on 64-bit MIPS,\n    and Linux on IBM z Systems.\n    And we’ve developed new garbage-collection techniques that reduce typical\n    “stop the world” pauses to under 100 microseconds.\n    (Contrast that with Go 1.5’s big news of 10 milliseconds or less.)\n    This year kicked off with a global Go hackathon,\n    the Gopher Gala, in January.\n    Then there were Go conferences in India and Dubai in February,\n    China and Japan in April, San Francisco in May, Denver in July,\n    London in August, Paris last month, and Brazil this past weekend.\n    And GothamGo in New York is next week.\n    This year also saw more than 30 new Go user groups,\n    eight new Women Who Go chapters,\n    and four GoBridge workshops around the world.\n    We continue to be overwhelmed by and grateful for\n    the enthusiasm and support of the Go community.\n    Whether you participate by contributing changes, reporting bugs,\n    sharing your expertise in design discussions, writing blog posts or books,\n    running meetups, helping others learn or improve,\n    open sourcing Go packages you wrote, or just being part of the Go community,\n    the Go team thanks you for your help, your time, and your energy.\n    Go would not be the success it is today without you.\n    Thank you, and here’s to another year of fun and success with Go!\n		\n			By The Go Team\n		\n	','https://blog.golang.org/7years','2016-11-10','2017-05-07 13:49:58.618010'),(472,'Go fonts','Nigel Tao, Chuck Bigelow and Rob Pike','		Go fonts\n		16 November 2016\n		\n  An Announcement\n    The experimental user interface toolkit being built at\n    golang.org/x/exp/shiny\n    includes several text elements, but there is a problem with testing them:\n    What font should be used?\n    Answering this question led us to today\'s announcement,\n    the release of a family of high-quality WGL4 TrueType fonts, \n    created by the Bigelow &amp; Holmes type foundry specifically for the Go project.\n    The font family, called Go (naturally), includes proportional- and fixed-width faces in normal, bold, and italic renderings.\n    The fonts have been tested for technical uses, particularly programming.\n    Go source code looks particularly good when displayed in Go fonts, as its name implies, with things like\n    punctuation characters easily distinguishable and operators lined up and placed consistently:\n    Perhaps the most remarkable feature of the Go fonts is their license:\n    They are licensed under the same open source license as the rest of the Go project\'s software,\n    an unusually free arrangement for a high-quality font set.\n    Here are samples of the proportionally-spaced...\n    and monospaced fonts:\n  How to use them\n    If you just want the TTF files, run\n  git clone https://go.googlesource.com/image\n    and copy them from the subsequent image/font/gofont/ttfs directory.\n    If you want to use Go (the fonts) with Go (the software), each font is provided by a separate package.\n    To use the Go Regular font in a program, import golang.org/x/image/font/gofont/goregular, and write:\n  font, err := truetype.Parse(goregular.TTF)\n    The github.com/golang/freetype/truetype\n    package provides the truetype.Parse function today.\n    There is also work underway to add a TrueType package under golang.org/x\n    again licensed under the same open source license as the rest of the Go project\'s software.\n    We leave it to you to find some of the other unusual properties the fonts have,\n    but for an overview of the fonts\' design we asked Chuck Bigelow to provide some background.\n    The remainder of this blog post is his response.\n  Notes on the fonts, by Chuck Bigelow\n    The Go fonts are divided into two sets, Go proportional, which is\n    sans-serif, and Go Mono, which is slab-serif.\n  Go proportional fonts\n  Sans-serif\n    Go proportional fonts are sans-serif, like several popular fonts\n    for screen displays. There is some evidence that some sans-serif \n    faces at small sizes and low resolutions on screens are slightly \n    more legible than their seriffed counterparts, while at large sizes, \n    there is not a significant difference in legibility between sans and \n    seriffed faces, at least in the pair tested. [1] (The bracketed numbers\n    refer to the references listed at the end of this article.)\n  Style\n    Go sans-serif fonts are \"humanist\" rather than \"grotesque\" in\n    style. This is an historical distinction, not an aesthetic judgment. \n    Widely used sans-serif fonts like Helvetica and Arial are called \n    grotesque because an early 19th century sans-serif typeface \n    was named \"Grotesque,\" and the name became generic. \n    The shapes of modern grotesque fonts like Helvetica are sculpted, \n    with smooth, assimilated forms. \n    Humanist sans-serifs are derived from Humanist handwriting \n    and early fonts of the Italian Renaissance and still show subtle \n    traces of pen-written calligraphy. There is some evidence that\n    humanist fonts are more legible than grotesque fonts. [2] \n  Italics\n    Go proportional italics have the same width metrics as the roman \n    fonts. Go italics are oblique versions of the romans, with one\n    noticeable exception: the italic lowercase \'a\' is redesigned as a \n    cursive single-story form to harmonize with the bowl shapes of \n    the b d g p q set, in which the upright forms also adapt well to\n    slanting, The addition of cursive \'a\' makes the italics appear more \n    lively than a simply slanted roman. Some typographers believe that \n    slanted roman sans-serif italics are preferable to truly \"cursive\" sans\n    Italics, in part because of history and design. [3]\n  The x-height\n    The x-height of a typeface is the height of the lowercase \'x\' relative \n    to the body size. The x-height of Go fonts is 53.0% of body size, a\n    bit larger than the x-heights of Helvetica (52.3%) or Arial (51.9%),\n    but the difference is usually unnoticeable at normal reading sizes. \n    Typographers believe that larger x-heights contribute to greater \n    legibility in small sizes and on screens. A study of \"print size\"\n    (particularly x-height) and reading noted that types for reading on \n    screens and for small sizes tend to have large x-heights. [4]\n  DIN Legibility Standard\n    The recent German DIN 1450 legibility standard recommends\n    several features for font legibility, including differentiation of\n    letter shapes to reduce confusion. The Go fonts conform to the \n    1450 standard by carefully differentiating zero from capital O;\n    numeral 1 from capital I (eye) and lowercase l (ell); numeral 5 from \n    capital S; and numeral 8 from capital B. The shapes of bowls of\n    b d p q follow the natural asymmetries of legible Renaissance \n    handwriting, aiding differentiation to reduce confusion. [5]\n  Weights\n    The Go proportional fonts come in three weights: Normal, Medium, \n    and Bold. The Normal weight is strong enough that it maintains \n    clarity on backlit screens, which often tend to erode letter features \n    and thickness. The Medium weight has stem thickness 1.25 times \n    the Normal, for greater sturdiness on bright screens or for users\n    who prefer a sturdy font. The Bold weight has stem thickness \n    1.5 times the Normal, bold enough to be distinct from the normal\n    weight. These Go fonts have CSS numerical weights of 400, 500, \n    and 600. Although CSS specifies \"Bold\" as a 700 weight and 600 \n    as Semibold or Demibold, the Go numerical weights match the \n    actual progression of the ratios of stem thicknesses: \n    Normal:Medium = 400:500; Normal:Bold = 400:600. The Bold\n    weight name matches the use of “Bold” as the usual corresponding\n    bold weight of a normal font. More discussion of the relationship of\n    stem thicknesses, weight names, and CSS numbering is in [6].\n  WGL4 character set\n    The WGL4 character set, originally developed by Microsoft, is often\n    used as an informal standard character set. WGL4 includes Western \n    and Eastern European Latin characters plus Modern Greek and \n    Cyrillic, with additional symbols, signs, and graphical characters,\n    totalling more than 650 characters in all. The Go WGL4 fonts can \n    be used to compose a wide range of languages. [7]\n  Metric compatibility with Arial and Helvetica\n    The Go sans-serif fonts are nearly metrically compatible with \n    standard Helvetica or Arial characters. Texts set in Go occupy\n    nearly the same space as texts in Helvetica or Arial (at the same \n    size), but Go has a different look and texture because of its\n    humanist style. Some Go letters with DIN legibility features are \n    wider than corresponding letters in Helvetica or Arial, so some \n    texts set in Go may take slightly more space. \n  Go Mono fonts\n  Monospaced\n    Go Mono fonts are monospaced—each letter has the same width as\n    the other letters. Monospaced fonts have been used in programming \n    since the beginning of computing and are still widely used because the\n    typewriter regularity of their spacing makes text align in columns and \n    rows, a style also found in Greek inscriptions of the 5th century BC. \n    (The ancient Greeks didn\'t have typewriters or computer keyboards, \n    but they did have great mathematicians and a great sense of symmetry\n    and pattern that shaped their alphabet.)\n  Slab-serif\n    The Go Mono fonts have slab-shaped serifs, giving them a sturdy \n    appearance. \n  Style\n    The underlying letter shapes of Go Mono are, like the Go sans-serif fonts, \n    derived from humanist handwriting, but the monospacing and slab serifs \n    tend to obscure the historical and stylistic connections. \n  Italics\n    Go Mono Italics are oblique versions of the romans, with the exception\n    that the italic lowercase \'a\' is redesigned as a cursive single-story form \n    to harmonize with the bowl shapes of the b d g p q. The cursive \'a\' makes \n    the italics appear more lively than a simply slanted roman. As with many\n    sans-serif fonts, it is believed that slanted roman slab-serifs fonts may\n    be more legible than truly \"cursive\" italics.\n  The x-height\n    Go Mono fonts have the same x-height as Go sans-serif fonts, 53% of\n    the body size. Go Mono looks almost 18% bigger than Courier, which \n    has an x-height 45% of body size. Yet Go Mono has the same width\n    as Courier, so the bigger look is gained with no loss of economy in\n    characters per line. \n  DIN Legibility Standard\n    Go Mono fonts conform to the DIN 1450 standard by differentiating \n    zero from capital O; numeral 1 from capital I (eye) and lowercase l (ell); \n    numeral 5 from capital S; and numeral 8 from capital B. The shapes of \n    bowls of b d p q follow the natural asymmetries of legible Renaissance \n    handwriting, aiding differentiation and reducing confusion.  \n  Weights\n    Go Mono fonts have two weights: Normal and Bold. The normal weight \n    stem is the same as in Go Normal and thus maintains clarity on backlit \n    screens, which tend to erode letter features and stem thickness. The\n    bold stem thickness is 1.5 times thicker than the normal weight, hence\n    the Bold Mono has the same stem thickness as Bold Go proportional. \n    Because the letter width of monospaced bold is identical to the width of \n    monospaced normal, the bold Mono appears slightly bolder than the \n    proportional Go Bold, as more black pixels are put into the same area.)\n  Metric compatibility with popular monospaced fonts\n    Go Mono is metrically compatible with Courier and other monospaced\n    fonts that match the \"Pica\" typewriter type widths of 10 characters per \n    linear inch at 12 point. At 10 point, Go Mono fonts set 12 characters \n    per inch. The TrueType fonts are scalable, of course, so Go Mono can\n    be set at any size. \n  WGL4 character set\n    The Go Mono fonts offer the WGL4 character set often used as an\n    informal standard character set. WGL4 includes Western and Eastern \n    European Latin characters plus Modern Greek and Cyrillic, with \n    additional symbols, signs, and graphical characters. The 650+ characters \n    of the Go WGL4 sets can be used for a wide range of languages.\n  References\n    [1] Morris, R. A., Aquilante, K., Yager, D., &amp; Bigelow, C. (2002, May). P‐13: Serifs Slow RSVP Reading at Very Small Sizes, but Don\'t Matter at Larger Sizes. In SID Symposium Digest of Technical Papers (Vol. 33, No. 1, pp. 244-247). Blackwell Publishing Ltd.\n    [2] Bryan Reimer et al. (2014) “Assessing the impact of typeface design in a text-rich automotive user interface”, Ergonomics, 57:11, 1643-1658.\n    http://www.tandfonline.com/doi/abs/10.1080/00140139.2014.940000\n    [3] Adrian Frutiger - Typefaces: The Complete Works. H. Osterer and P. Stamm, editors. Birkhäuser, Basel, 2009, page 257.\n    [4] Legge, G. E., &amp; Bigelow, C. A. (2011). Does print size matter for reading? A review of findings from vision science and typography. Journal of Vision, 11(5), 8-8. http://jov.arvojournals.org/article.aspx?articleid=2191906\n    [5] Charles Bigelow. \"Oh, oh, zero!\" TUGboat, Volume 34 (2013), No. 2. \n    https://tug.org/TUGboat/tb34-2/tb107bigelow-zero.pdf\n    https://tug.org/TUGboat/tb34-2/tb107bigelow-wang.pdf\n    [6] \"Lucida Basic Font Weights\" Bigelow &amp; Holmes.\n    http://lucidafonts.com/pages/facts\n    [7] WGL4 language coverage: Afrikaans, Albanian, Asu, Basque,\n    Belarusian, Bemba, Bena, Bosnian, Bulgarian, Catalan, Chiga,\n    Colognian, Cornish, Croatian, Czech, Danish, Embu, English, Esperanto,\n    Estonian, Faroese, Filipino, Finnish, French, Friulian, Galician,\n    Ganda, German, Greek, Gusii, Hungarian, Icelandic, Inari Sami,\n    Indonesian, Irish, Italian, Jola-Fonyi, Kabuverdianu, Kalaallisut,\n    Kalenjin, Kamba, Kikuyu, Kinyarwanda, Latvian, Lithuanian, Lower\n    Sorbian, Luo, Luxembourgish, Luyia, Macedonian, Machame, Makhuwa-Meetto,\n    Makonde, Malagasy, Malay, Maltese, Manx, Meru, Morisyen, North\n    Ndebele, Northern Sami, Norwegian Bokmål, Norwegian Nynorsk, Nyankole,\n    Oromo, Polish, Portuguese, Romanian, Romansh, Rombo, Rundi, Russian,\n    Rwa, Samburu, Sango, Sangu, Scottish Gaelic, Sena, Serbian, Shambala,\n    Shona, Slovak, Slovenian, Soga, Somali, Spanish, Swahili, Swedish,\n    Swiss German, Taita, Teso, Turkish, Turkmen, Upper Sorbian, Vunjo,\n    Walser, Welsh, Zulu\n  Jabberwocky in Go Regular\n    From en.wikipedia.org/wiki/Jabberwocky:\n    There is no Greek version listed. Instead, a pangram from clagnut.com/blog/2380/#Greek:\n		\n			By Nigel Tao, Chuck Bigelow and Rob Pike\n		\n	','https://blog.golang.org/go-fonts','2016-11-16','2017-05-07 13:49:58.653643'),(473,'Go 1.8 is released','Chris Broadfoot','		Go 1.8 is released\n		16 February 2017\n		\n    Today the Go team is happy to announce the release of Go 1.8.\n    You can get it from the download page.\n    There are significant performance improvements and changes across the standard library.\n    The compiler back end introduced in Go 1.7 for 64-bit x86 is now used\n    on all architectures, and those architectures should see significant performance improvements.\n    For instance, the CPU time required by our benchmark programs was reduced by 20-30% on 32-bit ARM systems.\n    There are also some modest performance improvements in this release for 64-bit x86 systems.\n    The compiler and linker have been made faster.\n    Compile times should be improved by about 15% over Go 1.7.\n    There is still more work to be done in this area: expect faster compilation speeds in future releases.\n    Garbage collection pauses should be significantly shorter,\n    usually under 100 microseconds and often as low as 10 microseconds.\n    The HTTP server adds support for HTTP/2 Push,\n    allowing servers to preemptively send responses to a client.\n    This is useful for minimizing network latency by eliminating roundtrips.\n    The HTTP server also adds support for graceful shutdown,\n    allowing servers to minimize downtime by shutting down only after serving all requests that are in flight.\n    Contexts (added to the standard library in Go 1.7)\n    provide a cancelation and timeout mechanism.\n    Go 1.8 adds support for contexts in more parts of the standard library,\n    including the database/sql and net packages\n    and Server.Shutdown in the net/http package.\n    It\'s now much simpler to sort slices using the newly added Slice\n    function in the sort package. For example, to sort a slice of structs by their Name field:\n  sort.Slice(s, func(i, j int) bool { return s[i].Name &lt; s[j].Name })\n    Go 1.8 includes many more additions, improvements, and fixes.\n    Find the complete set of changes, and more information about the improvements listed above, in the\n    Go 1.8 release notes.\n    To celebrate the release, Go User Groups around the world are holding release parties this week.\n    Release parties have become a tradition in the Go community, so if you missed out this time, keep an eye out when 1.9 nears.\n    Thank you to over 200 contributors who helped with this release.\n		\n			By Chris Broadfoot\n		\n	','https://blog.golang.org/go1.8','2017-02-16','2017-05-07 13:49:58.736710'),(474,'GopherChina Trip Report','Robert Griesemer','		GopherChina Trip Report\n		1 July 2015\n		\n    We have known for some time that Go is more popular in China than in any other\n    country.\n    According to Google Trends, most searches for the term “golang” come from The People’s Republic than anywhere else.\n    Others have speculated on\n    the same observation, yet so far we have had\n    sparse concrete information\n    about the phenomenon.\n    The first Go conference in China, GopherChina,\n    seemed like an excellent opportunity to explore the situation by putting some\n    Western Gopher feet on Chinese ground. An actual invitation made it real and I\n    decided to accept and give a presentation about gofmt’s impact on software\n    development.\n    Hello, Shanghai!\n    The conference took place over an April weekend in Shanghai, in the\n    Puruan Building\n    of the Shanghai Pudong Software Park, easily reachable by subway within an hour\n    or less from Shanghai’s more central parts.\n    Modelled after GopherCon, the conference was\n    single-track, with all talks presented in a conference room that fit about 400\n    attendees.\n    It was organized by volunteers, lead by Asta Xie,\n    and with robust sponsorship from major industry names. According to the\n    organizers, many more people were hoping to attend than could be accommodated\n    due to space constraints.\n    The welcoming committee with Asta Xie (2nd from left), the primary organizer.\n    Each attendee received a bag filled with the obligatory GopherChina t-shirt,\n    various sponsor-related informational brochures, stickers, and the occasional\n    stuffed “something” (no fluffy Gophers, though). At least one 3rd party vendor\n    was advertising technical books, including several original (not translated\n    from English) Go books.\n    Go books!\n    On first impression, the average attendee seemed pretty young, which made for\n    an enthusiastic crowd, and the event appeared well run.\n    With the exception of my talk, all presentations were given in Mandarin and\n    thus were incomprehensible to me. Asta Xie, the primary organizer, assisted\n    with a few simultaneous translations whispered into my ear, and the occasional\n    English slide provided additional clues: “69GB” stands out even without any\n    Mandarin knowledge (more on that below). Consequently, I ended up listening to\n    a handful of presentations only, and instead spent much of my time talking with\n    attendees outside the main conference room. Yet judging from the slides, the\n    quality of most presentations seemed high, comparable with our experience at\n    GopherCon in Denver last year. Each talk got a one hour time slot which allowed\n    for plenty of technical detail, and many (dozens) of questions from an\n    enthusiastic audience.\n    As expected, many of the presentations were about web services, backends for\n    mobile applications, and so on. Some of the systems appear to be huge by any\n    measure.\n    For instance, a talk by Yang Zhou\n    described a large-scale internal messaging system, used by\n    Qihoo 360, a major Chinese software firm, all written\n    in Go. The presentation discussed how his team managed to reduce an original\n    heap size of 69GB (!) and the resulting long GC pauses of 3-6s to more\n    manageable numbers, and how they run millions of goroutines per machine, on a\n    fleet of thousands of machines. A future guest blog post is planned describing\n    this system in more detail.\n    Packed conference room on Saturday.\n    In another presentation, Feng Guo from\n    DaoCloud talked about how they use Go in their\n    company for what they call the “continuous delivery” of applications. DaoCloud\n    takes care of automatically moving software hosted on GitHub (and Chinese\n    equivalents) to the cloud. A software developer simply pushes a new version on\n    GitHub and DaoCloud takes care of the rest: running tests,\n    Dockerizing it, and shipping it using your\n    preferred cloud service provider.\n    Several speakers were from well-recognized major software firms (I showed the\n    conference program to non-technical people and they easily recognized several\n    of the firm’s names). Much more so than in the US, it seems Go is not just\n    hugely popular with newcomers and startups, but has very much found its way\n    into larger organizations and is employed at a scale that we are only starting\n    to see elsewhere.\n    Not being an expert in web services myself, in my presentation I veered off the\n    general conference theme a bit by talking about\n    gofmt and how its widespread use has started\n    to shape expectations not just for Go but other languages as well.\n    I presented in English but had my slides translated to Mandarin beforehand. Due\n    to the significant language barrier I wasn’t expecting too many questions on my\n    talk itself.\n    Instead I decided the keep it short and leave plenty of time for general\n    questions on Go, which the audience appreciated.\n    No social event in China is complete without fantastic food.\n    A couple of days after the conference I visited the 4-year-old startup company\n    Qiniu (“Seven Bulls”), at the invitation of its\n    CEO Wei Hsu, facilitated and\n    translated with the help of Asta Xie. Qiniu is a cloud-based storage provider\n    for mobile applications; Wei Hsu presented at the conference and also happens\n    to be the author of one of the first Chinese books on Go (the leftmost one in\n    the picture above).\n    Qiniu lobby, engineering.\n    Qiniu is an extremely successful all-Go shop, with about 160 employees, serving\n    over 150,000 companies and developers, storing over 50 Billion files, and\n    growing by over 500 Million files per day. When asked about the reasons for\n    Go’s success in China, Wei Hsu is quick to answer: PHP is extremely popular in\n    China, but relatively slow and not well-suited for large systems. Like in the\n    US, universities teach C++ and Java as primary languages, but for many\n    applications C++ is too complex a tool and Java too bulky. In his opinion, Go\n    now plays the role that traditionally belonged to PHP, but Go runs much faster,\n    is type safe, and scales more easily. He loves the fact that Go is simple and\n    applications are easy to deploy. He thought the language to be “perfect” for\n    them and his primary request was for a recommended or even standardized package\n    to easily access database systems. He did mention that they had GC problems in\n    the past but were able to work around them. Hopefully our upcoming 1.5 release\n    will address this. For Qiniu, Go appeared just at the right time and the right\n    (open source) place.\n    According to Asta Xie, Qiniu is just one of many Go shops in the PRC. Large\n    companies such as Alibaba, Baidu, Tencent, and Weibo, are now all using Go in\n    one form or another. He pointed out that while Shanghai and neighboring cities\n    like Suzhou are\n    high-tech centres, even more software developers are found in the Beijing area.\n    For 2016,  Asta hopes to organize a larger (1000, perhaps 1500 people)\n    successor conference in Beijing.\n    It appears that we have found the Go users in China: They are everywhere!\n    Some of the GopherChina materials, including videos, are now available alongside Go coursework on a 3rd party site.\n		\n			By Robert Griesemer\n		\n	','https://blog.golang.org/gopherchina','2015-07-01','2017-05-07 13:49:58.886791'),(475,'Qihoo 360 and Go','Yang Zhou','		Qihoo 360 and Go\n		6 July 2015\n		\n    This guest blog post was written by Yang Zhou, Software Engineer at Qihoo 360.\n    Qihoo 360 is a major provider of Internet and\n    mobile security products and services in China, and operates a major\n    Android-based mobile distribution platform. At the end of June 2014, Qihoo had\n    about 500 million monthly active PC Internet users and over 640 million mobile\n    users. Qihoo also operates one of China’s most popular Internet browsers and PC\n    search engines.\n    My team, the Push Service Team, provides fundamental messaging services for\n    more than 50 products across the company (both PC and mobile), including\n    thousands of Apps in our open platform.\n    Our \"love affair\" with Go dates back to 2012 when we first attempted to provide\n    push services for one of Qihoo’s products. The initial version was built with\n    nginx + lua + redis, which failed to satisfy our requirement for real-time\n    performance due to excessive load. Under these circumstances, the\n    newly-published Go 1.0.3 release came to our attention. We completed a\n    prototype in a matter of weeks, largely thanks to the goroutine and channel\n    features it provided.\n    Initially, our Go-based system ran on 20 servers, with 20 million real-time\n    connections in total. The system sent 2 million messages a day. That system now\n    runs on 400 servers, supporting 200 million+ real-time connections. It now\n    sends over 10 billion messages daily.\n    With rapid business expansion and increasing application needs for our push\n    service, the initial Go system quickly reached its bottleneck: heap size went\n    up to 69G, with maximum garbage collection (GC) pauses of 3-6 seconds. Worse\n    still, we had to reboot the system every week to release memory. It wouldn’t be\n    honest if we didn’t consider relinquishing Go and instead, re-writing the\n    entire core component with C. However, things didn’t go exactly as we planned,\n    we ran into trouble migrating the code of Business Logic Layer. As a result, it\n    was impossible for the only personnel at that time (myself) to maintain the Go\n    system while ensuring the logic transfer to the C service framework.\n    Therefore, I made the decision to stay with Go system (probably the wisest one\n    I had to make), and great headway was made soon enough.\n    Here are a few tweaks we made and key take-aways:\n    Replace short connections with persistent ones (using a connection pool), to reduce creation of buffers and objects during communication.\n    Use Objects and Memory pools appropriately, to reduce the load on the GC.\n    Use a Task Pool, a mechanism with a group of long-lived goroutines consuming global task or message queues sent by connection goroutines, to replace short-lived goroutines.\n    Monitor and control goroutine numbers in the program. The lack of control can cause unbearable burden on the GC, imposed by surges in goroutines due to uninhibited acceptance of external requests, as RPC invocations sent to inner servers may block goroutines recently created.\n    Remember to add read and write deadlines to connections when under a mobile network; otherwise, it may lead to goroutine blockage. Apply it properly and with caution when under a LAN network, otherwise your RPC communication efficiency will be hurt.\n    Use Pipeline (under Full Duplex feature of TCP) to enhance the communication efficiency of RPC framework.\n    As a result, we successfully launched three iterations of our architecture, and two iterations of our RPC framework even with limited human resources. This can all attributed to the development convenience of Go. Below you can find the up-to-date system architecture:\n    The continuous improvement journey can be illustrated by a table:\n    Also, no temporary release of memory or system reboot is required after these\n    optimizations.\n    What’s more exciting is we developed an on-line real-time Visibility Platform\n    for profiling Go programs. We can now easily access and diagnose the system\n    status, pinning down any potential risks. Here is a screen shot of the system\n    in action:\n    The great thing about this platform is that we can actually simulate the\n    connection and behavior of millions of online users, by applying the\n    Distributed Stress Test Tool (also built using Go), and observe all real-time\n    visualized data. This allows us to evaluate the effectiveness of any\n    optimization and preclude problems by identifying system bottlenecks.\n    Almost every possible system optimization has been practiced so far. And we\n    look forward to more good news from the GC team so that we could be further\n    relieved from heavy development work. I guess our experience may also grow\n    obsolete one day, as Go continues to evolve.\n    This is why I want to conclude my sharing by extending my sincere appreciation\n    to the opportunity to attend Gopher China.\n    It was a gala for us to learn, to share and for offering a window showcasing\n    Go’s popularity and prosperity in China. Many other teams within Qihoo have\n    already either got to know Go, or tried to use Go.\n    I am convinced that many more Chinese Internet firms will join us in\n    re-creating their system in Go and the Go team\'s efforts will benefit more\n    developers and enterprises in the foreseeable future.\n		\n			By Yang Zhou\n		\n	','https://blog.golang.org/qihoo','2015-07-06','2017-05-07 13:49:58.910424'),(476,'Go 1.5 is released','Andrew Gerrand','		Go 1.5 is released\n		19 August 2015\n		\n    Today the Go project is proud to release Go 1.5,\n    the sixth major stable release of Go.\n    This release includes significant changes to the implementation.\n    The compiler tool chain was translated from C to Go,\n    removing the last vestiges of C code from the Go code base.\n    The garbage collector was completely redesigned,\n    yielding a dramatic reduction\n    in garbage collection pause times.\n    Related improvements to the scheduler allowed us to change the default\n    GOMAXPROCS value\n    (the number of concurrently executing goroutines)\n    from 1 to the number of logical CPUs.\n    Changes to the linker enable distributing Go packages as shared libraries to\n    link into Go programs, and building Go packages into archives or shared\n    libraries that may be linked into or loaded by C programs\n    (design doc).\n    The release also includes improvements to the developer tools.\n    Support for \"internal\" packages\n    permits sharing implementation details between packages.\n    Experimental support for \"vendoring\"\n    external dependencies is a step toward a standard mechanism for managing\n    dependencies in Go programs.\n    The new \"go tool trace\" command enables the\n    visualisation of  program traces generated by new tracing infrastructure in the\n    runtime.\n    The new \"go doc\"\n    command provides an improved command-line interface for viewing Go package documentation.\n    There are also several new operating system and architecture ports.\n    The more mature new ports are darwin/arm,\n    darwin/arm64 (Apple\'s iPhone and iPad devices),\n    and linux/arm64.\n    There is also experimental support for ppc64 and ppc64le\n    (IBM 64-bit PowerPC, big and little endian).\n    The new darwin/arm64 port and external linking features fuel the\n    Go mobile project, an experiment to\n    see how Go might be used for building apps on Android and iOS devices.\n    (The Go mobile work itself is not part of this release.)\n    The only language change is very minor,\n    the lifting of a restriction in the map literal syntax\n    to make them more succinct and consistent with slice literals.\n    The standard library saw many additions and improvements, too.\n    The flag package now shows cleaner usage messages.\n    The math/big package now provides a Float\n    type for computing with arbitrary-precision floating point numbers.\n    An improvement to the DNS resolver on\n    Linux and BSD systems has removed the cgo requirement for programs that do name\n    lookups.\n    The go/types package has been\n    moved to the standard library from\n    the golang.org/x/tools repository.\n    (The new go/constant and\n    go/importer packages are also a result\n    of this move.)\n    The reflect package has added the\n    ArrayOf and\n    FuncOf functions, analogous to the\n    existing SliceOf function.\n    And, of course, there is the usual\n    list of smaller fixes and improvements.\n    For the full story, see the detailed release notes.\n    Or if you just can\'t wait to get started,\n    head over to the downloads page to get Go 1.5 now.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go1.5','2015-08-19','2017-05-07 13:49:58.945276'),(477,'Go GC: Prioritizing low latency and simplicity','Richard Hudson','		Go GC: Prioritizing low latency and simplicity\n		31 August 2015\n		\n  The Setup\n    Go is building a garbage collector (GC) not only for 2015 but for 2025 and\n    beyond: A GC that supports today’s software development and scales along with\n    new software and hardware throughout the next decade. Such a future has no\n    place for stop-the-world GC pauses, which have been an impediment to broader\n    uses of safe and secure languages such as Go.\n    Go 1.5, the first glimpse of this future, achieves GC latencies well below the\n    10 millisecond goal we set a year ago. We presented some impressive numbers\n    in a talk at Gophercon.\n    The latency improvements have generated a lot of attention;\n    Robin Verlangen’s blog post\n    Billions of requests per day meet Go 1.5\n    validates our direction with end to end results.\n    We also particularly enjoyed\n    Alan Shreve’s production server graphs\n    and his \"Holy 85% reduction\" comment.\n    Today 16 gigabytes of RAM costs $100 and CPUs come with many cores, each with\n    multiple hardware threads. In a decade this hardware will seem quaint but the\n    software being built in Go today will need to scale to meet expanding needs and\n    the next big thing. Given that hardware will provide the power to increase\n    throughput, Go’s garbage collector is being designed to favor low latency and\n    tuning via only a single knob. Go 1.5 is the first big step down this path and\n    these first steps will forever influence Go and the applications it best\n    supports. This blog post gives a high-level overview of what we have done for\n    the Go 1.5 collector.\n  The Embellishment\n    To create a garbage collector for the next decade, we turned to an algorithm\n    from decades ago. Go\'s new garbage collector is a concurrent, tri-color,\n    mark-sweep collector, an idea first proposed by\n    Dijkstra in 1978.\n    This is a deliberate divergence from most \"enterprise\" grade garbage collectors\n    of today, and one that we believe is well suited to the properties of modern\n    hardware and the latency requirements of modern software.\n    In a tri-color collector, every object is either white, grey, or black and we\n    view the heap as a graph of connected objects. At the start of a GC cycle all\n    objects are white. The GC visits all roots, which are objects directly\n    accessible by the application such as globals and things on the stack, and\n    colors these grey. The GC then chooses a grey object, blackens it, and then\n    scans it for pointers to other objects. When this scan finds a pointer to a\n    white object, it turns that object grey. This process repeats until there are\n    no more grey objects. At this point, white objects are known to be unreachable\n    and can be reused.\n    This all happens concurrently with the application, known as the mutator,\n    changing pointers while the collector is running. Hence, the mutator must\n    maintain the invariant that no black object points to a white object, lest the\n    garbage collector lose track of an object installed in a part of the heap it\n    has already visited. Maintaining this invariant is the job of the\n    write barrier, which is a small function run by the mutator whenever a\n    pointer in the heap is modified. Go’s write barrier colors the now-reachable\n    object grey if it is currently white, ensuring that the garbage collector will\n    eventually scan it for pointers.\n    Deciding when the job of finding all grey objects is done is subtle and can be\n    expensive and complicated if we want to avoid blocking the mutators. To keep\n    things simple Go 1.5 does as much work as it can concurrently and then briefly\n    stops the world to inspect all potential sources of grey objects. Finding the\n    sweet spot between the time needed for this final stop-the-world and the total\n    amount of work that this GC does is a major deliverable for Go 1.6.\n    Of course the devil is in the details. When do we start a GC cycle? What\n    metrics do we use to make that decision? How should the GC interact with the Go\n    scheduler? How do we pause a mutator thread long enough to scan its stack?\n     How do we represent white, grey, and black so we can efficiently find and scan\n    grey objects? How do we know where the roots are? How do we know where in an\n    object pointers are located? How do we minimize memory fragmentation? How do we\n    deal with cache performance issues? How big should the heap be? And on and on,\n    some related to allocation, some to finding reachable objects, some related to\n    scheduling, but many related to performance. Low-level discussions of each of\n    these areas are beyond the scope of this blog post.\n    At a higher level, one approach to solving performance problems is to add GC\n    knobs, one for each performance issue. The programmer can then turn the knobs\n    in search of appropriate settings for their application. The downside is that\n    after a decade with one or two new knobs each year you end up with the GC Knobs\n    Turner Employment Act. Go is not going down that path. Instead we provide a\n    single knob, called GOGC. This value controls the total size of the heap\n    relative to the size of reachable objects. The default value of 100 means that\n    total heap size is now 100% bigger than (i.e., twice) the size of the reachable\n    objects after the last collection. 200 means total heap size is 200% bigger\n    than (i.e., three times) the size of the reachable objects. If you want to\n    lower the total time spent in GC, increase GOGC. If you want to trade more GC\n    time for less memory, lower GOGC.\n    More importantly as RAM doubles with the next generation of hardware, simply\n    doubling GOGC will halve the number of GC cycles. On the other hand since GOGC\n    is based on reachable object size, doubling the load by doubling the reachable\n    objects requires no retuning. The application just scales.\n    Furthermore, unencumbered by ongoing support for dozens of knobs, the runtime\n    team can focus on improving the runtime based on feedback from real customer\n    applications.\n  The Punchline\n    Go 1.5’s GC ushers in a future where stop-the-world pauses are no longer a\n    barrier to moving to a safe and secure language. It is a future where\n    applications scale effortlessly along with hardware and as hardware becomes\n    more powerful the GC will not be an impediment to better, more scalable\n    software. It’s a good place to be for the next decade and beyond.\n    For more details about the 1.5 GC and how we eliminated latency issues see the\n    Go GC: Latency Problem Solved presentation\n    or the slides.\n		\n			By Richard Hudson\n		\n	','https://blog.golang.org/go15gc','2015-08-31','2017-05-07 13:49:58.985561'),(478,'Six years of Go','Andrew Gerrand','		Six years of Go\n		10 November 2015\n		\n    Six years ago today the Go language was released as an open source project.\n    Since then, more than 780 contributors have made over 30,000 commits to the\n    project\'s 22 repositories. The ecosystem continues to grow, with GitHub\n    reporting more than 90,000 Go repositories. And, offline, we see new Go events\n    and user groups pop up around\n    the\n    world with regularity.\n    In August we released Go 1.5, the most\n    significant release since Go 1. It features a completely\n    redesigned garbage collector that makes\n    the language more suitable for latency-sensitive applications; it marks the\n    transition from a C-based compiler tool chain to one\n    written entirely in Go; and it includes\n    ports to new architectures, with better\n    support for ARM processors (the chips that power most smartphones).\n    These improvements make Go better suited to a broader range of tasks, a trend\n    that we hope will continue over the coming years.\n    Improvements to tools continue to boost developer productivity.\n    We introduced the execution tracer and the\n    \"go doc\"\n    command, as well as more enhancements to our various\n    static analysis tools.\n    We are also working on an\n    official Go plugin for Sublime Text,\n    with better support for other editors in the pipeline.\n    Early next year we will release more improvements in Go 1.6, including \n    HTTP/2 support for net/http servers and\n    clients, an official package vendoring mechanism, support for blocks in text\n    and HTML templates, a memory sanitizer that checks both Go and C/C++ code, and\n    the usual assortment of other improvements and fixes.\n    This is the sixth time we have had the pleasure of writing a birthday blog post\n    for Go, and we would not be doing so if not for the wonderful and passionate\n    people in our community. The Go team would like to thank everyone who has\n    contributed code, written an open source library, authored a blog post, helped\n    a new gopher, or just given Go a try. Without you, Go would not be as complete,\n    useful, or successful as it is today. Thank you, and celebrate!\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/6years','2015-11-10','2017-05-07 13:49:59.039738'),(479,'Language and Locale Matching in Go','Marcel van Lohuizen','		Language and Locale Matching in Go\n		9 February 2016\n		\n  Introduction\n    Consider an application, such as a web site, with support for multiple languages\n    in its user interface.\n    When a user arrives with a list of preferred languages, the application must\n    decide which language it should use in its presentation to the user.\n    This requires finding the best match between the languages the application supports\n    and those the user prefers.\n    This post explains why this is a difficult decision and how Go can help.\n  Language Tags\n    Language tags, also known as locale identifiers, are machine-readable\n    identifiers for the language and/or dialect being used.\n    The most common reference for them is the IETF BCP 47 standard, and that is the\n    standard the Go libraries follow.\n    Here are some examples of BCP 47 language tags and the language or dialect they\n    represent.\n	.padtable { padding-left: 40px; }\n	.padtable td, .padtable th { padding-right: 10px; }\n	.tag { font-family: courier; }\n	Tag\n	Description\n	en\n	English\n	en-US\n	American English\n	cmn\n	Mandarin Chinese\n	zh\n	Chinese, typically Mandarin\n	nl\n	Dutch\n	nl-BE\n	Flemish\n	es-419\n	Latin American Spanish\n	az, az-Latn\n	both Azerbaijani written in Latin script\n	az-Arab\n	Azerbaijani written in Arabic\n    The general form of the language tag is\n    a language code (“en”, “cmn”, “zh”, “nl”, “az” above)\n    followed by an optional subtag for script (“-Arab”),\n    region (“-US”, “-BE”, “-419”),\n    variants (“-oxendict” for Oxford English Dictionary spelling),\n    and extensions (“-u-co-phonebk” for phone-book sorting).\n    The most common form is assumed if a subtag is omitted, for instance\n    “az-Latn-AZ” for “az”.\n    The most common use of language tags is to select from a set of system-supported\n    languages according to a list of the user\'s language preferences, for example\n    deciding that a user who prefers Afrikaans would be best served (assuming\n    Afrikaans is not available) by the system showing Dutch. Resolving such matches\n    involves consulting data on mutual language comprehensibility.\n    The tag resulting from this match is subsequently used to obtain\n    language-specific resources such as translations, sorting order,\n    and casing algorithms.\n    This involves a different kind of matching. For example, as there is no specific\n    sorting order for Portuguese, a collate package may fall back to the sorting\n    order for the default, or “root”, language.\n  The Messy Nature of Matching Languages\n    Handling language tags is tricky.\n    This is partly because the boundaries of human languages are not well defined\n    and partly because of the legacy of evolving language tag standards.\n    In this section we will show some of the messy aspects of handling language tags.\n     Tags with different language codes can indicate the same language\n    For historical and political reasons, many language codes have changed over\n    time, leaving languages with an older legacy code as well as a new one.\n    But even two current codes may refer to the same language.\n    For example, the official language code for Mandarin is “cmn”, but “zh” is by\n    far the most commonly used designator for this language.\n    The code “zh” is officially reserved for a so called macro language, identifying\n    the group of Chinese languages.\n    Tags for macro languages are often used interchangeably with the most-spoken\n    language in the group.\n    Matching language code alone is not sufficient\n    Azerbaijani (“az”), for example, is written in different scripts depending on\n    the country in which it is spoken: \"az-Latn\" for Latin (the default script),\n    \"az-Arab\" for Arabic, and “az-Cyrl” for Cyrillic.\n    If you replace \"az-Arab\" with just \"az\", the result will be in Latin script and\n    may not be understandable to a user who only knows the Arabic form.\n    Also different regions may imply different scripts.\n    For example: “zh-TW” and “zh-SG” respectively imply the use of Traditional and\n    Simplified Han. As another example, “sr” (Serbian) defaults to Cyrillic script,\n    but “sr-RU” (Serbian as written in Russia) implies the Latin script!\n    A similar thing can be said for Kyrgyz and other languages.\n    If you ignore subtags, you might as well present Greek to the user.\n    The best match might be a language not listed by the user\n    The most common written form of Norwegian (“nb”) looks an awful lot like Danish.\n    If Norwegian is not available, Danish may be a good second choice.\n    Similarly, a user requesting Swiss German (“gsw”) will likely be happy to be\n    presented German (“de”), though the converse is far from true.\n    A user requesting Uygur may be happier to fall back to Chinese than to English.\n    Other examples abound.\n    If a user-requested language is not supported, falling back to English is often\n    not the best thing to do.\n    The choice of language decides more than translation\n    Suppose a user asks for Danish, with German as a second choice.\n    If an application chooses German, it must not only use German translations\n    but also use German (not Danish) collation.\n    Otherwise, for example, a list of animals might sort “Bär” before “Äffin”.\n    Selecting a supported language given the user’s preferred languages is like a\n    handshaking algorithm: first you determine which protocol to communicate in (the\n    language) and then you stick with this protocol for all communication for the\n    duration of a session.\n    Using a “parent” of a language as fallback is non-trivial\n    Suppose your application supports Angolan Portuguese (“pt-AO”).\n    Packages in golang.org/x/text, like collation and display, may not\n    have specific support for this dialect.\n    The correct course of action in such cases is to match the closest parent dialect.\n    Languages are arranged in a hierarchy, with each specific language having a more\n    general parent.\n    For example, the parent of “en-GB-oxendict” is “en-GB”, whose parent is “en”,\n    whose parent is the undefined language “und”, also known as the root language.\n    In the case of collation, there is no specific collation order for Portugese,\n    so the collate package will select the sorting order of the root language.\n    The closest parent to Angolan Portuguese supported by the display package is\n    European Portuguese (“pt-PT”) and not the more obvious “pt”, which implies\n    Brazilian.\n    In general, parent relationships are non-trivial.\n    To give a few more examples, the parent of “es-CL” is “es-419”, the parent of\n    “zh-TW” is “zh-Hant”, and the parent of “zh-Hant” is “und”.\n    If you compute the parent by simply removing subtags, you may select a “dialect”\n    that is incomprehensible to the user.\n  Language Matching in Go\n    The Go package golang.org/x/text/language implements the BCP 47\n    standard for language tags and adds support for deciding which language to use\n    based on data published in the Unicode Common Locale Data Repository (CLDR).\n    Here is a sample program, explained below, matching a user\'s language\n    preferences against an application\'s supported languages:\n	\npackage main\nimport (\n    \"fmt\"\n    \"golang.org/x/text/language\"\n    \"golang.org/x/text/language/display\"\n)\nvar userPrefs = []language.Tag{\n    language.Make(\"gsw\"), // Swiss German\n    language.Make(\"fr\"),  // French\n}\nvar serverLangs = []language.Tag{\n    language.AmericanEnglish, // en-US fallback\n    language.German,          // de\n}\nvar matcher = language.NewMatcher(serverLangs)\nfunc main() {\n    tag, index, confidence := matcher.Match(userPrefs...)\n    fmt.Printf(\"best match: %s (%s) index=%d confidence=%v\\n\",\n        display.English.Tags().Name(tag),\n        display.Self.Name(tag),\n        index, confidence)\n    // best match: German (Deutsch) index=1 confidence=High\n}\n  Creating Language Tags\n    The simplest way to create a language.Tag from a user-given language code string\n    is with language.Make.\n    It extracts meaningful information even from malformed input.\n    For example, “en-USD” will result in “en” even though USD is not a valid subtag.\n    Make doesn’t return an error.\n    It is common practice to use the default language if an error occurs anyway so\n    this makes it more convenient. Use Parse to handle any error manually.\n    The HTTP Accept-Language header is often used to pass a user’s desired\n    languages.\n    The ParseAcceptLanguage function parses it into a slice of language tags,\n    ordered by preference.\n    By default, the language package does not canonicalize tags.\n    For example, it does not follow the BCP 47 recommendation of eliminating scripts\n    if it is the common choice in the “overwhelming majority”.\n    It similarly ignores CLDR recommendations: “cmn” is not replaced by “zh” and\n    “zh-Hant-HK” is not simplified to “zh-HK”.\n    Canonicalizing tags may throw away useful information about user intent.\n    Canonicalization is handled in the Matcher instead.\n    A full array of canonicalization options are available if the programmer still\n    desires to do so.\n  Matching User-Preferred Languages to Supported Languages\n    A Matcher matches user-preferred languages to supported languages.\n    Users are strongly advised to use it if they don’t want to deal with all the\n    intricacies of matching languages.\n    The Match method may pass through user settings (from BCP 47 extensions) from\n    the preferred tags to the selected supported tag.\n    It is therefore important that the tag returned by Match is used to obtain\n    language-specific resources.\n    For example, “de-u-co-phonebk” requests phone-book ordering for German.\n    The extension is ignored for matching, but is used by the collate package to\n    select the respective sorting order variant.\n    A Matcher is initialized with the languages supported by an application, which\n    are usually the languages for which there are translations.\n    This set is typically fixed, allowing a matcher to be created at startup.\n    Matcher is optimized to improve the performance of Match at the expense of\n    initialization cost.\n    The language package provides a predefined set of the most commonly used\n    language tags that can be used for defining the supported set.\n    Users generally don’t have to worry about the exact tags to pick for supported\n    languages.\n    For example, AmericanEnglish (“en-US”) may be used interchangeably with the more\n    common English (“en”), which defaults to American.\n    It is all the same for the Matcher. An application may even add both, allowing\n    for more specific American slang for “en-US”.\n  Matching Example\n    Consider the following Matcher and lists of supported languages:\n  var supported = []language.Tag{\n    language.AmericanEnglish,    // en-US: first language is fallback\n    language.German,             // de\n    language.Dutch,              // nl\n    language.Portuguese          // pt (defaults to Brazilian)\n    language.EuropeanPortuguese, // pt-pT\n    language.Romanian            // ro\n    language.Serbian,            // sr (defaults to Cyrillic script)\n    language.SerbianLatin,       // sr-Latn\n    language.SimplifiedChinese,  // zh-Hans\n    language.TraditionalChinese, // zh-Hant\n}\nvar matcher = language.NewMatcher(supported)\n    Let\'s look at the matches against this list of supported languages for various\n    user preferences.\n    For a user preference of \"he\" (Hebrew), the best match is \"en-US\" (American\n    English).\n    There is no good match, so the matcher uses the fallback language (the first in\n    the supported list).\n    For a user preference of \"hr\" (Croatian), the best match is \"sr-Latn\" (Serbian\n    with Latin script), because, once they are written in the same script, Serbian\n    and Croatian are mutually intelligible.\n    For a user preference of \"ru, mo\" (Russian, then Moldavian), the best match is\n    \"ro\" (Romanian), because Moldavian is now canonically classified as \"ro-MD\"\n    (Romanian in Moldova).\n    For a user preference of \"zh-TW\" (Mandarin in Taiwan), the best match is\n    \"zh-Hant\" (Mandarin written in Traditional Chinese), not \"zh-Hans\" (Mandarin\n    written in Simplified Chinese).\n    For a user preference of \"af, ar\" (Afrikaans, then Arabic), the best match is\n    \"nl\" (Dutch). Neither preference is supported directly, but Dutch is a\n    significantly closer match to Afrikaans than the fallback language English is to\n    either.\n    For a user preference of \"pt-AO, id\" (Angolan Portuguese, then Indonesian), the\n    best match is \"pt-PT\" (European Portuguese), not \"pt\" (Brazilian Portuguese).\n    For a user preference of \"gsw-u-co-phonebk\" (Swiss German with phone-book\n    collation order), the best match is \"de-u-co-phonebk\" (German with phone-book\n    collation order).\n    German is the best match for Swiss German in the server\'s language list, and the\n    option for phone-book collation order has been carried over.\n  Confidence Scores\n    Go uses coarse-grained confidence scoring with rule-based elimination.\n    A match is classified as Exact, High (not exact, but no known ambiguity), Low\n    (probably the correct match, but maybe not), or No.\n    In case of multiple matches, there is a set of tie-breaking rules that are\n    executed in order.\n    The first match is returned in the case of multiple equal matches.\n    These confidence scores may be useful, for example, to reject relatively weak\n    matches.\n    They are also used to score, for example, the most likely region or script from\n    a language tag.\n    Implementations in other languages often use more fine-grained, variable-scale\n    scoring.\n    We found that using coarse-grained scoring in the Go implementation ended up\n    simpler to implement, more maintainable, and faster, meaning that we could\n    handle more rules.\n  Displaying Supported Languages\n    The golang.org/x/text/language/display package allows naming language\n    tags in many languages.\n    It also contains a “Self” namer for displaying a tag in its own language.\n    For example:\n	\n    var supported = []language.Tag{\n        language.English,            // en\n        language.French,             // fr\n        language.Dutch,              // nl\n        language.Make(\"nl-BE\"),      // nl-BE\n        language.SimplifiedChinese,  // zh-Hans\n        language.TraditionalChinese, // zh-Hant\n        language.Russian,            // ru\n    }\n    en := display.English.Tags()\n    for _, t := range supported {\n        fmt.Printf(\"%-20s (%s)\\n\", en.Name(t), display.Self.Name(t))\n    }\n    prints\n  English              (English)\nFrench               (français)\nDutch                (Nederlands)\nFlemish              (Vlaams)\nSimplified Chinese   (简体中文)\nTraditional Chinese  (繁體中文)\nRussian              (русский)\n    In the second column, note the differences in capitalization, reflecting the\n    rules of the respective language.\n  Conclusion\n    At first glance, language tags look like nicely structured data, but because\n    they describe human languages, the structure of relationships between language\n    tags is actually quite complex.\n    It is often tempting, especially for English-speaking programmers, to write\n    ad-hoc language matching using nothing other than string manipulation of the\n    language tags.\n    As described above, this can produce awful results.\n    Go\'s golang.org/x/text/language package solves this complex problem\n    while still presenting a simple, easy-to-use API. Enjoy.\n		\n			By Marcel van Lohuizen\n		\n	','https://blog.golang.org/matchlang','2016-02-09','2017-05-07 13:49:59.060984'),(480,'Go 1.6 is released','Andrew Gerrand','		Go 1.6 is released\n		17 February 2016\n		\n    Today we release Go version 1.6,\n    the seventh major stable release of Go.\n    You can grab it right now from the download page.\n    Although the release of Go 1.5 six months ago\n    contained dramatic implementation changes,\n    this release is more incremental.\n    The most significant change is support for HTTP/2\n    in the net/http package.\n    HTTP/2 is a new protocol, a follow-on to HTTP that has already seen\n    widespread adoption by browser vendors and major websites.\n    In Go 1.6, support for HTTP/2 is enabled by default\n    for both servers and clients when using HTTPS,\n    bringing the benefits of the new protocol\n    to a wide range of Go projects,\n    such as the popular Caddy web server.\n    The template packages have learned some new tricks,\n    with support for trimming spaces around template actions\n    to produce cleaner template output,\n    and the introduction of the {{block}} action\n    that can be used to create templates that build on other templates.\n    A new template example program demonstrates these new features.\n    Go 1.5 introduced experimental support\n    for a “vendor” directory that was enabled by an environment variable.\n    In Go 1.6, the feature is now enabled by default.\n    Source trees that contain a directory named “vendor” that is not used in accordance with the new feature\n    will require changes to avoid broken builds (the simplest fix is to rename the directory).\n    The runtime has added lightweight, best-effort detection of concurrent misuse of maps.\n    As always, if one goroutine is writing to a map, no other goroutine should be reading or writing the map concurrently.\n    If the runtime detects this condition, it prints a diagnosis and crashes the program.\n    The best way to find out more about the problem is to run it under the\n    race detector,\n    which will more reliably identify the race and give more detail.\n    The runtime has also changed how it prints program-ending panics.\n    It now prints only the stack of the panicking goroutine, rather than all existing goroutines.\n    This behavior can be configured using the\n    GOTRACEBACK environment variable\n    or by calling the debug.SetTraceback function.\n    Users of cgo should be aware of major changes to the rules for sharing pointers between Go and C code.\n    The rules are designed to ensure that such C code can coexist with Go\'s garbage collector\n    and are checked during program execution, so code may require changes to avoid crashes.\n    See the release notes and\n    cgo documentation for the details.\n    The compiler, linker, and go command have a new -msan flag\n    analogous to -race and only available on linux/amd64,\n    that enables interoperation with the\n    Clang MemorySanitizer.\n    This is useful for testing a program containing suspect C or C++ code.\n    You might like to try it while testing your cgo code with the new pointer rules.\n    Performance of Go programs built with Go 1.6 remains similar to those built with Go 1.5.\n    Garbage-collection pauses are even lower than with Go 1.5,\n    but this is particularly noticeable for programs using large amounts of memory.\n    With regard to the performance of the compiler tool chain,\n    build times should be similar to those of Go 1.5.\n    The algorithm inside sort.Sort\n    was improved to run about 10% faster,\n    but the change may break programs that expect a specific ordering\n    of equal but distinguishable elements.\n    Such programs should refine their Less methods to indicate the desired ordering\n    or use sort.Stable\n    to preserve the input order for equal values.\n    And, of course, there are many more additions, improvements, and fixes.\n    You can find them all in the comprehensive release notes.\n    To celebrate the release,\n    Go User Groups around the world\n    are holding release parties on the 17th of February.\n    Online, the Go contributors are hosting a question and answer session\n    on the golang subreddit for the next 24 hours.\n    If you have questions about the project, the release, or just Go in general,\n    then please join the discussion.\n    Thanks to everyone that contributed to the release.\n    Happy hacking.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go1.6','2016-02-17','2017-05-07 13:49:59.076158'),(481,'Go 1.7 is released','Chris Broadfoot','		Go 1.7 is released\n		15 August 2016\n		\n    Today we are happy to announce the release of Go 1.7.\n    You can get it from the download page.\n    There are several significant changes in this release: a port for\n    Linux on IBM z Systems (s390x),\n    compiler improvements, the addition of the context package,\n    and support for hierarchical tests and benchmarks.\n    A new compiler back end, based on static single-assignment form (SSA),\n    has been under development for the past year.\n    By representing a program in SSA form, a compiler may perform advanced optimizations more easily.\n    This new back end generates more compact, more efficient code that includes\n    optimizations like\n    bounds check elimination and\n    common subexpression elimination.\n    We observed a 5–35% speedup across our benchmarks.\n    For now, the new backend is only available for the 64-bit x86 platform (\"amd64\"),\n    but we’re planning to convert more architecture backends to SSA in future releases.\n    The compiler front end uses a new, more compact export data format, and\n    processes import declarations more efficiently.\n    While these changes across the compiler toolchain are mostly invisible,\n    users have observed\n    a significant speedup in compile time and a reduction in binary size by as much as 20–30%.\n    Programs should run a bit faster due to speedups in the garbage collector and optimizations in the standard library.\n    Programs with many idle goroutines will experience much shorter garbage collection pauses than in Go 1.6.\n    Over the past few years, the golang.org/x/net/context\n    package has proven to be essential to many Go applications.\n    Contexts are used to great effect in applications related to networking, infrastructure, and microservices\n    (such as Kubernetes and Docker).\n    They make it easy to enable cancelation, timeouts, and passing request-scoped data.\n    To make use of contexts within the standard library and to encourage more extensive use,\n    the package has been moved from the x/net repository\n    to the standard library as the context package.\n    Support for contexts has been added to the\n    net,\n    net/http, and\n    os/exec packages.\n    For more information about contexts, see the package documentation\n    and the Go blog post Go Concurrency Patterns: Context.\n    Go 1.5 introduced experimental support for a \"vendor\" directory,\n    enabled by the GO15VENDOREXPERIMENT environment variable.\n    Go 1.6 enabled this behavior by default, and in Go 1.7, this switch has been removed and the \"vendor\" behavior is always enabled.\n    Go 1.7 includes many more additions, improvements, and fixes.\n    Find the complete set of changes, and details of the points above, in the\n    Go 1.7 release notes.\n    Finally, the Go team would like thank everyone who contributed to the release.\n    170 people contributed to this release, including 140 from the Go community.\n    These contributions ranged from changes to the compiler and linker, to the standard library, to documentation, and code reviews.\n    We welcome contributions; if you\'d like to get involved, check out the\n    contribution guidelines.\n		\n			By Chris Broadfoot\n		\n	','https://blog.golang.org/go1.7','2016-08-15','2017-05-07 13:49:59.112802'),(482,'Deploying Go servers with Docker','Andrew Gerrand','		Deploying Go servers with Docker\n		26 September 2014\n		\n  Introduction\n    This week Docker announced\n    official base images for Go and other major languages,\n    giving programmers a trusted and easy way to build containers for their Go programs.\n    In this article we\'ll walk through a recipe for creating a Docker container for\n    a simple Go web application and deploying that container to Google Compute Engine.\n    If you\'re not familiar with Docker, you should read\n    Understanding Docker\n    before reading on.\n  The demo app\n    For our demonstration we will use the\n    outyet program from the\n    Go examples repository,\n    a simple web server that reports whether the next version of Go has been released\n    (designed to power sites like isgo1point4.outyet.org).\n    It has no dependencies outside the standard library and requires no additional\n    data files at run time; for a web server, it\'s about as simple as it gets.\n    Use \"go get\" to fetch and install outyet in your\n    workspace:\n  $ go get github.com/golang/example/outyet\n  Write a Dockerfile\n    Replace a file named Dockerfile in the outyet directory with the following contents:\n  # Start from a Debian image with the latest version of Go installed\n# and a workspace (GOPATH) configured at /go.\nFROM golang\n# Copy the local package files to the container\'s workspace.\nADD . /go/src/github.com/golang/example/outyet\n# Build the outyet command inside the container.\n# (You may fetch or manage dependencies here,\n# either manually or with a tool like \"godep\".)\nRUN go install github.com/golang/example/outyet\n# Run the outyet command by default when the container starts.\nENTRYPOINT /go/bin/outyet\n# Document that the service listens on port 8080.\nEXPOSE 8080\n    This Dockerfile specifies how to construct a container that runs outyet,\n    starting with the basic dependencies (a Debian system with Go installed;\n    the official golang docker image),\n    adding the outyet package source, building it, and then finally running it.\n    The ADD, RUN, and ENTRYPOINT steps are common tasks for any Go project.\n    To simplify this, there is an\n    onbuild variant\n    of the golang image that automatically copies the package source, fetches the\n    application dependencies, builds the program, and configures it to run on\n    startup.\n    With the onbuild variant, the Dockerfile is much simpler:\n  FROM golang:onbuild\nEXPOSE 8080\n  Build and run the image\n    Invoke Docker from the outyet package directory to build an image using the Dockerfile:\n  $ docker build -t outyet .\n    This will fetch the golang base image from Docker Hub, copy the package source\n    to it, build the package inside it, and tag the resulting image as outyet.\n    To run a container from the resulting image:\n  $ docker run --publish 6060:8080 --name test --rm outyet\n    The --publish flag tells docker to publish the container\'s port 8080 on the\n    external port 6060.\n    The --name flag gives our container a predictable name to make it easier to work with.\n    The --rm flag tells docker to remove the container image when the outyet server exits.\n    With the container running, open http://localhost:6060/ in a web browser and\n    you should see something like this:\n    (If your docker daemon is running on another machine (or in a virtual machine),\n    you should replace localhost with the address of that machine. If you\'re\n    using boot2docker on OS X or Windows you can find\n    that address with boot2docker ip.)\n    Now that we\'ve verified that the image works, shut down the running container\n    from another terminal window:\n  $ docker stop test\n  Create a repository on Docker Hub\n    Docker Hub, the container registry from which we\n    pulled the golang image earlier, offers a feature called\n    Automated Builds that builds\n    images from a GitHub or BitBucket repository.\n    By committing the Dockerfile\n    to the repository and creating an\n    automated build\n    for it, anyone with Docker installed can download and run our image with a\n    single command. (We will see the utility of this in the next section.)\n    To set up an Automated Build, commit the Dockerfile to your repo on\n    GitHub or BitBucket,\n    create an account on Docker Hub, and follow the instructions for\n    creating an Automated Build.\n    When you\'re done, you can run your container using the name of the automated build:\n  $ docker run goexample/outyet\n    (Replace goexample/outyet with the name of the automated build you created.)\n  Deploy the container to Google Compute Engine\n    Google provides\n    container-optimized Google Compute Engine images\n    that make it easy to spin up a virtual machine running an arbitrary Docker container.\n    On startup, a program running on the instance reads a configuration file that\n    specifies which container to run, fetches the container image, and runs it.\n    Create a containers.yaml\n    file that specifies the docker image to run and the ports to expose:\n  version: v1beta2\ncontainers:\n- name: outyet\n  image: goexample/outyet\n  ports:\n  - name: http\n    hostPort: 80\n    containerPort: 8080\n    (Note that we\'re publishing the container\'s port 8080 as external port 80,\n    the default port for serving HTTP traffic. And, again, you should replace\n    goexample/outyet with the name of your Automated Build.)\n    Use the gcloud tool\n    to create a VM instance running the container:\n  $ gcloud compute instances create outyet \\\n    --image container-vm-v20140925 \\\n    --image-project google-containers \\\n    --metadata-from-file google-container-manifest=containers.yaml \\\n    --tags http-server \\\n    --zone us-central1-a \\\n    --machine-type f1-micro\n    The first argument (outyet) specifies the instance name, a convenient label\n    for administrative purposes.\n    The --image and --image-project flags specify the special\n    container-optimized system image to use (copy these flags verbatim).\n    The --metadata-from-file flag supplies your containers.yaml file to the VM.\n    The --tags flag tags your VM instance as an HTTP server, adjusting the\n    firewall to expose port 80 on the public network interface.\n    The --zone and --machine-type flags specify the zone in which to run the VM\n    and the type of machine to run. (To see a list of machine types and the zones,\n    run gcloud compute machine-types list.)\n    Once this has completed, the gcloud command should print some information about\n    the instance. In the output, locate the networkInterfaces section to find the\n    instance\'s external IP address. Within a couple of minutes you should be able\n    to access that IP with your web browser and see the \"Has Go 1.4 been released\n    yet?\" page.\n    (To see what\'s happening on the new VM instance you can ssh into it with\n    gcloud compute ssh outyet. From there, try sudo docker ps to see which\n    Docker containers are running.)\n  Learn more\n    This is just the tip of the iceberg—there\'s a lot more you can do with Go, Docker, and Google Compute Engine.\n    To learn more about Docker, see their extensive documentation.\n    To learn more about Docker and Go, see the official golang Docker Hub repository and Kelsey Hightower\'s Optimizing Docker Images for Static Go Binaries.\n    To learn more about Docker and Google Compute Engine,\n    see the Container-optimized VMs page\n    and the google/docker-registry Docker Hub repository.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/docker','2014-09-26','2017-05-07 13:49:59.452576'),(483,'Go at Google I/O and Gopher SummerFest','Francesc Campoy','		Go at Google I/O and Gopher SummerFest\n		6 October 2014\n		\n  Introduction\n    The week of June 23rd was a good week for gophers in San Francisco. Go was a big\n    part of Google I/O on Wednesday and\n    Thursday, and on Monday we took advantage of the large gopher population to run\n    the Go SummerFest, a\n    special instance of the GoSF meetup. This\n    blog post is a recap of both events.\n  Gopher SummerFest\n    On the Monday, more than 200 gophers gathered at the Google office in San\n    Francisco to hear a series of talks:\n    The State of Go, (slides and video) by Andrew Gerrand.\n    I was wrong, again! (slides and video), by Derek Collison.\n    Go at Splice (slides), by Matt Aimonetti\n    Quick testing with quick (slides), by Evan Shaw\n    Something about Go (no slides), by Blake Mizerany.\n    More comments and pictures from the event are available on the\n    meetup event page.\n  Go at Google I/O\n    On the Wednesday and Thursday, Go was at Google I/O in two different\n    formats: the Go booth in the sandbox area and the Go code labs available in the\n    code lab area and all around the world through\n    I/O Extended.\n  The Go booth\n    The Go booth was part of the Developer Sandbox area.\n    For the two days of the conference, some gophers from Google and other\n    companies gave a series of talks and demonstrations. The talks were not\n    recorded, but the slides and some screencasts and blog posts will be shared\n    soon.\n    Organizing Go Code, by David Crawshaw. (slides)\n    Testing Techniques, by Andrew Gerrand. (video and slides)\n    Go for Java Developers, by Francesc Campoy. (slides)\n    Camlistore: Android, ARM, App Engine, Everywhere, by Brad Fitzpatrick. (slides)\n    Go Compilation Complexities, by Ian Lance Taylor. (slides)\n    SourceGraph: a Code Search Engine in Go, by Quinn Slack. (video and slides)\n    We also organized Q&amp;A sessions and lightning talks by members of the Go\n    community:\n    Brad Rydzewski talked about his project drone.io.\n    Barak Michener presented Cayley, an open source graph database.\n    Matt Aimonetti discussed how Go is used at Splice.\n    Sugu Sougoumarane talked about how vitess solved scalability problems at YouTube. (video)\n  The Go code lab\n    This year attendees of Google I/O had a code lab area with self-service\n    computers where they could sit and learn Go. The code labs were also available\n    to anyone through the Google I/O extended brand. You can try it yourself at\n    io2014codelabs.appspot.com.\n  Conclusion\n    Thanks to the organizers, speakers, and attendees who helped make these events a\n    great success. See you next year. (Or at dotGo this week!)\n		\n			By Francesc Campoy\n		\n	','https://blog.golang.org/io2014','2014-10-06','2017-05-07 13:49:59.484667'),(484,'Half a decade with Go','Andrew Gerrand','		Half a decade with Go\n		10 November 2014\n		\n    Five years ago we launched the Go project. It seems like only yesterday that we\n    were preparing the initial public release: our\n    website was\n    a lovely shade of yellow, we were calling Go a \"systems language\", and you had\n    to terminate statements with a semicolon and write Makefiles to build your\n    code. We had no idea how Go would be received. Would people share our vision\n    and goals? Would people find Go useful?\n    At launch, there was a flurry of attention. Google had produced a new\n    programming language, and everyone was eager to check it out. Some programmers\n    were turned off by Go\'s conservative feature set—at first glance they saw\n    \"nothing to see here\"—but a smaller group saw the beginnings of an ecosystem\n    tailored to their needs as working software engineers. These few would form the\n    kernel of the Go community.\n    Gopher illustration by Renee French\n    After the initial release, it took us a while to properly communicate the\n    goals and design ethos behind Go. Rob Pike did so eloquently in his 2012 essay\n    Go at Google: Language Design in the Service of Software Engineering and\n    more personally in his blog post\n    Less is exponentially more.\n    Andrew Gerrand\'s\n    Code that grows with grace\n    (slides) and\n    Go for Gophers\n    (slides) give a\n    more in-depth, technical take on Go\'s design philosophy.\n    Over time, the few became many. The turning point for the project was the\n    release of Go 1 in March 2012, which provided a stable language and standard\n    library that developers could trust. By 2014, the project had hundreds of core\n    contributors, the ecosystem had countless libraries and tools\n    maintained by thousands of developers, and the greater community had\n    many passionate members (or, as we call them, \"gophers\"). Today, by our current\n    metrics, the Go community is growing faster than we believed possible.\n    Where can those gophers be found? They are at the many Go events that are\n    popping up around the world. This year we saw several dedicated Go conferences:\n    the inaugural GopherCon and\n    dotGo conferences in Denver and Paris, the\n    Go DevRoom at FOSDEM and two more\n    instances of the biannual GoCon conference\n    in Tokyo. At each event, gophers from around the globe eagerly presented their\n    Go projects. For the Go team, it is very satisfying to meet so many programmers\n    that share our vision and excitement.\n    More than 1,200 gophers attended GopherCon in Denver and dotGo in Paris.\n    There are also dozens of community-run\n    Go User Groups spread across cities\n    worldwide. If you haven\'t visited your local group, consider going along. And\n    if there isn\'t a group in your area, maybe you should\n    start one?\n    Today, Go has found a home in the cloud. Go arrived as the industry underwent a\n    tectonic shift toward cloud computing, and we were thrilled to see it quickly\n    become an important part of that movement. Its simplicity, efficiency, built-in\n    concurrency primitives, and modern standard library make it a great fit for\n    cloud software development (after all, that\'s what it was designed for).\n    Significant open source cloud projects like\n    Docker and\n    Kubernetes have been\n    written in Go, and infrastructure companies like Google, CloudFlare, Canonical,\n    Digital Ocean, GitHub, Heroku, and Microsoft are now using Go to do some heavy\n    lifting.\n    So, what does the future hold? We think that 2015 will be Go\'s biggest year yet.\n    Go 1.4—in addition to its new features and fixes—lays\n    the groundwork for a new low-latency garbage collector and support for running\n    Go on mobile devices. It is due to be released on December 1st 2014.\n    We expect the new GC to be available in Go 1.5, due June 1st 2015, which will\n    make Go appealing for a broader range of applications.\n    We can\'t wait to see where people take it.\n    And there will be more great events, with GothamGo in\n    New York (15 Nov), another Go DevRoom at FOSDEM in Brussels (Jan 31 and Feb 1;\n    get involved!),\n    GopherCon India in Bengaluru (19-21 Feb),\n    the original GopherCon back at Denver in July, and\n    dotGo on again at Paris in November.\n    The Go team would like to extend its thanks to all the gophers out there.\n    Here\'s to the next five years.\n    To celebrate 5 years of Go, over the coming month the\n    Gopher Academy\n    will publish a series of articles by prominent Go users. Be sure to check out\n    their blog\n    for more Go action.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/5years','2014-11-10','2017-05-07 13:49:59.518481'),(485,'Go 1.4 is released','Andrew Gerrand','		Go 1.4 is released\n		10 December 2014\n		\n    Today we announce Go 1.4, the fifth major stable release of Go, arriving six\n    months after our previous major release Go 1.3.\n    It contains a small language change, support for more operating systems\n    and processor architectures, and improvements to the tool chain and libraries.\n    As always, Go 1.4 keeps the promise of compatibility, and almost everything\n    will continue to compile and run without change when moved to 1.4.\n    For the full details, see the Go 1.4 release notes.\n    The most notable new feature in this release is official support for Android.\n    Using the support in the core and the libraries in the\n    golang.org/x/mobile repository,\n    it is now possible to write simple Android apps using only Go code.\n    At this stage, the support libraries are still nascent and under heavy development.\n    Early adopters should expect a bumpy ride, but we welcome the community to get involved.\n    The language change is a tweak to the syntax of for-range loops.\n    You may now write \"for range s {\" to loop over each item from s,\n    without having to assign the value, loop index, or map key.\n    See the release notes for details.\n    The go command has a new subcommand, go generate, to automate the running of\n    tools to generate source code before compilation.\n    For example, it can be used to automate the generation of String methods for\n    typed constants using the\n    new stringer tool.\n    For more information, see the design document.\n    Most programs will run about the same speed or slightly faster in 1.4 than in\n    1.3; some will be slightly slower.\n    There are many changes, making it hard to be precise about what to expect.\n    See the release notes for more discussion.\n    And, of course, there are many more improvements and bug fixes.\n    In case you missed it, a few weeks ago the sub-repositories were moved to new locations.\n    For example, the go.tools packages are now imported from \"golang.org/x/tools\".\n    See the announcement post for details.\n    This release also coincides with the project\'s move from Mercurial to Git (for\n    source control), Rietveld to Gerrit (for code review), and Google Code to\n    Github (for issue tracking and wiki).\n    The move affects the core Go repository and its sub-repositories.\n    You can find the canonical Git repositories at\n    go.googlesource.com,\n    and the issue tracker and wiki at the\n    golang/go GitHub repo.\n    While development has already moved over to the new infrastructure,\n    for the 1.4 release we still recommend that users who\n    install from source\n    use the Mercurial repositories.\n    For App Engine users, Go 1.4 is now available for beta testing.\n    See the announcement for details.\n    From all of us on the Go team, please enjoy Go 1.4, and have a happy holiday season.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go1.4','2014-12-10','2017-05-07 13:49:59.538236'),(486,'Participate in the 2016 Go User Survey and Company Questionnaire','Steve Francia','		Participate in the 2016 Go User Survey and Company Questionnaire\n		13 December 2016\n		\n  The Go project wants to hear from you!\n    The Go project wants to hear from you!  Our goal is to create the best language\n    for developing simple, reliable, scalable software.  We are asking you to help\n    by participating in a survey and if applicable, a company questionnaire.\n  The Go User Survey\n    Who: If you use Go, have ever used Go, have ever stopped using Go, or have\n    any interest in the language, please help by sharing your feedback to improve\n    Go for you and your fellow Gophers.\n    Where: Please take this 20-minute survey by December 22nd: Go User Survey 2016\n    The survey is anonymous and confidential.\n    Why: The Go project leadership depends on your feedback to guide the future\n    of the Go project. Your responses will help to understand how we are doing and\n    help plan improvements for the Go language, libraries, and tools.\n    After the survey closes, we will publish the anonymous aggregate results to the Go blog.\n  The Go Company Questionnaire\n    Who: If you are in a position to share details like “company name”, “if your\n    company is hiring Go developers”, and “reasons your team or company adopted Go”\n    then please help us by taking this questionnaire. We only need one response per\n    company (or department for larger companies).\n    Where: Please take this 5-minute questionnaire by December 22nd: Go Company Questionnaire 2016\n    The questionnaire is confidential, but not anonymous.\n    Why: The Go project would like to better understand the companies using Go.\n    After the questionnaire closes the Go project leadership will use this\n    information to better understand how companies are utilizing Go and in what\n    ways the project can improve their experience. If you participate we may reach\n    out for further information.\n  Spread the word!\n    Please help us spread the word by sharing this post on your social network\n    feeds, at meetups, around your office and in other communities.\n    The survey &amp; questionnaire close by December 22nd so please share today.\n		\n			By Steve Francia\n		\n	','https://blog.golang.org/survey2016','2016-12-13','2017-05-07 13:49:59.551736'),(487,'Go 2016 Survey Results','Steve Francia, for the Go team','		Go 2016 Survey Results\n		6 March 2017\n		\n  Thank you\n    This post summarizes the result of our December 2016 user survey along with our commentary and insights.\n    We are grateful to everyone who provided their feedback through the survey to help shape the future of Go.\n  Programming background\n    Of the 3,595 survey respondents, 89% said they program in Go at work or outside of work,\n    with 39% using Go both at home and at work, 27% using Go only at home, and 23% using Go only at work.\n    We asked about the areas in which people work.\n    63% said they work in web development, but only 9% listed web development alone.\n    In fact, 77% chose two or more areas, and 53% chose three or more.\n    We also asked about the kinds of programs people write in Go.\n    63% of respondents write command-line programs, 60% write API or RPC services, and 52% write web services.\n    Like in the previous question, most made multiple choices, with 85% choosing two or more and 72% choosing three or more.\n    We asked about people’s expertise and preference among programming languages.\n    Unsurprisingly, Go ranked highest among respondents’ first choices in both expertise (26%) and preference (62%).\n    With Go excluded, the top five first choices for language expertise were\n    Python (18%), Java (17%), JavaScript (13%), C (11%), and PHP (8%);\n    and the top five first choices for language preference were\n    Python (22%), JavaScript (10%), C (9%), Java (9%), and Ruby (7%).\n    Go is clearly attracting many programmers from dynamic languages.\np.note {\n  font-size: 0.80em;\n  font-family: \"Helvetica Neue\", Arial, sans-serif; /* Helvetica on Mac aka sans-serif has broken U+2007 */\n}\nThe following apply to me: (multiple choice)\n2,386 (66%)\nI program in Go outside of work\n2,235 (62%)\nI program at work in Go\n2,004 (56%)\nI program at work in another language\n618 (17%)\nI manage a programming team\n337  (9%)\nI am a student\n78  (2%)\nOther\n10  (0%)\nNo response\nReading the data: This question was “multiple choice,” so the percentages add up to well over 100%. All graphs in this post show both the total count and the corresponding percentage of the 3,595 surveys completed.\nI work in the following areas: (multiple choice)\n2,272 (63%)\nWeb development\n1,359 (38%)\nSystems programming\n1,251 (35%)\nDevOps\n1,169 (33%)\nNetwork programming\n1,006 (28%)\nDatabases\n533 (15%)\nMobile\n490 (14%)\nDesktop/GUI applications\n457 (13%)\nSecurity\n435 (12%)\nData Science\n417 (12%)\nFinance/Commerce\n394 (11%)\nEmbedded devices/Internet of Things\n379 (11%)\nAcademic/Scientific/Numeric\n228  (6%)\nGaming\n238  (7%)\nOther\n74  (2%)\nNo response\nI\'ve used Go for: (single choice)\n432 (12%)\nLess than 3 months\n1,009 (28%)\n3 - 12 months\n829 (23%)\n13 - 24 months\n903 (25%)\n2 - 4 years\n321  (9%)\n4+ years\n77  (2%)\nI\'ve never used Go\n24  (1%)\nNo response\nI write the following in Go: (multiple choice)\n2,247 (63%)\nA runnable/interactive program (CLI)\n2,174 (60%)\nAPI/RPC services (returning non-HTML)\n1,886 (52%)\nWeb services (returning HTML)\n1,583 (44%)\nAgents and daemons (e.g, monitoring)\n1,417 (39%)\nLibraries or Frameworks\n1,209 (34%)\nData processing (pipeline, aggregation)\n1,120 (31%)\nAutomation/scripts (e.g, deployment, configuration management)\n107  (3%)\nI don\'t write in Go\n137  (4%)\nOther\n45  (1%)\nNo response\nI write in Go: (single choice)\n1,567 (44%)\nAs part of my daily routine\n1,054 (29%)\nWeekly\n486 (14%)\nInfrequently\n368 (10%)\nMonthly\n77  (2%)\nI\'ve never written in Go\n43  (1%)\nNo response\nRank the following languages in terms of your expertise: (ordered choice, up to 5)\n3,111 (26, 26, 19, 10, 5%)\nGo\n2,048 (8, 15, 14, 11, 8%)\nJavaScript\n1,896 (12, 12, 10, 10, 7%)\nPython\n1,618 (13, 8, 8, 8, 8%)\nJava\n1,512 (8, 8, 9, 9, 7%)\nC\n1,064 (2, 4, 7, 8, 8%)\nBash\n1,039 (5, 5, 7, 6, 6%)\nC++\n830 (6, 4, 4, 5, 4%)\nPHP\n668 (5, 4, 3, 4, 3%)\nRuby\n622 (5, 3, 3, 4, 3%)\nC#\n294 (2, 1, 2, 2, 2%)\nPerl\n184 (1, 1, 1, 1, 1%)\nScala\n156 (0, 0, 1, 1, 2%)\nRust\n142 (0, 0, 1, 1, 1%)\nLua\n136 (0, 0, 0, 1, 2%)\nHaskell\n94 (0, 0, 0, 1, 1%)\nR\n93 (0, 0, 0, 1, 1%)\nClojure\n72 (0, 0, 0, 0, 1%)\nErlang\n18 (0, 0, 0, 0, 0%)\nJulia\n499 (2, 3, 3, 3, 3%)\nOther\n134 (3.7%)\nNo response\nReading the data: This question was “ordered choice.” The first, second, third, fourth, and fifth choices are displayed as progressively lighter sections of the bars. The total count shown next to the bar is for all choices; the percentage list shows how the choices are divided.\nRank the following languages in terms of your preference: (ordered choice, up to 5)\n3,248 (62, 19, 6, 2, 1%)\nGo\n1,796 (7, 17, 12, 9, 5%)\nPython\n1,482 (3, 9, 13, 10, 8%)\nJavaScript\n1,235 (2, 8, 9, 9, 6%)\nC\n1,167 (3, 7, 8, 7, 7%)\nJava\n809 (2, 4, 6, 6, 5%)\nC++\n647 (1, 3, 5, 5, 5%)\nBash\n563 (3, 5, 4, 3, 2%)\nRuby\n557 (2, 4, 4, 3, 2%)\nC#\n475 (2, 4, 3, 3, 2%)\nRust\n449 (1, 2, 3, 3, 3%)\nPHP\n278 (1, 2, 2, 2, 1%)\nHaskell\n215 (1, 1, 1, 1, 1%)\nPerl\n214 (1, 1, 1, 1, 1%)\nScala\n178 (0, 1, 2, 2, 1%)\nLua\n168 (0, 1, 1, 1, 1%)\nErlang\n156 (1, 1, 1, 1, 1%)\nClojure\n79 (0, 0, 0, 1, 1%)\nR\n43 (0, 0, 0, 0, 0%)\nJulia\n507 (3, 4, 4, 2, 1%)\nOther\n166 (4.6%)\nNo response\n  Go usage\n    Users are overwhelmingly happy with Go:\n    they agree that they would recommend Go to others by a ratio of 19:1,\n    that they’d prefer to use Go for their next project (14:1),\n    and that Go is working well for their teams (18:1).\n    Fewer users agree that Go is critical to their company’s success (2.5:1).\n    When asked what they like most about Go, users most commonly mentioned\n    Go’s simplicity, ease of use, concurrency features, and performance.\n    When asked what changes would most improve Go,\n    users most commonly mentioned generics, package versioning, and dependency management.\n    Other popular responses were GUIs, debugging, and error handling.\n    When asked about the biggest challenges to their own personal use of Go,\n    users mentioned many of the technical changes suggested in the previous question.\n    The most common themes in the non-technical challenges were convincing others to use Go\n    and communicating the value of Go to others, including management.\n    Another common theme was learning Go or helping others learn,\n    including finding documentation like getting-started walkthroughs,\n    tutorials, examples, and best practices.\n    Some representative common feedback, paraphrased for confidentiality:\n“The documentation is not clear enough for beginners.\nIt needs more examples and often assumes experience with other languages and various computer science topics.”\n“I want to use Go at work but struggle to convince my team to even try Go.”\n“I can’t get management approval to use Go; they don’t see its value and worry about adoption and finding developers.”\n    We appreciate the feedback given to identify these challenges faced by our users and community.\n    In 2017 we are focusing on addressing these issues and hope to make as many significant improvements as we can.\n    We welcome suggestions and contributions from the community in making these challenges into strengths for Go.\nTo what extent do you agree or disagree with the following statements:\n(strongly disagree, disagree, somewhat disagree, neutral, somewhat agree, agree, strongly agree)\n3,250 (2, 1, 1, 2, 5, 21, 57%)\nI would recommend using Go to others (19:1)\n3,219 (3, 1, 2, 4, 8, 19, 52%)\nI would prefer to use Go for my next new project (14:1)\n2,325 (1, 1, 1, 7, 8, 25, 22%)\nGo is working well for my team. (18:1)\n2,336 (4, 7, 3, 14, 12, 12, 12%)\nGo is critical to my company\'s success. (2.5:1)\nReading the data: This question asked how strongly the respondent agreed or disagreed with the statement.\nThe responses for each statement are displayed as sections of a single bar, from “strongly disagree” in deep red on the left end\nto “strongly agree” in deep blue on the right end. The bars use the same scale as the rest of the graphs,\nso they can (and do, especially later in the survey) vary in overall length due to lack of responses.\nThe ratio after the text compares the number of respondents who agreed (including “somewhat agree” and “strongly agree”)\nto those who disagreed (including “somewhat disagree” and “strongly disagree”).\nFor example, the ratio of respondents agreeing that they would recommend Go to respondents disagreeing was 19 to 1.\nWhat do you like most about Go?\n595 (17%)\nsimplicity\n543 (15%)\neasy\n523 (15%)\nconcurrency\n495 (14%)\nsimple\n454 (13%)\nfast\n293  (8%)\nsyntax\n287  (8%)\nstandard library\n286  (8%)\ntooling\n270  (8%)\nstatic\n266  (7%)\nperformance\n235  (7%)\nspeed\n202  (6%)\ninterfaces\n184  (5%)\nchannels\n183  (5%)\ncommunity\n180  (5%)\ngood\n177  (5%)\ncompilation\n177  (5%)\ngoroutines\n167  (5%)\nbinary\n156  (4%)\ngreat\n148  (4%)\ntools\n146  (4%)\ncompiled\n137  (4%)\ncompile\n127  (4%)\ntype\n124  (3%)\nsmall\n118  (3%)\nc\n114  (3%)\ngofmt\n114  (3%)\nlibraries\n88  (2%)\nclean\n87  (2%)\neasy to learn\n82  (2%)\ndeployment\n78  (2%)\nmemory\n78  (2%)\nstrong\n76  (2%)\nconcise\n76  (2%)\nsingle binary\n73  (2%)\nlow\n73  (2%)\nstatic typing\n71  (2%)\nbuild\n68  (2%)\neasy to read\n63  (2%)\nfast compilation\n56  (2%)\nsimple syntax\n55  (2%)\ntype system\n54  (2%)\nsimple language\n51  (1%)\neasy concurrency\n47  (1%)\nstatic binaries\n46  (1%)\ngo fmt\n45  (1%)\nfast compile\n43  (1%)\nsmall language\n41  (1%)\nerror handling\n39  (1%)\nconcurrency model\n39  (1%)\ngo routines\n38  (1%)\neasy to use\n38  (1%)\nstatically typed\n36  (1%)\ncross platform\n35  (1%)\nconcurrency primitives\n35  (1%)\ngoroutines channels\n33  (1%)\neasy to write\n27  (1%)\ngreat standard library\n23  (1%)\nease of use\n940 (26%)\nNo response\nReading the data: This question asked for write-in responses. The bars above show the fraction of surveys mentioning common words or phrases. Only words or phrases that appeared in twenty or more surveys are listed, and meaningless common words or phrases like “the” or “to be” are omitted. The displayed results do overlap: for example, the 287 responses that mentioned “standard library” do include the 27 listed separately that mentioned “great standard library.”\nHowever, nearly or completely redundant shorter entries are omitted: there are not twenty or more surveys that listed\n“standard” without mentioning “standard library,” so there is no separate entry for “standard.”\nWhat changes would improve Go most?\n572 (16%)\ngenerics\n451 (13%)\nmanagement\n330  (9%)\ndependency\n314  (9%)\npackage\n266  (7%)\ndependency management\n164  (5%)\nlibrary\n159  (4%)\ngui\n134  (4%)\npackage management\n134  (4%)\nvendoring\n128  (4%)\ndebugger\n126  (4%)\nlibraries\n122  (3%)\nstandard\n117  (3%)\ntype\n109  (3%)\nerror\n94  (3%)\nsystem\n89  (2%)\ntypes\n88  (2%)\nofficial\n85  (2%)\ntools\n84  (2%)\nc\n82  (2%)\ngopath\n78  (2%)\nperformance\n70  (2%)\nerror handling\n70  (2%)\nide\n69  (2%)\npackage manager\n66  (2%)\ndocumentation\n66  (2%)\nfaster\n64  (2%)\ngood\n63  (2%)\nsimple\n63  (2%)\ntool\n62  (2%)\nmobile\n60  (2%)\ndebugging\n57  (2%)\nbuild\n56  (2%)\npackages\n55  (2%)\neasier\n55  (2%)\nstandard library\n55  (2%)\ntooling\n54  (2%)\ninterface\n51  (1%)\ndependencies\n51  (1%)\ngeneric\n48  (1%)\nprogramming\n48  (1%)\nversioning\n47  (1%)\nsyntax\n45  (1%)\ncompile\n45  (1%)\nsolution\n44  (1%)\nframework\n43  (1%)\nexamples\n43  (1%)\ngc\n43  (1%)\ntype system\n42  (1%)\ngui library\n41  (1%)\ntemplates\n40  (1%)\nandroid\n40  (1%)\ncommunity\n40  (1%)\nfunction\n40  (1%)\nnative\n40  (1%)\nui\n40  (1%)\nweb\n39  (1%)\nfunctions\n21  (1%)\ncross platform\n1,215 (34%)\nNo response\nWhat is the biggest challenge you personally face using Go today?\n249 (6.9%)\nlack\n206 (5.7%)\nmanagement\n146 (4.1%)\nlibraries\n129 (3.6%)\ngenerics\n127 (3.5%)\ndependency management\n84 (2.3%)\nwork\n78 (2.2%)\npackage\n76 (2.1%)\nhard\n68 (1.9%)\ntime\n67 (1.9%)\ngood\n67 (1.9%)\njava\n66 (1.8%)\ngui\n61 (1.7%)\nweb\n60 (1.7%)\nc\n60 (1.7%)\ndebugging\n59 (1.6%)\nvendoring\n58 (1.6%)\nprojects\n56 (1.6%)\nlack of generics\n56 (1.6%)\nlibrary\n51 (1.4%)\ntype\n51 (1.4%)\nwrite\n50 (1.4%)\nfinding\n49 (1.4%)\nide\n49 (1.4%)\npackages\n48 (1.3%)\ndependencies\n46 (1.3%)\npackage management\n45 (1.3%)\ndebugger\n44 (1.2%)\nadoption\n42 (1.2%)\npeople\n41 (1.1%)\nlearning\n41 (1.1%)\nteam\n40 (1.1%)\nconvincing\n40 (1.1%)\ntools\n39 (1.1%)\nerror handling\n39 (1.1%)\ninterfaces\n39 (1.1%)\nother languages\n39 (1.1%)\nwriting\n38 (1.1%)\ninterface\n38 (1.1%)\nothers\n37 (1.0%)\npython\n35 (1.0%)\nfind\n35 (1.0%)\ngopath\n35 (1.0%)\nprogramming\n34 (0.9%)\ncan\'t\n34 (0.9%)\nstandard\n33 (0.9%)\nbuild\n33 (0.9%)\ntooling\n32 (0.9%)\ngeneric\n31 (0.9%)\nboilerplate\n30 (0.8%)\napplications\n30 (0.8%)\ndevelopers\n30 (0.8%)\nhaving\n30 (0.8%)\ntypes\n30 (0.8%)\nworking\n26 (0.7%)\nat work\n26 (0.7%)\nusing go\n22 (0.6%)\nno generics\n20 (0.6%)\nnot enough\n1,581 (44.0%)\nNo response\nIf it were not for the following reasons I would use Go more: (ordered choice, up to 3)\n1,485 (24, 14, 4%)\nI work on an existing project written in another language\n1,160 (16, 12, 4%)\nMy project / team / TL prefers another language\n841 (11, 8, 5%)\nGo isn’t an appropriate fit for what I’m working on (eg. iOS, JS)\n596 (6, 6, 4%)\nGo lacks critical libraries\n412 (6, 3, 2%)\nGo lacks critical features\n319 (3, 3, 3%)\nNot enough education or support resources for Go\n121 (1, 1, 1%)\nGo lacks critical performance\n374 (4, 3, 3%)\nOther\n1,042 (29%)\nNo response\nIf you desire, please elaborate on your reasons above.\n58 (1.6%)\nc\n58 (1.6%)\njava\n58 (1.6%)\nlibraries\n50 (1.4%)\npython\n47 (1.3%)\nweb\n45 (1.3%)\ngenerics\n45 (1.3%)\nwork\n40 (1.1%)\nprojects\n34 (0.9%)\nlanguages\n33 (0.9%)\nhard\n32 (0.9%)\nlack\n32 (0.9%)\nteam\n31 (0.9%)\nlibrary\n31 (0.9%)\npeople\n29 (0.8%)\ngui\n25 (0.7%)\ngood\n25 (0.7%)\nperformance\n24 (0.7%)\nmobile\n24 (0.7%)\nwritten\n23 (0.6%)\nprogramming\n23 (0.6%)\ntime\n22 (0.6%)\ngolang\n20 (0.6%)\ncompany\n20 (0.6%)\nexisting\n20 (0.6%)\ngreat\n20 (0.6%)\nphp\n20 (0.6%)\ntools\n3,033 (84.4%)\nNo response\n  Development and deployment\n    When asked which operating systems they develop Go on,\n    63% of respondents say they use Linux, 44% use MacOS, and 19% use Windows,\n    with multiple choices allowed and 49% of respondents developing on multiple systems.\n    The 51% of responses choosing a single system split into\n    29% on Linux, 17% on MacOS, 5% on Windows, and 0.2% on other systems.\n    Go deployment is roughly evenly split between privately managed servers and hosted cloud servers.\nI primarily develop Go on: (multiple choice)\n2,263 (63%)\nLinux\n1,592 (44%)\nMacOS\n682 (19%)\nWindows\n82  (2%)\nOther\n434 (12%)\nNo response\nMy preferred code editor: (ordered choice, up to 2)\n1,359 (25, 13%)\nVim\n814 (14, 9%)\nVSCode\n676 (10, 9%)\nAtom\n687 (13, 6%)\nIntelliJ\n655 (10, 8%)\nSublime Text\n305 (6, 2%)\nEmacs\n137 (2, 2%)\nVisual Studio\n153 (3, 2%)\nLiteIDE\n99 (1, 2%)\nEclipse\n37 (1, 1%)\nAcme\n238 (4, 3%)\nOther\n425 (12%)\nNo response\nHow satisfied are you with Go support in your preferred editor: (single choice)\n69 (1.9%)\nVery Dissatisfied\n52 (1.4%)\nDissatisfied\n164 (4.6%)\nSomewhat Dissatisfied\n134 (3.7%)\nNeither Satisfied or Unsatisfied\n609 (16.9%)\nSomewhat Satisfied\n1,258 (35.0%)\nSatisfied\n838 (23.3%)\nVery Satisfied\n471 (13.1%)\nNo response\nWhat one addition would make the biggest improvement to Go editing in your preferred editor?\n180 (5.0%)\ndebugging\n136 (3.8%)\ndebugger\n116 (3.2%)\nrefactoring\n79 (2.2%)\nintegration\n72 (2.0%)\ntools\n68 (1.9%)\ncompletion\n58 (1.6%)\neditor\n46 (1.3%)\ndebug\n43 (1.2%)\ncode completion\n43 (1.2%)\nwork\n41 (1.1%)\nvim\n40 (1.1%)\nautocomplete\n40 (1.1%)\nvscode\n37 (1.0%)\npackage\n37 (1.0%)\nplugin\n36 (1.0%)\ndefinition\n36 (1.0%)\neasier\n36 (1.0%)\ngood\n36 (1.0%)\nide\n36 (1.0%)\nintellij\n35 (1.0%)\nfaster\n35 (1.0%)\nfunction\n34 (0.9%)\natom\n34 (0.9%)\ninterface\n33 (0.9%)\nvim-go\n32 (0.9%)\ngopath\n31 (0.9%)\nintegrated\n30 (0.8%)\nworking\n29 (0.8%)\nauto\n28 (0.8%)\nrefactoring support\n27 (0.8%)\ndelve\n27 (0.8%)\ntype\n26 (0.7%)\nguru\n26 (0.7%)\nsyntax\n25 (0.7%)\nerror\n25 (0.7%)\nmethod\n25 (0.7%)\npackages\n25 (0.7%)\nplugins\n24 (0.7%)\ncompile\n24 (0.7%)\njump\n23 (0.6%)\nfeatures\n23 (0.6%)\nfind\n23 (0.6%)\ngoimports\n23 (0.6%)\nnavigation\n23 (0.6%)\nperformance\n23 (0.6%)\nrefactoring tools\n23 (0.6%)\nworks\n22 (0.6%)\nautocompletion\n22 (0.6%)\ndebugging support\n22 (0.6%)\nerrors\n22 (0.6%)\ngofmt\n22 (0.6%)\nrun\n21 (0.6%)\nhighlighting\n21 (0.6%)\nsave\n21 (0.6%)\nsetup\n21 (0.6%)\nvisual\n20 (0.6%)\ndocumentation\n20 (0.6%)\ngreat\n2,291 (63.7%)\nNo response\nMy team deploys Go/non-Go programs to: (multiple choice)\n1,489 (41%)\nSelf/Company Owned Servers (Go)\n1,714 (48%)\n(non-Go)\n928 (26%)\nAWS EC2\n1,122 (31%)\n503 (14%)\nNone\n249  (7%)\n412 (11%)\nDigital Ocean\n360 (10%)\n292  (8%)\nAWS Container\n343 (10%)\n221  (6%)\nGoogle Compute Engine\n186  (5%)\n188  (5%)\nGoogle App Engine\n94  (3%)\n161  (4%)\nGoogle Container Engine (GKE)\n115  (3%)\n121  (3%)\nHeroku\n185  (5%)\n114  (3%)\nMicrosoft Azure\n210  (6%)\n104  (3%)\nLinode\n100  (3%)\n94  (3%)\nAWS Lambda\n233  (6%)\n301  (8%)\nOther\n297  (8%)\n639 (18%)\nNo response\n660 (18%)\n  Working Effectively\n    We asked how strongly people agreed or disagreed with various statements about Go.\n    Users most agreed that Go’s performance meets their needs (57:1 ratio agree versus disagree),\n    that they are able to quickly find answers to their questions (20:1),\n    and that they are able to effectively use Go’s concurrency features (14:1).\n    On the other hand, users least agreed that they are able to effectively debug uses of Go’s concurrency features (2.7:1).\n    Users mostly agreed that they were able to quickly find libraries they need (7.5:1).\n    When asked what libraries are still missing, the most common request by far was a library for writing GUIs.\n    Another popular topic was requests around data processing, analytics, and numerical and scientific computing.\n    Of the 30% of users who suggested ways to improve Go’s documentation, the most common suggestion by far was more examples.\n    The primary sources for Go news are the Go blog, Reddit’s /r/golang and Twitter; there may be some bias here since these are also how the survey was announced.\n    The primary sources for finding answers to Go questions are the Go web site, Stack Overflow, and reading source code directly.\nTo what extent do you agree or disagree with the following statements:\n(strongly disagree, disagree, somewhat disagree, neutral, somewhat agree, agree, strongly agree)\n3,094 (1, 2, 5, 6, 27, 32, 12%)\nI have a good understanding of Go best practices. (9.6:1)\n3,083 (0, 1, 3, 4, 17, 41, 20%)\nI am able to quickly find answers to my questions. (20:1)\n3,053 (0, 0, 1, 2, 7, 32, 42%)\nGo\'s performance meets my needs. (57:1)\n2,523 (1, 3, 5, 14, 15, 26, 8%)\nGo\'s support for language interoperability meets my needs. (6.0:1)\n3,049 (1, 2, 6, 7, 24, 34, 11%)\nI am able to quickly find libraries that I need. (7.5:1)\n3,083 (1, 2, 4, 5, 18, 37, 20%)\nGo language, library, and tool documentation meet my needs. (11:1)\nWhat Go libraries do you need that aren\'t available today?\n208 (5.8%)\ngui\n144 (4.0%)\nlibrary\n121 (3.4%)\nlibraries\n63 (1.8%)\nnative\n60 (1.7%)\nui\n53 (1.5%)\ngood\n33 (0.9%)\norm\n33 (0.9%)\nstandard\n33 (0.9%)\nweb\n32 (0.9%)\nframework\n32 (0.9%)\ngui library\n31 (0.9%)\nmobile\n28 (0.8%)\nandroid\n28 (0.8%)\ndatabase\n28 (0.8%)\ndesktop\n28 (0.8%)\nlibs\n28 (0.8%)\nsql\n26 (0.7%)\ncross platform\n25 (0.7%)\nprocessing\n25 (0.7%)\nxml\n24 (0.7%)\napi\n24 (0.7%)\nmachine learning\n24 (0.7%)\nofficial\n24 (0.7%)\nwindows\n23 (0.6%)\nsoap\n22 (0.6%)\ntoolkit\n21 (0.6%)\npdf\n21 (0.6%)\npython\n20 (0.6%)\nbindings\n20 (0.6%)\ngraphics\n20 (0.6%)\npackage\n2,498 (69.5%)\nNo response\nWhat changes would most improve the Go documentation?\n512 (14%)\nexamples\n300  (8%)\nmore examples\n134  (4%)\ndocumentation\n69  (2%)\nexample\n62  (2%)\ndocs\n49  (1%)\ngodoc\n34  (1%)\nusage\n32  (1%)\nfunctions\n32  (1%)\npackage\n31  (1%)\ngood\n29  (1%)\nfunction\n29  (1%)\ngreat\n29  (1%)\npackages\n29  (1%)\nsearch\n28  (1%)\ncases\n26  (1%)\nbest practices\n26  (1%)\nlibraries\n23  (1%)\ndoc\n23  (1%)\nmore example\n22  (1%)\ncode examples\n21  (1%)\nsyntax\n20  (1%)\ninterface\n2,532 (70%)\nNo response\nTo what extent do you agree or disagree with the following statements:\n(strongly disagree, disagree, somewhat disagree, neutral, somewhat agree, agree, strongly agree)\n3,002 (1, 2, 6, 7, 23, 34, 11%)\nI am able to effectively diagnose bugs in my Go programs. (7.2:1)\n2,725 (1, 2, 6, 13, 22, 24, 7%)\nI am able to effectively diagnose performance issues in my Go programs. (5.8:1)\n2,932 (1, 2, 3, 5, 17, 33, 22%)\nI am able to effectively use Go\'s concurrency features (goroutines, channels, select). (14:1)\n2,801 (2, 5, 11, 14, 23, 18, 5%)\nI am able to effectively debug uses of Go\'s concurrency features (goroutines, channels, select). (2.7:1)\nRank the following in terms of where you get Go answers from: (ordered choice, up to 5)\n2,226 (23, 18, 12, 7, 3%)\nStack Overflow\n2,101 (30, 15, 8, 4, 1%)\ngolang.org\n1,814 (13, 17, 12, 7, 2%)\nReading source code (e.g., standard library, open-source packages)\n1,200 (3, 8, 12, 7, 4%)\nGitHub\n854 (3, 7, 7, 5, 3%)\ngolang-nuts mailing list (groups.google.com/d/forum/golang-nuts)\n682 (2, 3, 5, 5, 3%)\nReddit (r/golang)\n630 (3, 4, 5, 3, 2%)\nCoworkers\n334 (2, 2, 2, 2, 2%)\nGopher Slack (invite.slack.golangbridge.org)\n214 (1, 1, 2, 1, 1%)\nFriends\n161 (0, 0, 1, 1, 1%)\nTwitter\n156 (1, 1, 1, 1, 0%)\nIRC\n126 (0, 1, 1, 1, 1%)\nGo Forum (forum.golangbridge.org)\n262 (2, 2, 1, 1, 1%)\nOther\n643 (18%)\nNo response\nRank the following in terms of where you get Go news from: (ordered choice, up to 5)\n1,659 (17, 14, 9, 4, 2%)\nblog.Golang.org\n1,153 (17, 8, 4, 2, 1%)\nReddit (r/golang)\n1,053 (14, 8, 4, 3, 1%)\nTwitter\n903 (6, 8, 6, 3, 1%)\nHacker News\n777 (9, 6, 4, 2, 0%)\nGolangweekly.com\n633 (2, 6, 5, 4, 1%)\nCommunity Blogs\n430 (2, 3, 4, 2, 1%)\nGitHub\n418 (3, 3, 3, 2, 1%)\ngolang-nuts mailing list (groups.google.com/d/forum/golang-nuts)\n394 (3, 3, 3, 1, 1%)\nCoworkers\n212 (1, 1, 2, 1, 1%)\nGopher Slack (invite.slack.golangbridge.org)\n203 (1, 2, 1, 1, 1%)\nGolangnews.com\n199 (1, 2, 1, 1, 1%)\ngolang-announce (groups.google.com/d/forum/golang-announce)\n176 (1, 1, 1, 1, 1%)\nGo Time podcast\n65 (0, 0, 0, 1, 0%)\nGo Forum (forum.golangbridge.org)\n42 (0, 0, 0, 0, 0%)\nFacebook\n160 (1, 1, 1, 0, 0%)\nOther\n747 (21%)\nNo response\nI have attended: (multiple choice)\n1,315 (37%)\nNone\n879 (24%)\nA Go meetup\n523 (15%)\nA Go themed conference (GopherCon, GothamGo, etc)\n276  (8%)\nA Go remote meetup / online event\n186  (5%)\nGo training\n165  (5%)\nA technical conference for it\'s Go content\n43  (1%)\nA GoBridge event\n37  (1%)\nA Women Who Go event\n65  (2%)\nOther\n993 (28%)\nNo response\n  The Go Project\n    55% of respondents expressed interest in contributing in some way to the Go community and projects.\n    Unfortunately, relatively few agreed that they felt welcome to do so (3.3:1) and even fewer felt that the process was clear (1.3:1).\n    In 2017, we intend to work on improving the contribution process and to continue to work to make all contributors feel welcome.\n    Respondents agree that they are confident in the leadership of the Go project (9:1),\n    but they agree much less that the project leadership understands their needs (2.6:1),\n    and they agree even less that they feel comfortable approaching project leadership with questions and feedback (2.2:1).\n    In fact, these were the only questions in the survey for which more than half of respondents\n    did not mark “somewhat agree”, “agree”, or “strongly agree” (many were neutral or did not answer).\n    We hope that the survey and this blog post convey to those of you\n    who are aren’t comfortable reaching out that the Go project leadership is listening.\n    Throughout 2017 we will be exploring new ways to engage with users to better understand their needs.\nI contribute to open source projects written in Go: (single choice)\n1,227 (34%)\nInfrequently\n890 (25%)\nNever\n345 (10%)\nMonthly\n295  (8%)\nWeekly\n234  (7%)\nAs part of my daily routine\n604 (17%)\nNo response\nI have contributed or am interested in contributing in the following ways to the Go community and Projects: (multiple choice)\n892 (25%)\nStandard library\n663 (18%)\nTools (go guru, go vet, go doc, etc)\n602 (17%)\nTutorials\n560 (16%)\nDocumentation\n557 (15%)\nCommunity support via Stack Overflow, Slack, mailing list, etc  \n472 (13%)\nCommunity involvement (workgroups, meetup attendance)\n440 (12%)\nBeing a technical mentor\n374 (10%)\nToolchain (compiler, linker, etc)\n275  (8%)\nGo Project maintenance (issue triage)\n246  (7%)\nEvent planning (meetup, conference, etc)\n236  (7%)\nLanguage translation\n165  (5%)\nGeneral UX &amp; Design contributions\n154  (4%)\ngolang.org website (code, UX, IA, content, etc)\n92  (3%)\nOther\n1,621 (45%)\nNo response\nTo what extent do you agree or disagree with the following statements:\n(strongly disagree, disagree, somewhat disagree, neutral, somewhat agree, agree, strongly agree)\n2,091 (1, 3, 5, 19, 10, 14, 6%)\nI feel welcome to contribute to Go (compiler, standard library, documentation, website) (3.3:1)\n2,168 (3, 7, 9, 16, 10, 11, 4%)\nThe process of contributing to the Go project is clear to me (1.3:1)\n1,900 (1, 2, 5, 22, 8, 11, 3%)\nThe Go project leadership understands my needs (2.6:1)\n2,114 (2, 4, 6, 18, 10, 14, 5%)\nI feel comfortable approaching the Go project leadership with questions and feedback (2.2:1)\n2,374 (1, 1, 3, 12, 9, 24, 15%)\nI am confident in the leadership of Go (9.0:1)\nWhat is the biggest challenge facing the Go project today?\n71 (2.0%)\ncommunity\n68 (1.9%)\ngoogle\n63 (1.8%)\ngenerics\n62 (1.7%)\nmanagement\n49 (1.4%)\nadoption\n45 (1.3%)\nlack\n43 (1.2%)\nfeatures\n43 (1.2%)\npeople\n40 (1.1%)\ndependency management\n37 (1.0%)\njava\n32 (0.9%)\nlanguages\n31 (0.9%)\nkeeping\n29 (0.8%)\nc\n27 (0.8%)\ndevelopers\n27 (0.8%)\nleadership\n24 (0.7%)\ngood\n24 (0.7%)\nlibraries\n24 (0.7%)\npackage\n23 (0.6%)\nsimple\n21 (0.6%)\ncore\n21 (0.6%)\nfeature\n20 (0.6%)\nprogramming\n20 (0.6%)\nteam\n2,771 (77.1%)\nNo response\n  Community\n    At the end of the survey, we asked some demographic questions.\n    The country distribution of responses roughly matches the country distribution of site visits to golang.org,\n    but the responses under-represent some Asian countries.\n    In particular, India, China, and Japan each accounted for about 5% of the site visits to golang.org in 2016\n    but only 3%, 2%, and 1% of survey responses.\n    An important part of a community is making everyone feel welcome, especially people from under-represented demographics.\n    We asked an optional question about identification across a few diversity groups.\n    37% of respondents left the question blank and 12% of respondents chose “I prefer not to answer”,\n    so we cannot make many broad conclusions from the data.\n    However, one comparison stands out: the 9% who identified as underrepresented agreed\n    with the statement “I feel welcome in the Go community” by a ratio of 7.5:1,\n    compared with 15:1 in the survey as a whole.\n    We aim to make the Go community even more welcoming.\n    We support and are encouraged by the efforts of organizations like GoBridge and Women Who Go.\n    The final question on the survey was just for fun: what’s your favorite Go keyword?\n    Perhaps unsurprisingly, the most popular response was go, followed by defer, func, interface, and select.\nTo what extent do you agree or disagree with the following statements:\n(strongly disagree, disagree, somewhat disagree, neutral, somewhat agree, agree, strongly agree)\n2,701 (1, 1, 2, 11, 10, 31, 19%)\nI feel welcome in the Go community. (15:1)\nWhat changes would make the Go community more welcoming?\n115 (3.2%)\ncommunity\n52 (1.4%)\npeople\n32 (0.9%)\nr/golang\n31 (0.9%)\ngo community\n30 (0.8%)\ngoogle\n30 (0.8%)\nreddit\n24 (0.7%)\nwelcoming\n23 (0.6%)\nofficial\n23 (0.6%)\nopen\n22 (0.6%)\ncode of conduct\n21 (0.6%)\ngolang\n21 (0.6%)\nteam\n3,017 (83.9%)\nNo response\nIn which country do you currently reside? (single choice)\n928 (26%)\nUnited States of America\n253  (7%)\nGermany\n168  (5%)\nUnited Kingdom\n148  (4%)\nRussia\n119  (3%)\nFrance\n112  (3%)\nCanada\n91  (3%)\nIndia\n73  (2%)\nChina\n72  (2%)\nAustralia\n55  (2%)\nNetherlands\n54  (2%)\nSpain\n45  (1%)\nSweden\n43  (1%)\nPoland\n40  (1%)\nItaly\n36  (1%)\nBrazil\n36  (1%)\nSwitzerland\n35  (1%)\nUkraine\n27  (1%)\nJapan\n24  (1%)\nCzech Republic\n23  (1%)\nBelgium\n441 (12%)\nOther\n772 (21%)\nNo response\nWe want the Go community to be inclusive; we want to see how we\'re doing and how to improve.\nPlease select the groups you identify with: (multiple choice)\n1,499 (42%)\nI do not identify as part of an underrepresented group\n438 (12%)\nI prefer not to answer\n101  (3%)\nI identify as LGBTQIA\n95  (3%)\nI identify as ethnically or racially underrepresented\n77  (2%)\nI identify as neurodiverse or as having a disability\n49  (1%)\nI identify as a woman\n47  (1%)\nWrite-in: objection to the question.\n38  (1%)\nI identify as part of an underrepresented group, but I prefer not to specify\n34  (1%)\nI identify with an underrepresented group not listed.\n1,332 (37%)\nNo response\nJust for fun: What is your favorite Go keyword?\n854 (24%)\ngo\n455 (13%)\ndefer\n253  (7%)\nfunc\n240  (7%)\nselect\n227  (6%)\ninterface\n145  (4%)\nstruct\n139  (4%)\nchan\n129  (4%)\nrange\n67  (2%)\nfallthrough\n56  (2%)\nswitch\n53  (1%)\nfor\n48  (1%)\ntype\n47  (1%)\nmap\n44  (1%)\ngoto\n36  (1%)\nimport\n22  (1%)\nif\n20  (1%)\npackage\n19  (1%)\nvar\n17  (0%)\nconst\n14  (0%)\ncontinue\n13  (0%)\nreturn\n12  (0%)\nbreak\n3  (0%)\nelse\n2  (0%)\ncase\n2  (0%)\ndefault\n678 (19%)\nNo response\nIs there anything else you would like to share with us?\n95 (2.6%)\nthanks\n94 (2.6%)\ngreat\n86 (2.4%)\nthank you\n47 (1.3%)\nkeep up the good work\n47 (1.3%)\nprogramming\n43 (1.2%)\ncommunity\n39 (1.1%)\nc\n37 (1.0%)\nawesome\n33 (0.9%)\ni love\n31 (0.9%)\npeople\n29 (0.8%)\ngolang\n27 (0.8%)\ngreat work\n27 (0.8%)\njava\n27 (0.8%)\nlanguages\n26 (0.7%)\nfun\n26 (0.7%)\njob\n26 (0.7%)\ntime\n25 (0.7%)\nlove go\n24 (0.7%)\ngenerics\n24 (0.7%)\nteam\n23 (0.6%)\nprojects\n22 (0.6%)\nbest\n22 (0.6%)\nwish\n22 (0.6%)\nyears\n21 (0.6%)\nsimple\n2,886 (80.3%)\nNo response\n		\n			By Steve Francia, for the Go team\n		\n	','https://blog.golang.org/survey2016-results','2017-03-06','2017-05-07 13:49:59.583531'),(488,'Testable Examples in Go','Andrew Gerrand','		Testable Examples in Go\n		7 May 2015\n		\n  Introduction\n    Godoc examples are snippets of\n    Go code that are displayed as package documentation and that are verified by\n    running them as tests.\n    They can also be run by a user visiting the godoc web page for the package\n    and clicking the associated \"Run\" button.\n    Having executable documentation for a package guarantees that the information\n    will not go out of date as the API changes.\n    The standard library includes many such examples\n    (see the strings package,\n    for instance).\n    This article explains how to write your own example functions.\n  Examples are tests\n    Examples are compiled (and optionally executed) as part of a package\'s test\n    suite.\n    As with typical tests, examples are functions that reside in a package\'s\n    _test.go files.\n    Unlike normal test functions, though, example functions take no arguments\n    and begin with the word Example instead of Test.\n    The stringutil package\n    is part of the Go example repository.\n    Here\'s an example that demonstrates its Reverse function:\n  package stringutil_test\nimport (\n    \"fmt\"\n    \"github.com/golang/example/stringutil\"\n)\nfunc ExampleReverse() {\n    fmt.Println(stringutil.Reverse(\"hello\"))\n    // Output: olleh\n}\n    This code might live in example_test.go in the stringutil directory.\n    Godoc will present this example alongside the Reverse function\'s documentation:\n    Running the package\'s test suite, we can see the example function is executed\n    with no further arrangement from us:\n  $ go test -v\n=== RUN TestReverse\n--- PASS: TestReverse (0.00s)\n=== RUN: ExampleReverse\n--- PASS: ExampleReverse (0.00s)\nPASS\nok      github.com/golang/example/stringutil    0.009s\n  Output comments\n    What does it mean that the ExampleReverse function \"passes\"?\n    As it executes the example,\n    the testing framework captures data written to standard output\n    and then compares the output against the example\'s \"Output:\" comment.\n    The test passes if the test\'s output matches its output comment.\n    To see a failing example we can change the output comment text to something\n    obviously incorrect\n  func ExampleReverse() {\n    fmt.Println(stringutil.Reverse(\"hello\"))\n    // Output: golly\n}\n    and run the tests again:\n  $ go test\n--- FAIL: ExampleReverse (0.00s)\ngot:\nolleh\nwant:\ngolly\nFAIL\n    If we remove the output comment entirely\n  func ExampleReverse() {\n    fmt.Println(stringutil.Reverse(\"hello\"))\n}\n    then the example function is compiled but not executed:\n  $ go test -v\n=== RUN TestReverse\n--- PASS: TestReverse (0.00s)\nPASS\nok      github.com/golang/example/stringutil    0.009s\n    Examples without output comments are useful for demonstrating code that cannot\n    run as unit tests, such as that which accesses the network, \n    while guaranteeing the example at least compiles.\n  Example function names\n    Godoc uses a naming convention to associate an example function with a\n    package-level identifier.\n  func ExampleFoo()     // documents the Foo function or type\nfunc ExampleBar_Qux() // documents the Qux method of type Bar\nfunc Example()        // documents the package as a whole\n    Following this convention, godoc displays the ExampleReverse example\n    alongside the documentation for the Reverse function.\n    Multiple examples can be provided for a given identifier by using a suffix\n    beginning with an underscore followed by a lowercase letter.\n    Each of these examples documents the Reverse function:\n  func ExampleReverse()\nfunc ExampleReverse_second()\nfunc ExampleReverse_third()\n  Larger examples\n    Sometimes we need more than just a function to write a good example.\n    For instance, to demonstrate the sort package\n    we should show an implementation of sort.Interface. \n    Since methods cannot be declared inside a function body, the example must\n    include some context in addition to the example function.\n    To achieve this we can use a \"whole file example.\"\n    A whole file example is a file that ends in _test.go and contains exactly one\n    example function, no test or benchmark functions, and at least one other\n    package-level declaration.\n    When displaying such examples godoc will show the entire file.\n    Here is a whole file example from the sort package:\n  package sort_test\nimport (\n    \"fmt\"\n    \"sort\"\n)\ntype Person struct {\n    Name string\n    Age  int\n}\nfunc (p Person) String() string {\n    return fmt.Sprintf(\"%s: %d\", p.Name, p.Age)\n}\n// ByAge implements sort.Interface for []Person based on\n// the Age field.\ntype ByAge []Person\nfunc (a ByAge) Len() int           { return len(a) }\nfunc (a ByAge) Swap(i, j int)      { a[i], a[j] = a[j], a[i] }\nfunc (a ByAge) Less(i, j int) bool { return a[i].Age &lt; a[j].Age }\nfunc Example() {\n    people := []Person{\n        {\"Bob\", 31},\n        {\"John\", 42},\n        {\"Michael\", 17},\n        {\"Jenny\", 26},\n    }\n    fmt.Println(people)\n    sort.Sort(ByAge(people))\n    fmt.Println(people)\n    // Output:\n    // [Bob: 31 John: 42 Michael: 17 Jenny: 26]\n    // [Michael: 17 Jenny: 26 Bob: 31 John: 42]\n}\n    A package can contain multiple whole file examples; one example per file.\n    Take a look at the sort package\'s source code\n    to see this in practice.\n  Conclusion\n    Godoc examples are a great way to write and maintain code as documentation.\n    They also present editable, working, runnable examples your users can build on.\n    Use them!\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/examples','2015-05-07','2017-05-07 13:49:59.618052'),(489,'The Gopher Gala is the first worldwide Go hackathon','Francesc Campoy','		The Gopher Gala is the first worldwide Go hackathon\n		7 January 2015\n		\n    The Gopher Gala is the first Go hackathon at a\n    global scale and will take place from January 23rd through the 25th. The event\n    is organized by the community, supported by the Go team, and sponsored by\n    Google among others.\n  Fancy Gopher, by Renée French\n    You can read about the rules of the hackathon\n    here, but if you know about\n    Rails Rumble or\n    Node Knockout you already have a pretty good idea\n    of what to expect.\n    During this event gophers from all around the globe will form teams to build\n    great applications using Go as the main tool. Afterwards, all the participants\n    will vote for their favorite applications and the 20 highest voted applications\n    will be ranked by a jury of renowned names from the community, including some\n    members of the Go core team. More information on the judging phase can be found\n    here.\n    And in case you needed one more reason to\n    get involved there will be prizes!\n    \"What prizes?\" you ask. Well, that’s a\n    secret until January 10th but we’re pretty\n    sure you won’t be disappointed.\n		\n			By Francesc Campoy\n		\n	','https://blog.golang.org/gophergala','2015-01-07','2017-05-07 13:49:59.640119'),(490,'Go, Open Source, Community','Russ Cox','		Go, Open Source, Community\n		8 July 2015\n		\n  Welcome\n    [This is the text of my opening keynote at Gophercon 2015.\n    The video is available here.]\n    Thank you all for traveling to Denver to be here,\n    and thank you to everyone watching on video.\n    If this is your first Gophercon, welcome.\n    If you were here last year, welcome back.\n    Thank you to the organizers\n    for all the work it takes\n    to make a conference like this happen.\n    I am thrilled to be here and to be able to talk to all of you.\n    I am the tech lead for the Go project\n    and the Go team at Google.\n    I share that role with Rob Pike.\n    In that role, I spend a lot of time thinking about\n    the overall Go open source project,\n    in particular the way it runs,\n    what it means to be open source,\n    and the interaction between\n    contributors inside and outside Google.\n    Today I want to share with you\n    how I see the Go project as a whole\n    and then based on that explain\n    how I see the Go open source project\n    evolving.\n  Why Go?\n    To get started,\n    we have to go back to the beginning.\n    Why did we start working on Go?\n    Go is an attempt to make programmers more productive.\n    We wanted to improve the software development process\n    at Google,\n    but the problems Google has\n    are not unique to Google.\n    There were two overarching goals.\n    The first goal is to make a better language\n    to meet the challenges of scalable concurrency.\n    By scalable concurrency I mean\n    software that deals with many concerns simultaneously,\n    such as coordinating a thousand back end servers\n    by sending network traffic back and forth.\n    Today, that kind of software has a shorter name:\n    we call it cloud software.\n    It\'s fair to say that Go was designed for the cloud\n    before clouds ran software.\n    The larger goal is to make a better environment\n    to meet the challenges of scalable software development,\n    software worked on and used by many people,\n    with limited coordination between them,\n    and maintained for years.\n    At Google we have thousands of engineers\n    writing and sharing their code with each other,\n    trying to get their work done,\n    reusing the work of others as much as possible,\n    and working in a code base with a history\n    dating back over ten years.\n    Engineers often work on or at least look at\n    code originally written by someone else,\n    or that they wrote years ago,\n    which often amounts to the same thing.\n    That situation inside Google\n    has a lot in common with\n    large scale, modern open source development\n    as practiced on sites like GitHub.\n    Because of this,\n    Go is a great fit for open source projects,\n    helping them accept and manage\n    contributions from a large community\n    over a long period of time.\n    I believe much of Go\'s success is explained by the fact that\n    Go is a great fit for cloud software,\n    Go is a great fit for open source projects,\n    and, serendipitously, both of those are\n    growing in popularity and importance\n    in the software industry.\n    Other people have made similar observations.\n    Here are two.\n    Last year, on RedMonk.com, Donnie Berkholz\n    wrote about \n    “Go as the emerging language of cloud infrastructure,”\n    observing that\n    “[Go\'s] marquee projects ... are cloud-centric or otherwise\n    made for dealing with distributed systems\n    or transient environments.”\n    This year, on Texlution.com, the author\n    wrote an article titled\n    “Why Golang is doomed to succeed,”\n    pointing out that this focus on large-scale development\n    was possibly even better suited to open source than\n    to Google itself: “This open source fitness is why I think\n    you are about to see more and more Go around ...”\n  The Go Balance\n    How does Go accomplish those things?\n    How does it make scalable concurrency\n    and scalable software development easier?\n    Most people answer this question by talking about\n    channels and goroutines, and interfaces, and fast builds,\n    and the go command, and good tool support.\n    Those are all important parts of the answer,\n    but I think there is a broader idea behind them.\n    I think of that idea as Go\'s balance.\n    There are competing concerns in any software design,\n    and there is a very natural tendency to try to solve \n    all the problems you foresee.\n    In Go, we have explicitly tried not to solve everything.\n    Instead, we\'ve tried to do just enough that you can build\n    your own custom solutions easily.\n    The way I would summarize Go\'s chosen balance is this: Do Less. Enable More.\n    Do less, but enable more.\n    Go can\'t do everything.\n    We shouldn\'t try.\n    But if we work at it,\n    Go can probably do\n    a few things well.\n    If we select those things carefully,\n    we can lay a foundation \n    on which developers can easily build\n    the solutions and tools they need,\n    and ideally can interoperate with\n    the solutions and tools built by others.\n  Examples\n    Let me illustrate this with some examples.\n    First, the size of the Go language itself.\n    We worked hard to put in as few concepts as possible,\n    to avoid the problem of mutually incomprehensible dialects\n    forming in different parts of a large developer community.\n    No idea went into Go until\n    it had been simplified to its essence\n    and then had clear benefits\n    that justified the complexity being added.\n    In general, if we have 100 things\n    we want Go to do well,\n    we can\'t make 100 separate changes.\n    Instead, we try to research and understand\n    the design space\n    and then identify a few changes\n    that work well together\n    and that enable maybe 90 of those things.\n    We\'re willing to sacrifice the remaining 10\n    to avoid bloating the language,\n    to avoid adding complexity\n    only to address specific use cases\n    that seem important today\n    but might be gone tomorrow.\n    Keeping the language small\n    enables more important goals.\n    Being small makes Go\n    easier to learn,\n    easier to understand,\n    easier to implement,\n    easier to reimplement,\n    easier to debug,\n    easier to adjust,\n    and easier to evolve.\n    Doing less enables more.\n    I should point out that\n    this means we say no\n    to a lot of other people\'s ideas,\n    but I assure you\n    we\'ve said no\n    to even more of our own ideas.\n    Next, channels and goroutines.\n    How should we structure and coordinate\n    concurrent and parallel computations?\n    Mutexes and condition variables are very general\n    but so low-level that they\'re difficult to use correctly.\n    Parallel execution frameworks like OpenMP are so high-level\n    that they can only be used to solve a narrow range of problems.\n    Channels and goroutines sit between these two extremes.\n    By themselves, they aren\'t a solution to much.\n    But they are powerful enough to be easily arranged\n    to enable solutions to many common problems\n    in concurrent software.\n    Doing less—really doing just enough—enables more.\n    Next, types and interfaces.\n    Having static types enables useful compile-time checking,\n    something lacking in dynamically-typed languages\n    like Python or Ruby.\n    At the same time,\n    Go\'s static typing avoids\n    much of the repetition\n    of traditional statically typed languages,\n    making it feel more lightweight,\n    more like the dynamically-typed languages.\n    This was one of the first things people noticed,\n    and many of Go\'s early adopters came from\n    dynamically-typed languages.\n    Go\'s interfaces are a key part of that.\n    In particular,\n    omitting the ``implements\'\' declarations\n    of Java or other languages with static hierarchy\n    makes interfaces lighter weight and more flexible.\n    Not having that rigid hierarchy\n    enables idioms such as test interfaces that describe\n    existing, unrelated production implementations.\n    Doing less enables more.\n    Next, testing and benchmarking.\n    Is there any shortage of testing\n    and benchmarking frameworks in most languages?\n    Is there any agreement between them?\n    Go\'s testing package is not meant\n    to address every possible facet of these topics.\n    Instead, it is meant to provide\n    the basic concepts necessary\n    for most higher-level tooling.\n    Packages have test cases that pass, fail, or are skipped.\n    Packages have benchmarks that run and can be measured\n    by various metrics.\n    Doing less here is an attempt\n    to reduce these concepts to their essence,\n    to create a shared vocabulary\n    so that richer tools can interoperate.\n    That agreement enables higher-level testing software\n    like Miki Tebeka\'s go2xunit converter,\n    or the benchcmp and benchstat\n    benchmark analysis tools.\n    Because there is agreement\n    about the representation of the basic concepts,\n    these higher-level tools work for all Go packages,\n    not just ones that make the effort to opt in,\n    and they interoperate with each other,\n    in that using, say, go2xunit\n    does not preclude also using benchstat,\n    the way it would if these tools were, say,\n    plugins for competing testing frameworks.\n    Doing less enables more.\n    Next, refactoring and program analysis.\n    Because Go is for large code bases,\n    we knew it would need to support automatic\n    maintenance and updating of source code.\n    We also knew that this topic was too large\n    to build in directly.\n    But we knew one thing that we had to do.\n    In our experience attempting\n    automated program changes in other settings,\n    the most significant barrier we hit \n    was actually writing the modified program out\n    in a format that developers can accept.\n    In other languages,\n    it\'s common for different teams to use\n    different formatting conventions.\n    If an edit by a program uses the wrong convention,\n    it either writes a section of the source file that looks nothing\n    like the rest of the file, or it reformats the entire file,\n    causing unnecessary and unwanted diffs.\n    Go does not have this problem.\n    We designed the language to make gofmt possible,\n    we worked hard\n    to make gofmt\'s formatting acceptable\n    for all Go programs,\n    and we made sure gofmt was there\n    from day one of the original public release.\n    Gofmt imposes such uniformity that\n    automated changes blend into the rest of the file.\n    You can\'t tell whether a particular change\n    was made by a person or a computer.\n    We didn\'t build explicit refactoring support.\n    Establishing an agreed-upon formatting algorithm\n    was enough of a shared base\n    for independent tools to develop and to interoperate.\n    Gofmt enabled gofix, goimports, eg, and other tools.\n    I believe the work here is only just getting started.\n    Even more can be done.\n    Last, building and sharing software.\n    In the run up to Go 1, we built goinstall, \n    which became what we all know as \"go get\".\n    That tool defined a standard zero-configuration way\n    to resolve import paths on sites like github.com,\n    and later a way to resolve paths on other sites\n    by making HTTP requests.\n    This agreed-upon resolution algorithm\n    enabled other tools that work in terms of those paths,\n    most notably Gary Burd\'s creation of godoc.org.\n    In case you haven\'t used it,\n    you go to godoc.org/the-import-path\n    for any valid \"go get\" import path,\n    and the web site will fetch the code\n    and show you the documentation for it.\n    A nice side effect of this has been that\n    godoc.org serves as a rough master list\n    of the Go packages publicly available.\n    All we did was give import paths a clear meaning.\n    Do less, enable more.\n    You\'ll notice that many of these tooling examples\n    are about establishing a shared convention.\n    Sometimes people refer to this as Go being “opinionated,”\n    but there\'s something deeper going on.\n    Agreeing to the limitations\n    of a shared convention\n    is a way to enable\n    a broad class of tools that interoperate,\n    because they all speak the same base language.\n    This is a very effective way\n    to do less but enable more.\n    Specifically, in many cases\n    we can do the minimum required\n    to establish a shared understanding\n    of a particular concept, like remote imports,\n    or the proper formatting of a source file,\n    and thereby enable\n    the creation of packages and tools\n    that work together\n    because they all agree\n    about those core details.\n    I\'m going to return to that idea later.\n  Why is Go open source?\n    But first, as I said earlier,\n    I want to explain how I see\n    the balance of Do Less and Enable More\n    guiding our work\n    on the broader\n    Go open source project.\n    To do that, I need to start with\n    why Go is open source at all.\n    Google pays me and others to work on Go, because,\n    if Google\'s programmers are more productive,\n    Google can build products faster,\n    maintain them more easily,\n    and so on.\n    But why open source Go?\n    Why should Google share this benefit with the world?\n    Of course, many of us\n    worked on open source projects before Go,\n    and we naturally wanted Go\n    to be part of that open source world.\n    But our preferences are not a business justification.\n    The business justification is that \n    Go is open source\n    because that\'s the only way\n    that Go can succeed.\n    We, the team that built Go within Google,\n    knew this from day one.\n    We knew that Go had to be made available\n    to as many people as possible\n    for it to succeed.\n    Closed languages die.\n    A language needs large, broad communities.\n    A language needs lots of people writing lots of software,\n    so that when you need a particular tool or library,\n    there\'s a good chance it has already been written,\n    by someone who knows the topic better than you,\n    and who spent more time than you have to make it great.\n    A language needs lots of people reporting bugs,\n    so that problems are identified and fixed quickly.\n    Because of the much larger user base,\n    the Go compilers are much more robust and spec-compliant\n    than the Plan 9 C compilers they\'re loosely based on ever were.\n    A language needs lots of people using it\n    for lots of different purposes,\n    so that the language doesn\'t overfit to one use case\n    and end up useless when the technology landscape changes.\n    A language needs lots of people who want to learn it,\n    so that there is a market for people to write books\n    or teach courses,\n    or run conferences like this one.\n    None of this could have happened\n    if Go had stayed within Google.\n    Go would have suffocated inside Google,\n    or inside any single company\n    or closed environment.\n    Fundamentally,\n    Go must be open,\n    and Go needs you.\n    Go can\'t succeed without all of you,\n    without all the people using Go\n    for all different kinds of projects\n    all over the world.\n    In turn, the Go team at Google\n    could never be large enough\n    to support the entire Go community.\n    To keep scaling,\n    we\n    need to enable all this ``more\'\'\n    while doing less.\n    Open source is a huge part of that.\n  Go\'s open source\n    What does open source mean?\n    The minimum requirement is to open the source code,\n    making it available under an open source license,\n    and we\'ve done that.\n    But we also opened our development process:\n    since announcing Go,\n    we\'ve done all our development in public,\n    on public mailing lists open to all.\n    We accept and review \n    source code contributions from anyone.\n    The process is the same\n    whether you work for Google or not.\n    We maintain our bug tracker in public,\n    we discuss and develop proposals for changes in public,\n    and we work toward releases in public.\n    The public source tree is the authoritative copy.\n    Changes happen there first.\n    They are only brought into\n    Google\'s internal source tree later.\n    For Go, being open source means\n    that this is a collective effort\n    that extends beyond Google, open to all.\n    Any open source project starts with a few people,\n    often just one, but with Go it was three:\n    Robert Griesemer, Rob Pike, and Ken Thompson.\n    They had a vision of\n    what they wanted Go to be,\n    what they thought Go could do better\n    than existing languages, and\n    Robert will talk more about that tomorrow morning.\n    I was the next person to join the team,\n    and then Ian Taylor,\n    and then, one by one,\n    we\'ve ended up where we are today,\n    with hundreds of contributors.\n    Thank You\n    to the many people who have contributed\n    code\n    or ideas\n    or bug reports\n    to the Go project so far.\n    We tried to list everyone we could\n    in our space in the program today.\n    If your name is not there,\n    I apologize,\n    but thank you.\n    I believe\n    the hundreds of contributors so far\n    are working toward a shared vision\n    of what Go can be.\n    It\'s hard to put words to these things,\n    but I did my best\n    to explain one part of the vision\n    earlier:\n    Do Less, Enable More.\n  Google\'s role\n    A natural question is:\n    What is the role\n    of the Go team at Google,\n    compared to other contributors?\n    I believe that role\n    has changed over time,\n    and it continues to change.\n    The general trend is that\n    over time\n    the Go team at Google\n    should be doing less\n    and enabling more.\n    In the very early days,\n    before Go was known to the public,\n    the Go team at Google\n    was obviously working by itself.\n    We wrote the first draft of everything: \n    the specification,\n    the compiler,\n    the runtime,\n    the standard library.\n    Once Go was open sourced, though,\n    our role began to change.\n    The most important thing\n    we needed to do\n    was communicate our vision for Go.\n    That\'s difficult,\n    and we\'re still working at it..\n    The initial implementation\n    was an important way\n    to communicate that vision,\n    as was the development work we led\n    that resulted in Go 1,\n    and the various blog posts,\n    and articles,\n    and talks we\'ve published.\n    But as Rob said at Gophercon last year,\n    \"the language is done.\"\n    Now we need to see how it works,\n    to see how people use it,\n    to see what people build.\n    The focus now is on\n    expanding the kind of work\n    that Go can help with.\n    Google\'s primarily role is now\n    to enable the community,\n    to coordinate,\n    to make sure changes work well together,\n    and to keep Go true to the original vision.\n    Google\'s primary role is:\n    Do Less. Enable More.\n    I mentioned earlier\n    that we\'d rather have a small number of features\n    that enable, say, 90% of the target use cases,\n    and avoid the orders of magnitude\n    more features necessary\n    to reach 99 or 100%.\n    We\'ve been successful in applying that strategy\n    to the areas of software that we know well.\n    But if Go is to become useful in many new domains,\n    we need experts in those areas\n    to bring their expertise\n    to our discussions,\n    so that together\n    we can design small adjustments\n    that enable many new applications for Go.\n    This shift applies not just to design\n    but also to development.\n    The role of the Go team at Google\n    continues to shift\n    more to one of guidance\n    and less of pure development.\n    I certainly spend much more time\n    doing code reviews than writing code,\n    more time processing bug reports\n    than filing bug reports myself.\n    We need to do less and enable more.\n    As design and development shift\n    to the broader Go community,\n    one of the most important things\n    we\n    the original authors of Go\n    can offer\n    is consistency of vision,\n    to help keep Go\n    Go.\n    The balance that we must strike\n    is certainly subjective.\n    For example, a mechanism for extensible syntax\n    would be a way to\n    enable more\n    ways to write Go code,\n    but that would run counter to our goal\n    of having a consistent language\n    without different dialects.\n    We have to say no sometimes,\n    perhaps more than in other language communities,\n    but when we do,\n    we aim to do so\n    constructively and respectfully,\n    to take that as an opportunity\n    to clarify the vision for Go.\n    Of course, it\'s not all coordination and vision.\n    Google still funds Go development work.\n    Rick Hudson is going to talk later today\n    about his work on reducing garbage collector latency,\n    and Hana Kim is going to talk tomorrow\n    about her work on bringing Go to mobile devices.\n    But I want to make clear that,\n    as much as possible,\n    we aim to treat\n    development funded by Google\n    as equal to\n    development funded by other companies\n    or contributed by individuals using their spare time.\n    We do this because we don\'t know\n    where the next great idea will come from.\n    Everyone contributing to Go\n    should have the opportunity to be heard.\n  Examples\n    I want to share some evidence for this claim\n    that, over time,\n    the original Go team at Google\n    is focusing more on \n    coordination than direct development.\n    First, the sources of funding\n    for Go development are expanding.\n    Before the open source release,\n    obviously Google paid for all Go development.\n    After the open source release,\n    many individuals started contributing their time,\n    and we\'ve slowly but steadily\n    been growing the number of contributors\n    supported by other companies\n    to work on Go at least part-time,\n    especially as it relates to\n    making Go more useful for those companies.\n    Today, that list includes\n    Canonical, Dropbox, Intel, Oracle, and others.\n    And of course Gophercon and the other\n    regional Go conferences are organized\n    entirely by people outside Google,\n    and they have many corporate sponsors\n    besides Google.\n    Second, the conceptual depth\n    of Go development\n    done outside the original team\n    is expanding.\n    Immediately after the open source release,\n    one of the first large contributions\n    was the port to Microsoft Windows,\n    started by Hector Chu\n    and completed by Alex Brainman and others.\n    More contributors ported Go\n    to other operating systems.\n    Even more contributors\n    rewrote most of our numeric code\n    to be faster or more precise or both.\n    These were all important contributions,\n    and very much appreciated,\n    but\n    for the most part\n    they did not involve new designs.\n    More recently,\n    a group of contributors led by Aram Hăvărneanu\n    ported Go to the ARM 64 architecture,\n    This was the first architecture port\n    by contributors outside Google.\n    This is significant, because\n    in general\n    support for a new architecture\n    requires more design work\n    than support for a new operating system.\n    There is more variation between architectures\n    than between operating systems.\n    Another example is the introduction\n    over the past few releases\n    of preliminary support\n    for building Go programs using shared libraries.\n    This feature is important for many Linux distributions\n    but not as important for Google,\n    because we deploy static binaries.\n    We have been helping guide the overall strategy,\n    but most of the design\n    and nearly all of the implementation\n    has been done by contributors outside Google,\n    especially Michael Hudson-Doyle.\n    My last example is the go command\'s\n    approach to vendoring.\n    I define vendoring as\n    copying source code for external dependencies\n    into your tree\n    to make sure that they doesn\'t disappear\n    or change underfoot.\n    Vendoring is not a problem Google suffers,\n    at least not the way the rest of the world does.\n    We copy open source libraries we want to use\n    into our shared source tree,\n    record what version we copied,\n    and only update the copy\n    when there is a need to do so.\n    We have a rule\n    that there can only be one version\n    of a particular library in the source tree,\n    and it\'s the job of whoever wants to upgrade that library\n    to make sure it keeps working as expected\n    by the Google code that depends on it.\n    None of this happens often.\n    This is the lazy approach to vendoring.\n    In contrast, most projects outside Google\n    take a more eager approach,\n    importing and updating code\n    using automated tools\n    and making sure that they are\n    always using the latest versions.\n    Because Google has relatively little experience\n    with this vendoring problem,\n    we left it to users outside Google to develop solutions.\n    Over the past five years,\n    people have built a series of tools.\n    The main ones in use today are \n    Keith Rarick\'s godep,\n    Owen Ou\'s nut,\n    and the gb-vendor plugin for Dave Cheney\'s gb,\n    There are two problems with the current situation.\n    The first is that these tools\n    are not compatible\n    out of the box\n    with the go command\'s \"go get\".\n    The second is that the tools\n    are not even compatible with each other.\n    Both of these problems\n    fragment the developer community by tool.\n    Last fall, we started a public design discussion\n    to try to build consensus on\n    some basics about\n    how these tools all operate,\n    so that they can work alongside \"go get\"\n    and each other.\n    Our basic proposal was that all tools agree\n    on the approach of rewriting import paths during vendoring,\n    to fit with \"go get\"\'s model,\n    and also that all tools agree on a file format\n    describing the source and version of the copied code,\n    so that the different vendoring tools\n    can be used together\n    even by a single project.\n    If you use one today,\n    you should still be able to use another tomorrow.\n    Finding common ground in this way\n    was very much in the spirit of Do Less, Enable More.\n    If we could build consensus\n    about these basic semantic aspects,\n    that would enable \"go get\" and all these tools to interoperate,\n    and it would enable switching between tools,\n    the same way that\n    agreement about how Go programs\n    are stored in text files\n    enables the Go compiler and all text editors to interoperate.\n    So we sent out our proposal for common ground.\n    Two things happened.\n    First, Daniel Theophanes\n    started a vendor-spec project on GitHub\n    with a new proposal\n    and took over coordination and design\n    of the spec for vendoring metadata.\n    Second, the community spoke\n    with essentially one voice\n    to say that\n    rewriting import paths during vendoring\n    was not tenable.\n    Vendoring works much more smoothly\n    if code can be copied without changes.\n    Keith Rarick posted an alternate proposal\n    for a minimal change to the go command\n    to support vendoring without rewriting import paths.\n    Keith\'s proposal was configuration-free\n    and fit in well with the rest of the go command\'s approach.\n    That proposal will ship\n    as an experimental feature in Go 1.5\n    and likely enabled by default in Go 1.6.\n    And I believe that the various vendoring tool authors\n    have agreed to adopt Daniel\'s spec once it is finalized.\n    The result\n    is that at the next Gophercon\n    we should have broad interoperability\n    between vendoring tools and the go command,\n    and the design to make that happen\n    was done entirely by contributors \n    outside the original Go team.\n    Not only that,\n    the Go team\'s proposal for how to do this\n    was essentially completely wrong.\n    The Go community told us that\n    very clearly.\n    We took that advice,\n    and now there\'s a plan for vendoring support\n    that I believe\n    everyone involved is happy with.\n    This is also a good example\n    of our general approach to design.\n    We try not to make any changes to Go\n    until we feel there is broad consensus\n    on a well-understood solution.\n    For vendoring,\n    feedback and design\n    from the Go community\n    was critical to reaching that point.\n    This general trend\n    toward both code and design\n    coming from the broader Go community\n    is important for Go.\n    You, the broader Go community,\n    know what is working\n    and what is not \n    in the environments where you use Go.\n    We at Google don\'t.\n    More and more,\n    we will rely on your expertise,\n    and we will try to help you develop\n    designs and code\n    that extend Go to be useful in more settings\n    and fit well with Go\'s original vision.\n    At the same time,\n    we will continue to wait\n    for broad consensus\n    on well-understood solutions.\n    This brings me to my last point.\n  Code of Conduct\n    I\'ve argued that Go must be open,\n    and that Go needs your help.\n    But in fact Go needs everyone\'s help.\n    And everyone isn\'t here.\n    Go needs ideas from as many people as possible.\n    To make that a reality,\n    the Go community needs to be\n    as inclusive,\n    welcoming,\n    helpful,\n    and respectful as possible.\n    The Go community is large enough now that,\n    instead of assuming that everyone involved\n    knows what is expected,\n    I and others believe that it makes sense\n    to write down those expectations explicitly.\n    Much like the Go spec\n    sets expectations for all Go compilers,\n    we can write a spec\n    setting expectations for our behavior\n    in online discussions\n    and in offline meetings like this one.\n    Like any good spec,\n    it must be general enough\n    to allow many implementations\n    but specific enough\n    that it can identify important problems.\n    When our behavior doesn\'t meet the spec,\n    people can point that out to us,\n    and we can fix the problem.\n    At the same time,\n    it\'s important to understand that\n    this kind of spec\n    cannot be as precise as a language spec.\n    We must start with the assumption\n    that we will all be reasonable in applying it. \n    This kind of spec\n    is often referred to as\n    a Code of Conduct.\n    Gophercon has one,\n    which we\'ve all agreed to follow\n    by being here,\n    but the Go community does not.\n    I and others\n    believe the Go community\n    needs a Code of Conduct.\n    But what should it say?\n    I believe\n    the most important\n    overall statement we can make\n    is that\n    if you want to use or discuss Go,\n    then you are welcome here,\n    in our community.\n    That is the standard\n    I believe we aspire to.\n    If for no other reason\n    (and, to be clear, there are excellent other reasons),\n    Go needs as large a community as possible.\n    To the extent that behavior\n    limits the size of the community,\n    it holds Go back.\n    And behavior can easily\n    limit the size of the community.\n    The tech community in general\n    and the Go community in particular\n    is skewed toward people who communicate bluntly.\n    I don\'t believe this is fundamental.\n    I don\'t believe this is necessary.\n    But it\'s especially easy to do\n    in online discussions like email and IRC,\n    where plain text is not supplemented\n    by the other cues and signals we have\n    in face-to-face interactions.\n    For example, I have learned\n    that when I am pressed for time\n    I tend to write fewer words,\n    with the end result that\n    my emails seem not just hurried\n    but blunt, impatient, even dismissive.\n    That\'s not how I feel,\n    but it\'s how I can come across,\n    and that impression can be enough\n    to make people think twice\n    about using or contributing\n    to Go.\n    I realized I was doing this\n    when some Go contributors\n    sent me private email to let me know.\n    Now, when I am pressed for time,\n    I pay extra attention to what I\'m writing,\n    and I often write more than I naturally would,\n    to make sure\n    I\'m sending the message I intend.\n    I believe\n    that correcting the parts\n    of our everyday interactions,\n    intended or not,\n    that drive away potential users and contributors\n    is one of the most important things\n    we can all do\n    to make sure the Go community\n    continues to grow.\n    A good Code of Conduct can help us do that.\n    We have no experience writing a Code of Conduct,\n    so we have been reading existing ones,\n    and we will probably adopt an existing one,\n    perhaps with minor adjustments.\n    The one I like the most is the Django Code of Conduct,\n    which originated with another project called SpeakUp!\n    It is structured as an elaboration of a list of\n    reminders for everyday interaction.\n    \"Be friendly and patient.\n    Be welcoming.\n    Be considerate.\n    Be respectful.\n    Be careful in the words that you choose.\n    When we disagree, try to understand why.\"\n    I believe this captures the tone we want to set,\n    the message we want to send,\n    the environment we want to create\n    for new contributors.\n    I certainly want to be\n    friendly,\n    patient,\n    welcoming,\n    considerate,\n    and respectful.\n    I won\'t get it exactly right all the time,\n    and I would welcome a helpful note\n    if I\'m not living up to that.\n    I believe most of us\n    feel the same way.\n    I haven\'t mentioned\n    active exclusion based on\n    or disproportionately affecting\n    race, gender, disability,\n    or other personal characteristics,\n    and I haven\'t mentioned harassment.\n    For me,\n    it follows from what I just said\n    that exclusionary behavior\n    or explicit harassment\n    is absolutely unacceptable,\n    online and offline.\n    Every Code of Conduct says this explicitly,\n    and I expect that ours will too.\n    But I believe the SpeakUp! reminders\n    about everyday interactions\n    are an equally important statement.\n    I believe that\n    setting a high standard\n    for those everyday interactions\n    makes extreme behavior\n    that much clearer\n    and easier to deal with.\n    I have no doubts that\n    the Go community can be\n    one of the most\n    friendly,\n    welcoming,\n    considerate,\n    and\n    respectful communities\n    in the tech industry.\n    We can make that happen,\n    and it will be\n    a benefit and credit to us all.\n    Andrew Gerrand\n    has been leading the effort\n    to adopt an appropriate Code of Conduct\n    for the Go community.\n    If you have suggestions,\n    or concerns,\n    or experience with Codes of Conduct,\n    or want to be involved,\n    please find Andrew or me\n    during the conference.\n    If you\'ll still be here on Friday,\n    Andrew and I are going to block off\n    some time for Code of Conduct discussions\n    during Hack Day.\n    Again, we don\'t know\n    where the next great idea will come from.\n    We need all the help we can get.\n    We need a large, diverse Go community.\n  Thank You\n    I consider the many people\n    releasing software for download using “go get,”\n    sharing their insights via blog posts,\n    or helping others on the mailing lists or IRC\n    to be part of this broad open source effort,\n    part of the Go community.\n    Everyone here today is also part of that community.\n    Thank you in advance\n    to the presenters\n    who over the next few days\n    will take time to share their experiences\n    using and extending Go.\n    Thank you in advance\n    to all of you in the audience\n    for taking the time to be here,\n    to ask questions,\n    and to let us know\n    how Go is working for you.\n    When you go back home,\n    please continue to share what you\'ve learned.\n    Even if you don\'t use Go\n    for daily work,\n    we\'d love to see what\'s working for Go\n    adopted in other contexts,\n    just as we\'re always looking for good ideas\n    to bring back into Go.\n    Thank you all again\n    for making the effort to be here\n    and for being part of the Go community.\n    For the next few days, please:\n    tell us what we\'re doing right,\n    tell us what we\'re doing wrong,\n    and help us all work together\n    to make Go even better.\n    Remember to\n    be friendly,\n    patient,\n    welcoming,\n    considerate,\n    and respectful.\n    Above all, enjoy the conference.\n		\n			By Russ Cox\n		\n	','https://blog.golang.org/open-source','2015-07-08','2017-05-07 13:49:59.658640'),(491,'GopherCon 2015 Roundup','Andrew Gerrand','		GopherCon 2015 Roundup\n		28 July 2015\n		\n    A few weeks ago, Go programmers from around the world descended on Denver,\n    Colorado for GopherCon 2015. The two-day, single-track conference attracted\n    more than 1,250 attendees—nearly double last year\'s number—and featured 22\n    talks presented by Go community members.\nThe Cowboy Gopher (a toy given to each attendee) watches over the ranch.\nPhotograph by Nathan Youngman. Gopher by Renee French.\n    Today the organizers have posted the videos online so you can now enjoy the\n    conference from afar:\n    Day 1:\n    Go, Open Source, Community — Russ Cox (video) (text)\n    Go kit: A Standard Library for Distributed Programming — Peter Bourgon (video) (slides)\n    Delve Into Go — Derek Parker (video) (slides)\n    How a complete beginner learned Go as her first backend language in 5 weeks — Audrey Lim (video) (slides)\n    A Practical Guide to Preventing Deadlocks and Leaks in Go — Richard Fliam (video)\n    Go GC: Solving the Latency Problem — Rick Hudson (video) (slides)\n    Simplicity and Go — Katherine Cox-Buday (video) (slides)\n    Rebuilding Parse.com in Go - an opinionated rewrite — Abhishek Kona (video) (slides)\n    Prometheus: Designing and Implementing a Modern Monitoring Solution in Go — Björn Rabenstein (video) (slides)\n    What Could Go Wrong? — Kevin Cantwell (video)\n    The Roots of Go — Baishampayan Ghose (video) (slides)\n    Day 2:\n    The Evolution of Go — Robert Griesemer (video) (slides)\n    Static Code Analysis Using SSA — Ben Johnson (video) (slides)\n    Go on Mobile — Hana Kim (video) (slides)\n    Go Dynamic Tools — Dmitry Vyukov (video) (slides)\n    Embrace the Interface — Tomás Senart (video) (slides)\n    Uptime: Building Resilient Services with Go — Blake Caldwell (video) (slides)\n    Cayley: Building a Graph Database — Barak Michener (video) (slides)\n    Code Generation For The Sake Of Consistency — Sarah Adams (video)\n    The Many Faces of Struct Tags — Sam Helman and Kyle Erf (video) (slides)\n    Betting the Company on Go and Winning — Kelsey Hightower (video)\n    How Go Was Made — Andrew Gerrand (video) (slides)\n    The hack day was also a ton of fun,\n    with hours of lightning talks and a range of activities from programming robots\n    to a Magic: the Gathering tournament.\n    Huge thanks to the event organizers Brian Ketelsen and Eric St. Martin and\n    their production team, the sponsors, the speakers, and the attendees for making\n    this such a fun and action-packed conference. Hope to see you there next year!\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/gophercon2015','2015-07-28','2017-05-07 13:49:59.683774'),(492,'GothamGo: gophers in the big apple','Francesc Campoy','		GothamGo: gophers in the big apple\n		9 January 2015\n		\n    Last November more than two hundred gophers from all across the United States got together for the first full-day Go conference in New York City.\n    The diverse speaker lineup included university students, industry experts, and Go team members.\n    And good news, everybody! All the talks were recorded and are available:\n    Launching into Go by Kathy Spardlin - a CockroachDB contributor provides pointers for people getting started with Go.\n    Error Handling by Bill Kennedy - ideas on how to use the Go error interface.\n    7 common mistakes in Go and how to avoid them by Steve Francia - the author of some popular Go libraries shares his experience.\n    Cancellation, Context, and Plumbing by Sameer Ajmani - the Google NYC Go team lead explains how cancellation works in Go, and how we\'re retrofitting the Google code base to use it.\n    Dethorning Package Management by Keith Rarick - the creator of godep talks about how to manage your dependencies well.\n    Everything You\'ve Always Wanted to Know About Go Web Apps (But were afraid to ask) by Mark Bates - a survey of Go packages for building web apps, and insights into why there are so few Go \"frameworks\".\n    Building high-performance database applications using Aerospike by Chris Stivers - a principal engineer at Aerospike shares his experiences building scalable applications.\n    The state of Go on the Android platform by David Crawshaw - the leader of the Go on mobile platforms project tells us what’s coming up and shows a working demo.\n    Mutexes and Locks by Jessie Frazelle - a member of the Docker core team tells us about concurrency and what to do when things go wrong.\n    Gobot.io by Ron Evans - awesome robots controlled by Go, with demos!\n    Doing Go by Bryan Liles - a DigitalOcean engineer delivers a hilarious comedy routine that happens to be about Go.\n    Things I learned teaching Go by Francesc Campoy - the Developer Advocate for the Go team shares his experience teaching Go and some advice on how to become a better gopher.\n    Two more talks come from the Go meetup in New York City, which met the day before GothamGo:\n    Benchmarking Go by Brian Bulkowski - the founder of Aerospike talks about profiling tools for Go and Linux, and micro benchmarks for goroutines, channels, buffers, and and other Go features.\n    Go Static Analysis Tools by Alan Donovan - a member of the Go team at Google NY gives a guided tour of several static analysis tools designed to help Go programmers understand, navigate , and refactor their code.\n    Make sure to have a look at all of those in preparation for the FOSDEM Go devroom FOSDEM in Brussels (Belgium) and gophercon.in in Bengaluru (India).\n		\n			By Francesc Campoy\n		\n	','https://blog.golang.org/gothamgo','2015-01-09','2017-05-07 13:49:59.746061'),(493,'Errors are values','Rob Pike','		Errors are values\n		12 January 2015\n		\n    A common point of discussion among Go programmers,\n    especially those new to the language, is how to handle errors.\n    The conversation often turns into a lament at the number of times the sequence\n  if err != nil {\n    return err\n}\n    shows up.\n    We recently scanned all the open source projects we could find and\n    discovered that this snippet occurs only once per page or two,\n    less often than some would have you believe.\n    Still, if the perception persists that one must type\n  if err != nil\n    all the time, something must be wrong, and the obvious target is Go itself.\n    This is unfortunate, misleading, and easily corrected.\n    Perhaps what is happening is that programmers new to Go ask,\n    \"How does one handle errors?\", learn this pattern, and stop there.\n    In other languages, one might use a try-catch block or other such mechanism to handle errors.\n    Therefore, the programmer thinks, when I would have used a try-catch\n    in my old language, I will just type if err != nil in Go.\n    Over time the Go code collects many such snippets, and the result feels clumsy.\n    Regardless of whether this explanation fits,\n    it is clear that these Go programmers miss a fundamental point about errors:\n    Errors are values.\n    Values can be programmed, and since errors are values, errors can be programmed.\n    Of course a common statement involving an error value is to test whether it is nil,\n    but there are countless other things one can do with an error value,\n    and application of some of those other things can make your program better,\n    eliminating much of the boilerplate that arises if every error is checked with a rote if statement.\n    Here\'s a simple example from the bufio package\'s\n    Scanner type.\n    Its Scan method performs the underlying I/O,\n    which can of course lead to an error.\n    Yet the Scan method does not expose an error at all.\n    Instead, it returns a boolean, and a separate method, to be run at the end of the scan,\n    reports whether an error occurred.\n    Client code looks like this:\n  scanner := bufio.NewScanner(input)\nfor scanner.Scan() {\n    token := scanner.Text()\n    // process token\n}\nif err := scanner.Err(); err != nil {\n    // process the error\n}\n    Sure, there is a nil check for an error, but it appears and executes only once.\n    The Scan method could instead have been defined as\n  func (s *Scanner) Scan() (token []byte, error)\n    and then the example user code might be (depending on how the token is retrieved),\n  scanner := bufio.NewScanner(input)\nfor {\n    token, err := scanner.Scan()\n    if err != nil {\n        return err // or maybe break\n    }\n    // process token\n}\n    This isn\'t very different, but there is one important distinction.\n    In this code, the client must check for an error on every iteration,\n    but in the real Scanner API, the error handling is abstracted away from the key API element,\n    which is iterating over tokens.\n    With the real API, the client\'s code therefore feels more natural:\n    loop until done, then worry about errors.\n    Error handling does not obscure the flow of control.\n    Under the covers what\'s happening, of course,\n    is that as soon as Scan encounters an I/O error, it records it and returns false.\n    A separate method, Err,\n    reports the error value when the client asks.\n    Trivial though this is, it\'s not the same as putting\n  if err != nil\n    everywhere or asking the client to check for an error after every token.\n    It\'s programming with error values.\n    Simple programming, yes, but programming nonetheless.\n    It\'s worth stressing that whatever the design,\n    it\'s critical that the program check the errors however they are exposed.\n    The discussion here is not about how to avoid checking errors,\n    it\'s about using the language to handle errors with grace.\n    The topic of repetitive error-checking code arose when I attended the autumn 2014 GoCon in Tokyo.\n    An enthusiastic gopher, who goes by @jxck_ on Twitter,\n    echoed the familiar lament about error checking.\n    He had some code that looked schematically like this:\n  _, err = fd.Write(p0[a:b])\nif err != nil {\n    return err\n}\n_, err = fd.Write(p1[c:d])\nif err != nil {\n    return err\n}\n_, err = fd.Write(p2[e:f])\nif err != nil {\n    return err\n}\n// and so on\n    It is very repetitive.\n    In the real code, which was longer,\n    there is more going on so it\'s not easy to just refactor this using a helper function,\n    but in this idealized form, a function literal closing over the error variable would help:\n  var err error\nwrite := func(buf []byte) {\n    if err != nil {\n        return\n    }\n    _, err = w.Write(buf)\n}\nwrite(p0[a:b])\nwrite(p1[c:d])\nwrite(p2[e:f])\n// and so on\nif err != nil {\n    return err\n}\n    This pattern works well, but requires a closure in each function doing the writes;\n    a separate helper function is clumsier to use because the err variable\n    needs to be maintained across calls (try it).\n    We can make this cleaner, more general, and reusable by borrowing the idea from the\n    Scan method above.\n    I mentioned this technique in our discussion but @jxck_ didn\'t see how to apply it.\n    After a long exchange, hampered somewhat by a language barrier,\n    I asked if I could just borrow his laptop and show him by typing some code.\n    I defined an object called an errWriter, something like this:\n  type errWriter struct {\n    w   io.Writer\n    err error\n}\n    and gave it one method, write.\n    It doesn\'t need to have the standard Write signature,\n    and it\'s lower-cased in part to highlight the distinction.\n    The write method calls the Write method of the underlying Writer\n    and records the first error for future reference:\n  func (ew *errWriter) write(buf []byte) {\n    if ew.err != nil {\n        return\n    }\n    _, ew.err = ew.w.Write(buf)\n}\n    As soon as an error occurs, the write method becomes a no-op but the error value is saved.\n    Given the errWriter type and its write method, the code above can be refactored:\n  ew := &amp;errWriter{w: fd}\new.write(p0[a:b])\new.write(p1[c:d])\new.write(p2[e:f])\n// and so on\nif ew.err != nil {\n    return ew.err\n}\n    This is cleaner, even compared to the use of a closure,\n    and also makes the actual sequence of writes being done easier to see on the page.\n    There is no clutter any more.\n    Programming with error values (and interfaces) has made the code nicer.\n    It\'s likely that some other piece of code in the same package can build on this idea,\n    or even use errWriter directly.\n    Also, once errWriter exists, there\'s more it could do to help,\n    especially in less artificial examples.\n    It could accumulate the byte count.\n    It could coalesce writes into a single buffer that can then be transmitted atomically.\n    And much more.\n    In fact, this pattern appears often in the standard library.\n    The archive/zip and\n    net/http packages use it.\n    More salient to this discussion, the bufio package\'s Writer\n    is actually an implementation of the errWriter idea.\n    Although bufio.Writer.Write returns an error,\n    that is mostly about honoring the io.Writer interface.\n    The Write method of bufio.Writer behaves just like our errWriter.write\n    method above, with Flush reporting the error, so our example could be written like this:\n  b := bufio.NewWriter(fd)\nb.Write(p0[a:b])\nb.Write(p1[c:d])\nb.Write(p2[e:f])\n// and so on\nif b.Flush() != nil {\n    return b.Flush()\n}\n    There is one significant drawback to this approach, at least for some applications:\n    there is no way to know how much of the processing completed before the error occurred.\n    If that information is important, a more fine-grained approach is necessary.\n    Often, though, an all-or-nothing check at the end is sufficient.\n    We\'ve looked at just one technique for avoiding repetitive error handling code.\n    Keep in mind that the use of errWriter or bufio.Writer isn\'t the only way to simplify error handling,\n    and this approach is not suitable for all situations.\n    The key lesson, however, is that errors are values and the full power of\n    the Go programming language is available for processing them.\n    Use the language to simplify your error handling.\n    But remember: Whatever you do, always check your errors!\n    Finally, for the full story of my interaction with @jxck_, including a little video he recorded,\n    visit his blog.\n		\n			By Rob Pike\n		\n	','https://blog.golang.org/errors-are-values','2015-01-12','2017-05-07 13:49:59.768209'),(494,'Package names','Sameer Ajmani','		Package names\n		4 February 2015\n		\n  Introduction\n    Go code is organized into packages.\n    Within a package, code can refer to any identifier (name) defined within, while\n    clients of the package may only reference the package\'s exported types,\n    functions, constants, and variables.\n    Such references always include the package name as a prefix: foo.Bar refers to\n    the exported name Bar in the imported package named foo.\n    Good package names make code better.\n    A package\'s name provides context for its contents, making it easier for clients\n    to understand what the package is for and how to use it.\n    The name also helps package maintainers determine what does and does not belong\n    in the package as it evolves.\n    Well-named packages make it easier to find the code you need.\n    Effective Go provides\n    guidelines for naming\n    packages, types, functions, and variables.\n    This article expands on that discussion and surveys names found in the standard\n    library.\n    It also discusses bad package names and how to fix them.\n  Package names\n    Good package names are short and clear.\n    They are lower case, with no under_scores or mixedCaps.\n    They are often simple nouns, such as:\n    time (provides functionality for measuring and displaying time)\n    list (implements a doubly linked list)\n    http (provides HTTP client and server implementations)\n    The style of names typical of another language might not be idiomatic in a Go\n    program.\n    Here are two examples of names that might be good style in other languages but\n    do not fit well in Go:\n    computeServiceClient\n    priority_queue\n    A Go package may export several types and functions.\n    For example, a compute package could export a Client type with methods for\n    using the service as well as functions for partitioning a compute task across\n    several clients.\n    Abbreviate judiciously.\n    Package names may be abbreviated when the abbreviation is familiar to the\n    programmer.\n    Widely-used packages often have compressed names:\n    strconv (string conversion)\n    syscall (system call)\n    fmt (formatted I/O)\n    On the other hand, if abbreviating a package name makes it ambiguous or unclear,\n    don\'t do it.\n    Don\'t steal good names from the user.\n    Avoid giving a package a name that is commonly used in client code.\n    For example, the buffered I/O package is called bufio, not buf, since buf\n    is a good variable name for a buffer.\n  Naming package contents\n    A package name and its contents\' names are coupled, since client code uses them\n    together.\n    When designing a package, take the client\'s point of view.\n    Avoid stutter.\n    Since client code uses the package name as a prefix when referring to the\n    package contents, the names for those contents need not repeat the package name.\n    The HTTP server provided by the http package is called Server, not\n    HTTPServer.\n    Client code refers to this type as http.Server, so there is no ambiguity.\n    Simplify function names.\n    When a function in package pkg returns a value of type pkg.Pkg (or\n    *pkg.Pkg), the function name can often omit the type name without confusion:\n  start := time.Now()                                  // start is a time.Time\nt, err := time.Parse(time.Kitchen, \"6:06PM\")         // t is a time.Time\n  ctx = context.WithTimeout(ctx, 10*time.Millisecond)  // ctx is a context.Context\nip, ok := userip.FromContext(ctx)                    // ip is a net.IP\n    A function named New in package pkg returns a value of type pkg.Pkg.\n    This is a standard entry point for client code using that type:\n  q := list.New()  // q is a *list.List\n    When a function returns a value of type pkg.T, where T is not Pkg, the\n    function name may include T to make client code easier to understand.\n    A common situation is a package with multiple New-like functions:\n  d, err := time.ParseDuration(\"10s\")  // d is a time.Duration\nelapsed := time.Since(start)         // elapsed is a time.Duration\nticker := time.NewTicker(d)          // ticker is a *time.Ticker\ntimer := time.NewTimer(d)            // timer is a *time.Timer\n    Types in different packages can have the same name, because from the client\'s\n    point of view such names are discriminated by the package name.\n    For example, the standard library includes several types named Reader,\n    including jpeg.Reader, bufio.Reader, and csv.Reader.\n    Each package name fits with Reader to yield a good type name.\n    If you cannot come up with a package name that\'s a meaningful prefix for the\n    package\'s contents, the package abstraction boundary may be wrong.\n    Write code that uses your package as a client would, and restructure your\n    packages if the result seems poor.\n    This approach will yield packages that are easier for clients to understand and\n    for the package developers to maintain.\n  Package paths\n    A Go package has both a name and a path.\n    The package name is specified in the package statement of its source files;\n    client code uses it as the prefix for the package\'s exported names.\n    Client code uses the package path when importing the package.\n    By convention, the last element of the package path is the package name:\n  import (\n    \"context\"                // package context\n    \"fmt\"                    // package fmt\n    \"golang.org/x/time/rate\" // package rate\n    \"os/exec\"                // package exec\n)\n    Build tools map package paths onto directories.\n    The go tool uses the GOPATH\n    environment variable to find the source files for path \"github.com/user/hello\"\n    in directory $GOPATH/src/github.com/user/hello.\n    (This situation should be familiar, of course, but it\'s important to be clear\n    about the terminology and structure of packages.)\n    Directories.\n    The standard library uses like directories crypto, container, encoding,\n    and image to group packages for related protocols and algorithms.\n    There is no actual relationship among the packages in one of these directories;\n    a directory just provides a way to arrange the files.\n    Any package can import any other package provided the import does not create a\n    cycle.\n    Just as types in different packages can have the same name without ambiguity,\n    packages in different directories can have the same name.\n    For example,\n    runtime/pprof provides profiling data\n    in the format expected by the pprof\n    profiling tool, while net/http/pprof\n    provides HTTP endpoints to present profiling data in this format.\n    Client code uses the package path to import the package, so there is no\n    confusion.\n    If a source file needs to import both pprof packages, it can\n    rename one or both locally.\n    When renaming an imported package, the local name should follow the same\n    guidelines as package names (lower case, no under_scores or mixedCaps).\n  Bad package names\n    Bad package names make code harder to navigate and maintain.\n    Here are some guidelines for recognizing and fixing bad names.\n    Avoid meaningless package names.\n    Packages named util, common, or misc provide clients with no sense of what\n    the package contains.\n    This makes it harder for clients to use the package and makes it harder for\n    maintainers to keep the package focused.\n    Over time, they accumulate dependencies that can make compilation significantly\n    and unnecessarily slower, especially in large programs.\n    And since such package names are generic, they are more likely to collide with\n    other packages imported by client code, forcing clients to invent names to\n    distinguish them.\n    Break up generic packages.\n    To fix such packages, look for types and functions with common name elements and\n    pull them into their own package.\n    For example, if you have\n  package util\nfunc NewStringSet(...string) map[string]bool {...}\nfunc SortStringSet(map[string]bool) []string {...}\n    then client code looks like\n  set := util.NewStringSet(\"c\", \"a\", \"b\")\nfmt.Println(util.SortStringSet(set))\n    Pull these functions out of util into a new package, choosing a name that fits\n    the contents:\n  package stringset\nfunc New(...string) map[string]bool {...}\nfunc Sort(map[string]bool) []string {...}\n    then the client code becomes\n  set := stringset.New(\"c\", \"a\", \"b\")\nfmt.Println(stringset.Sort(set))\n    Once you\'ve made this change, its easier to see how to improve the new package:\n  package stringset\ntype Set map[string]bool\nfunc New(...string) Set {...}\nfunc (s Set) Sort() []string {...}\n    which yields even simpler client code:\n  set := stringset.New(\"c\", \"a\", \"b\")\nfmt.Println(set.Sort())\n    The name of the package is a critical piece of its design.\n    Work to eliminate meaningless package names from your projects.\n    Don\'t use a single package for all your APIs.\n    Many well-intentioned programmers put all the interfaces exposed by their\n    program into a single package named api, types, or interfaces, thinking it\n    makes it easier to find the entry points to their code base.\n    This is a mistake.\n    Such packages suffer from the same problems as those named util or common,\n    growing without bound, providing no guidance to users, accumulating\n    dependencies, and colliding with other imports.\n    Break them up, perhaps using directories to separate public packages from\n    implementation.\n    Avoid unnecessary package name collisions.\n    While packages in different directories may have the same name, packages that\n    are frequently used together should have distinct names.\n    This reduces confusion and the need for local renaming in client code.\n    For the same reason, avoid using the same name as popular standard packages like\n    io or http.\n  Conclusion\n    Package names are central to good naming in Go programs.\n    Take the time to choose good package names and organize your code well.\n    This helps clients understand and use your packages and helps maintainers to\n    grow them gracefully.\n  Further reading\n    Effective Go\n    How to Write Go Code\n    Organizing Go Code (2012 blog post)\n    Organizing Go Code (2014 Google I/O talk)\n		\n			By Sameer Ajmani\n		\n	','https://blog.golang.org/package-names','2015-02-04','2017-05-07 13:49:59.780291'),(495,'Strings, bytes, runes and characters in Go','Rob Pike','		Strings, bytes, runes and characters in Go\n		23 October 2013\n		\n  Introduction\n    The previous blog post explained how slices\n    work in Go, using a number of examples to illustrate the mechanism behind\n    their implementation.\n    Building on that background, this post discusses strings in Go.\n    At first, strings might seem too simple a topic for a blog post, but to use\n    them well requires understanding not only how they work,\n    but also the difference between a byte, a character, and a rune,\n    the difference between Unicode and UTF-8,\n    the difference between a string and a string literal,\n    and other even more subtle distinctions.\n    One way to approach this topic is to think of it as an answer to the frequently\n    asked question, \"When I index a Go string at position n, why don\'t I get the\n    nth character?\"\n    As you\'ll see, this question leads us to many details about how text works\n    in the modern world.\n    An excellent introduction to some of these issues, independent of Go,\n    is Joel Spolsky\'s famous blog post,\n    The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!).\n    Many of the points he raises will be echoed here.\n  What is a string?\n    Let\'s start with some basics.\n    In Go, a string is in effect a read-only slice of bytes.\n    If you\'re at all uncertain about what a slice of bytes is or how it works,\n    please read the previous blog post;\n    we\'ll assume here that you have.\n    It\'s important to state right up front that a string holds arbitrary bytes.\n    It is not required to hold Unicode text, UTF-8 text, or any other predefined format.\n    As far as the content of a string is concerned, it is exactly equivalent to a\n    slice of bytes.\n    Here is a string literal (more about those soon) that uses the\n    \\xNN notation to define a string constant holding some peculiar byte values.\n    (Of course, bytes range from hexadecimal values 00 through FF, inclusive.)\n	\n    const sample = \"\\xbd\\xb2\\x3d\\xbc\\x20\\xe2\\x8c\\x98\"\n  Printing strings\n    Because some of the bytes in our sample string are not valid ASCII, not even\n    valid UTF-8, printing the string directly will produce ugly output.\n    The simple print statement\n	\n    fmt.Println(sample)\n    produces this mess (whose exact appearance varies with the environment):\n  ��=� ⌘\n    To find out what that string really holds, we need to take it apart and examine the pieces.\n    There are several ways to do this.\n    The most obvious is to loop over its contents and pull out the bytes\n    individually, as in this for loop:\n	\n    for i := 0; i &lt; len(sample); i++ {\n        fmt.Printf(\"%x \", sample[i])\n    }\n    As implied up front, indexing a string accesses individual bytes, not\n    characters. We\'ll return to that topic in detail below. For now, let\'s\n    stick with just the bytes.\n    This is the output from the byte-by-byte loop:\n  bd b2 3d bc 20 e2 8c 98\n    Notice how the individual bytes match the\n    hexadecimal escapes that defined the string.\n    A shorter way to generate presentable output for a messy string\n    is to use the %x (hexadecimal) format verb of fmt.Printf.\n    It just dumps out the sequential bytes of the string as hexadecimal\n    digits, two per byte.\n	\n    fmt.Printf(\"%x\\n\", sample)\n    Compare its output to that above:\n  bdb23dbc20e28c98\n    A nice trick is to use the \"space\" flag in that format, putting a\n    space between the % and the x. Compare the format string\n    used here to the one above,\n	\n    fmt.Printf(\"% x\\n\", sample)\n    and notice how the bytes come\n    out with spaces between, making the result a little less imposing:\n  bd b2 3d bc 20 e2 8c 98\n    There\'s more. The %q (quoted) verb will escape any non-printable\n    byte sequences in a string so the output is unambiguous.\n	\n    fmt.Printf(\"%q\\n\", sample)\n    This technique is handy when much of the string is\n    intelligible as text but there are peculiarities to root out; it produces:\n  \"\\xbd\\xb2=\\xbc ⌘\"\n    If we squint at that, we can see that buried in the noise is one ASCII equals sign,\n    along with a regular space, and at the end appears the well-known Swedish \"Place of Interest\"\n    symbol.\n    That symbol has Unicode value U+2318, encoded as UTF-8 by the bytes\n    after the space (hex value 20): e2 8c 98.\n    If we are unfamiliar or confused by strange values in the string,\n    we can use the \"plus\" flag to the %q verb. This flag causes the output to escape\n    not only non-printable sequences, but also any non-ASCII bytes, all\n    while interpreting UTF-8.\n    The result is that it exposes the Unicode values of properly formatted UTF-8\n    that represents non-ASCII data in the string:\n	\n    fmt.Printf(\"%+q\\n\", sample)\n    With that format, the Unicode value of the Swedish symbol shows up as a\n    \\u escape:\n  \"\\xbd\\xb2=\\xbc \\u2318\"\n    These printing techiques are good to know when debugging\n    the contents of strings, and will be handy in the discussion that follows.\n    It\'s worth pointing out as well that all these methods behave exactly the\n    same for byte slices as they do for strings.\n    Here\'s the full set of printing options we\'ve listed, presented as\n    a complete program you can run (and edit) right in the browser:\n	\n// +build OMIT\n// Copyright 2013 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\npackage main\nimport \"fmt\"\nfunc main() {\n    const sample = \"\\xbd\\xb2\\x3d\\xbc\\x20\\xe2\\x8c\\x98\"\n    fmt.Println(\"Println:\")\n    fmt.Println(sample)\n    fmt.Println(\"Byte loop:\")\n    for i := 0; i &lt; len(sample); i++ {\n        fmt.Printf(\"%x \", sample[i])\n    }\n    fmt.Printf(\"\\n\")\n    fmt.Println(\"Printf with %x:\")\n    fmt.Printf(\"%x\\n\", sample)\n    fmt.Println(\"Printf with % x:\")\n    fmt.Printf(\"% x\\n\", sample)\n    fmt.Println(\"Printf with %q:\")\n    fmt.Printf(\"%q\\n\", sample)\n    fmt.Println(\"Printf with %+q:\")\n    fmt.Printf(\"%+q\\n\", sample)\n}\n    [Exercise: Modify the examples above to use a slice of bytes\n    instead of a string. Hint: Use a conversion to create the slice.]\n    [Exercise: Loop over the string using the %q format on each byte.\n    What does the output tell you?]\n  UTF-8 and string literals\n    As we saw, indexing a string yields its bytes, not its characters: a string is just a\n    bunch of bytes.\n    That means that when we store a character value in a string,\n    we store its byte-at-a-time representation.\n    Let\'s look at a more controlled example to see how that happens.\n    Here\'s a simple program that prints a string constant with a single character\n    three different ways, once as a plain string, once as an ASCII-only quoted\n    string, and once as individual bytes in hexadecimal.\n    To avoid any confusion, we create a \"raw string\", enclosed by back quotes,\n    so it can contain only literal text. (Regular strings, enclosed by double\n    quotes, can contain escape sequences as we showed above.)\n	\n// +build OMIT\n// Copyright 2013 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\npackage main\nimport \"fmt\"\nfunc main() {\n    const placeOfInterest = `⌘`\n    fmt.Printf(\"plain string: \")\n    fmt.Printf(\"%s\", placeOfInterest)\n    fmt.Printf(\"\\n\")\n    fmt.Printf(\"quoted string: \")\n    fmt.Printf(\"%+q\", placeOfInterest)\n    fmt.Printf(\"\\n\")\n    fmt.Printf(\"hex bytes: \")\n    for i := 0; i &lt; len(placeOfInterest); i++ {\n        fmt.Printf(\"%x \", placeOfInterest[i])\n    }\n    fmt.Printf(\"\\n\")\n}\n    The output is:\n  plain string: ⌘\nquoted string: \"\\u2318\"\nhex bytes: e2 8c 98\n    which reminds us that the Unicode character value U+2318, the \"Place\n    of Interest\" symbol ⌘, is represented by the bytes e2 8c 98, and\n    that those bytes are the UTF-8 encoding of the hexadecimal\n    value 2318.\n    It may be obvious or it may be subtle, depending on your familiarity with\n    UTF-8, but it\'s worth taking a moment to explain how the UTF-8 representation\n    of the string was created.\n    The simple fact is: it was created when the source code was written.\n    Source code in Go is defined to be UTF-8 text; no other representation is\n    allowed. That implies that when, in the source code, we write the text\n  `⌘`\n    the text editor used to create the program places the UTF-8 encoding\n    of the symbol ⌘ into the source text.\n    When we print out the hexadecimal bytes, we\'re just dumping the\n    data the editor placed in the file.\n    In short, Go source code is UTF-8, so\n    the source code for the string literal is UTF-8 text.\n    If that string literal contains no escape sequences, which a raw\n    string cannot, the constructed string will hold exactly the\n    source text  between the quotes.\n    Thus by definition and\n    by construction the raw string will always contain a valid UTF-8\n    representation of its contents.\n    Similarly, unless it contains UTF-8-breaking escapes like those\n    from the previous section, a regular string literal will also always\n    contain valid UTF-8.\n    Some people think Go strings are always UTF-8, but they\n    are not: only string literals are UTF-8.\n    As we showed in the previous section, string values can contain arbitrary\n    bytes;\n    as we showed in this one, string literals always contain UTF-8 text\n    as long as they have no byte-level escapes.\n    To summarize, strings can contain arbitrary bytes, but when constructed\n    from string literals, those bytes are (almost always) UTF-8.\n  Code points, characters, and runes\n    We\'ve been very careful so far in how we use the words \"byte\" and \"character\".\n    That\'s partly because strings hold bytes, and partly because the idea of \"character\"\n    is a little hard to define.\n    The Unicode standard uses the term \"code point\" to refer to the item represented\n    by a single value.\n    The code point U+2318, with hexadecimal value 2318, represents the symbol ⌘.\n    (For lots more information about that code point, see\n    its Unicode page.)\n    To pick a more prosaic example, the Unicode code point U+0061 is the lower\n    case Latin letter \'A\': a.\n    But what about the lower case grave-accented letter \'A\', à?\n    That\'s a character, and it\'s also a code point (U+00E0), but it has other\n    representations.\n    For example we can use the \"combining\" grave accent code point, U+0300,\n    and attach it to the lower case letter a, U+0061, to create the same character à.\n    In general, a character may be represented by a number of different\n    sequences of code points, and therefore different sequences of UTF-8 bytes.\n    The concept of character in computing is therefore ambiguous, or at least\n    confusing, so we use it with care.\n    To make things dependable, there are normalization techniques that guarantee that\n    a given character is always represented by the same code points, but that\n    subject takes us too far off the topic for now.\n    A later blog post will explain how the Go libraries address normalization.\n    \"Code point\" is a bit of a mouthful, so Go introduces a shorter term for the\n    concept: rune.\n    The term appears in the libraries and source code, and means exactly\n    the same as \"code point\", with one interesting addition.\n    The Go language defines the word rune as an alias for the type int32, so\n    programs can be clear when an integer value represents a code point.\n    Moreover, what you might think of as a character constant is called a\n    rune constant in Go.\n    The type and value of the expression\n  \'⌘\'\n    is rune with integer value 0x2318.\n    To summarize, here are the salient points:\n    Go source code is always UTF-8.\n    A string holds arbitrary bytes.\n    A string literal, absent byte-level escapes, always holds valid UTF-8 sequences.\n    Those sequences represent Unicode code points, called runes.\n    No guarantee is made in Go that characters in strings are normalized.\n  Range loops\n    Besides the axiomatic detail that Go source code is UTF-8,\n    there\'s really only one way that Go treats UTF-8 specially, and that is when using\n    a for range loop on a string.\n    We\'ve seen what happens with a regular for loop.\n    A for range loop, by contrast, decodes one UTF-8-encoded rune on each\n    iteration.\n    Each time around the loop, the index of the loop is the starting position of the\n    current rune, measured in bytes, and the code point is its value.\n    Here\'s an example using yet another handy Printf format, %#U, which shows\n    the code point\'s Unicode value and its printed representation:\n	\n// +build OMIT\n// Copyright 2013 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\npackage main\nimport \"fmt\"\nfunc main() {\n    const nihongo = \"日本語\"\n    for index, runeValue := range nihongo {\n        fmt.Printf(\"%#U starts at byte position %d\\n\", runeValue, index)\n    }\n}\n    The output shows how each code point occupies multiple bytes:\n  U+65E5 \'日\' starts at byte position 0\nU+672C \'本\' starts at byte position 3\nU+8A9E \'語\' starts at byte position 6\n    [Exercise: Put an invalid UTF-8 byte sequence into the string. (How?)\n    What happens to the iterations of the loop?]\n  Libraries\n    Go\'s standard library provides strong support for interpreting UTF-8 text.\n    If a for range loop isn\'t sufficient for your purposes,\n    chances are the facility you need is provided by a package in the library.\n    The most important such package is\n    unicode/utf8,\n    which contains\n    helper routines to validate, disassemble, and reassemble UTF-8 strings.\n    Here is a program equivalent to the for range example above,\n    but using the DecodeRuneInString function from that package to\n    do the work.\n    The return values from the function are the rune and its width in\n    UTF-8-encoded bytes.\n	\n// +build OMIT\n// Copyright 2013 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\npackage main\nimport (\n	\"fmt\"\n	\"unicode/utf8\"\n)\nfunc main() {\n    const nihongo = \"日本語\"\n    for i, w := 0, 0; i &lt; len(nihongo); i += w {\n        runeValue, width := utf8.DecodeRuneInString(nihongo[i:])\n        fmt.Printf(\"%#U starts at byte position %d\\n\", runeValue, i)\n        w = width\n    }\n}\n    Run it to see that it performs the same.\n    The for range loop and DecodeRuneInString are defined to produce\n    exactly the same iteration sequence.\n    Look at the\n    documentation\n    for the unicode/utf8 package to see what\n    other facilities it provides.\n  Conclusion\n    To answer the question posed at the beginning: Strings are built from bytes\n    so indexing them yields bytes, not characters.\n    A string might not even hold characters.\n    In fact, the definition of \"character\" is ambiguous and it would\n    be a mistake to try to resolve the ambiguity by defining that strings are made\n    of characters.\n    There\'s much more to say about Unicode, UTF-8, and the world of multilingual\n    text processing, but it can wait for another post.\n    For now, we hope you have a better understanding of how Go strings behave\n    and that, although they may contain arbitrary bytes, UTF-8 is a central part\n    of their design.\n		\n			By Rob Pike\n		\n	','https://blog.golang.org/strings','2013-10-23','2017-05-07 13:49:59.804643'),(496,'Text normalization in Go','Marcel van Lohuizen','		Text normalization in Go\n		26 November 2013\n		\n  Introduction\n    An earlier post talked about strings, bytes\n    and characters in Go. I\'ve been working on various packages for multilingual\n    text processing for the go.text repository. Several of these packages deserve a\n    separate blog post, but today I want to focus on\n    go.text/unicode/norm,\n    which handles normalization, a topic touched in the\n    strings article and the subject of this\n    post. Normalization works at a higher level of abstraction than raw bytes.\n    To learn pretty much everything you ever wanted to know about normalization\n    (and then some), Annex 15 of the Unicode Standard\n    is a good read. A more approachable article is the corresponding\n    Wikipedia page. Here we\n    focus on how normalization relates to Go.\n  What is normalization?\n    There are often several ways to represent the same string. For example, an é\n    (e-acute) can be represented in a string as a single rune (\"\\u00e9\") or an \'e\'\n    followed by an acute accent (\"e\\u0301\"). According to the Unicode standard,\n    these two are \"canonically equivalent\" and should be treated as equal.\n    Using a byte-to-byte comparison to determine equality would clearly not give\n    the right result for these two strings. Unicode defines a set of normal forms\n    such that if two strings are canonically equivalent and are normalized to the\n    same normal form, their byte representations are the same.\n    Unicode also defines a \"compatibility equivalence\" to equate characters that\n    represent the same characters, but may have a different visual appearance. For\n    example, the superscript digit \'⁹\' and the regular digit \'9\' are equivalent in\n    this form.\n    For each of these two equivalence forms, Unicode defines a composing and\n    decomposing form. The former replaces runes that can combine into a single rune\n    with this single rune. The latter breaks runes apart into their components.\n    This table shows the names, all starting with NF, by which the Unicode\n    Consortium identifies these forms:\n	.padtable td, .padtable th { padding-right: 10px; }\n	\n		 \n		Composing\n		Decomposing\n	\n	\n		Canonical equivalence\n		NFC\n		NFD\n	\n	\n		Compatibility equivalence\n		NFKC\n		NFKD\n	\n  Go\'s approach to normalization\n    As mentioned in the strings blog post, Go does not guarantee that characters in\n    a string are normalized. However, the go.text packages can compensate. For\n    example, the\n    collate package, which\n    can sort strings in a language-specific way, works correctly even with\n    unnormalized strings. The packages in go.text do not always require normalized\n    input, but in general normalization may be necessary for consistent results.\n    Normalization isn\'t free but it is fast, particularly for collation and\n    searching or if a string is either in NFD or in NFC and can be converted to NFD\n    by decomposing without reordering its bytes. In practice,\n    99.98% of\n    the web\'s HTML page content is in NFC form (not counting markup, in which case\n    it would be more). By far most NFC can be decomposed to NFD without the need\n    for reordering (which requires allocation). Also, it is efficient to detect\n    when reordering is necessary, so we can save time by doing it only for the rare\n    segments that need it.\n    To make things even better, the collation package typically does not use the\n    norm package directly, but instead uses the norm package to interleave\n    normalization information with its own tables. Interleaving the two problems\n    allows for reordering and normalization on the fly with almost no impact on\n    performance. The cost of on-the-fly normalization is compensated by not having\n    to normalize text beforehand and ensuring that the normal form is maintained\n    upon edits. The latter can be tricky. For instance, the result of concatenating\n    two NFC-normalized strings is not guaranteed to be in NFC.\n    Of course, we can also avoid the overhead outright if we know in advance that a\n    string is already normalized, which is often the case.\n  Why bother?\n    After all this discussion about avoiding normalization, you might ask why it\'s\n    worth worrying about at all. The reason is that there are cases where\n    normalization is required and it is important to understand what those are, and\n    in turn how to do it correctly.\n    Before discussing those, we must first clarify the concept of \'character\'.\n  What is a character?\n    As was mentioned in the strings blog post, characters can span multiple runes.\n    For example, an \'e\' and \'◌́\' (acute \"\\u0301\") can combine to form \'é\' (\"e\\u0301\"\n    in NFD).  Together these two runes are one character. The definition of a\n    character may vary depending on the application. For normalization we will\n    define it as a sequence of runes that starts with a starter, a rune that does\n    not modify or combine backwards with any other rune, followed by possibly empty\n    sequence of non-starters, that is, runes that do (typically accents). The\n    normalization algorithm processes one character at at time.\n    Theoretically, there is no bound to the number of runes that can make up a\n    Unicode character. In fact, there are no restrictions on the number of\n    modifiers that can follow a character and a modifier may be repeated, or\n    stacked. Ever seen an \'e\' with three acutes? Here you go: \'é́́\'. That is a\n    perfectly valid 4-rune character according to the standard.\n    As a consequence, even at the lowest level, text needs to be processed in\n    increments of unbounded chunk sizes. This is especially awkward with a\n    streaming approach to text processing, as used by Go\'s standard Reader and\n    Writer interfaces, as that model potentially requires any intermediate buffers\n    to have unbounded size as well. Also, a straightforward implementation of\n    normalization will have a O(n²) running time.\n    There are really no meaningful interpretations for such large sequences of\n    modifiers for practical applications. Unicode defines a Stream-Safe Text\n    format, which allows capping the number of modifiers (non-starters) to at most\n    30, more than enough for any practical purpose. Subsequent modifiers will be\n    placed after a freshly inserted Combining Grapheme Joiner (CGJ or U+034F). Go\n    adopts this approach for all normalization algorithms. This decision gives up a\n    little conformance but gains a little safety.\n  Writing in normal form\n    Even if you don\'t need to normalize text within your Go code, you might still\n    want to do so when communicating to the outside world. For example, normalizing\n    to NFC might compact your text, making it cheaper to send down a wire. For some\n    languages, like Korean, the savings can be substantial. Also, some external\n    APIs might expect text in a certain normal form. Or you might just want to fit\n    in and output your text as NFC like the rest of the world.\n    To write your text as NFC, use the\n    unicode/norm package\n    to wrap your io.Writer of choice:\n  wc := norm.NFC.Writer(w)\ndefer wc.Close()\n// write as before...\n    If you have a small string and want to do a quick conversion, you can use this\n    simpler form:\n  norm.NFC.Bytes(b)\n    Package norm provides various other methods for normalizing text.\n    Pick the one that suits your needs best.\n  Catching look-alikes\n    Can you tell the difference between \'K\' (\"\\u004B\") and \'K\' (Kelvin sign\n    \"\\u212A\") or \'Ω\' (\"\\u03a9\") and \'Ω\' (Ohm sign \"\\u2126\")? It is easy to overlook\n    the sometimes minute differences between variants of the same underlying\n    character. It is generally a good idea to disallow such variants in identifiers\n    or anything where deceiving users with such look-alikes can pose a security\n    hazard.\n    The compatibility normal forms, NFKC and NFKD, will map many visually nearly\n    identical forms to a single value. Note that it will not do so when two symbols\n    look alike, but are really from two different alphabets. For example the Latin\n    \'o\', Greek \'ο\', and Cyrillic \'о\' are still different characters as defined by\n    these forms.\n  Correct text modifications\n    The norm package might also come to the rescue when one needs to modify text.\n    Consider a case where you want to search and replace the word \"cafe\" with its\n    plural form \"cafes\".  A code snippet could look like this.\n  s := \"We went to eat at multiple cafe\"\ncafe := \"cafe\"\nif p := strings.Index(s, cafe); p != -1 {\n    p += len(cafe)\n    s = s[:p] + \"s\" + s[p:]\n}\nfmt.Println(s)\n    This prints \"We went to eat at multiple cafes\" as desired and expected. Now\n    consider our text contains the French spelling \"café\" in NFD form:\n  s := \"We went to eat at multiple cafe\\u0301\"\n    Using the same code from above, the plural \"s\" would still be inserted after\n    the \'e\', but before the acute, resulting in  \"We went to eat at multiple\n    cafeś\".  This behavior is undesirable.\n    The problem is that the code does not respect the boundaries between multi-rune\n    characters and inserts a rune in the middle of a character.  Using the norm\n    package, we can rewrite this piece of code as follows:\n  s := \"We went to eat at multiple cafe\\u0301\"\ncafe := \"cafe\"\nif p := strings.Index(s, cafe); p != -1 {\n    p += len(cafe)\n    if bp := norm.FirstBoundary(s[p:]); bp &gt; 0 {\n        p += bp\n    }\n    s = s[:p] + \"s\" + s[p:]\n}\nfmt.Println(s)\n    This may be a contrived example, but the gist should be clear. Be mindful of\n    the fact that characters can span multiple runes. Generally these kinds of\n    problems can be avoided by using search functionality that respects character\n    boundaries (such as the planned go.text/search package.)\n  Iteration\n    Another tool provided by the norm package that may help dealing with character\n    boundaries is its iterator,\n    norm.Iter.\n    It iterates over characters one at a time in the normal form of choice.\n  Performing magic\n    As mentioned earlier, most text is in NFC form, where base characters and\n    modifiers are combined into a single rune whenever possible.  For the purpose\n    of analyzing characters, it is often easier to handle runes after decomposition\n    into their smallest components. This is where the NFD form comes in handy. For\n    example, the following piece of code creates a transform.Transformer that\n    decomposes text into its smallest parts, removes all accents, and then\n    recomposes the text into NFC:\n  import (\n    \"unicode\"\n    \"golang.org/x/text/transform\"\n    \"golang.org/x/text/unicode/norm\"\n)\nisMn := func(r rune) bool {\n    return unicode.Is(unicode.Mn, r) // Mn: nonspacing marks\n}\nt := transform.Chain(norm.NFD, transform.RemoveFunc(isMn), norm.NFC)\n    The resulting Transformer can be used to remove accents from an io.Reader\n    of choice as follows:\n  r = transform.NewReader(r, t)\n// read as before ...\n    This will, for example, convert any mention of \"cafés\" in the text to \"cafes\",\n    regardless of the normal form in which the original text was encoded.\n  Normalization info\n    As mentioned earlier, some packages precompute normalizations into their tables\n    to minimize the need for normalization at run time. The type norm.Properties\n    provides access to the per-rune information needed by these packages, most\n    notably the Canonical Combining Class and decomposition information. Read the\n    documentation\n    for this type if you want to dig deeper.\n  Performance\n    To give an idea of the performance of normalization, we compare it against the\n    performance of strings.ToLower. The sample in the first row is both lowercase\n    and NFC and can in every case be returned as is. The second sample is neither\n    and requires writing a new version.\n	\n		Input\n		ToLower\n		NFC Append\n		NFC Transform\n		NFC Iter\n	\n	\n		nörmalization\n		199 ns\n		137 ns\n		133 ns\n		251 ns (621 ns)\n	\n	\n		No\\u0308rmalization\n		427 ns\n		836 ns\n		845 ns\n		573 ns (948 ns)\n	\n    The column with the results for the iterator shows both the measurement with\n    and without initialization of the iterator, which contain buffers that don\'t\n    need to be reinitialized upon reuse.\n    As you can see, detecting whether a string is normalized can be quite\n    efficient. A lot of the cost of normalizing in the second row is for the\n    initialization of buffers, the cost of which is amortized when one is\n    processing larger strings. As it turns out, these buffers are rarely needed, so\n    we may change the implementation at some point to speed up the common case for\n    small strings even further.\n  Conclusion\n    If you\'re dealing with text inside Go, you generally do not have to use the\n    unicode/norm package to normalize your text. The package may still be useful\n    for things like ensuring that strings are normalized before sending them out or\n    to do advanced text manipulation.\n    This article briefly mentioned the existence of other go.text packages as well\n    as multilingual text processing and it may have raised more questions than it\n    has given answers. The discussion of these topics, however, will have to wait\n    until another day.\n		\n			By Marcel van Lohuizen\n		\n	','https://blog.golang.org/normalization','2013-11-26','2017-05-07 13:49:59.836308'),(497,'The Go Gopher','Rob Pike and Andrew Gerrand','		The Go Gopher\n		24 March 2014\n		\n    The Go gopher is an iconic mascot and one of the most distinctive features of the Go project. In this post we\'ll talk about its origins, evolution, and behavior.\n    About 15 years ago—long before the Go project—the gopher first appeared as a promotion for the WFMU radio station in New Jersey. Renee French was commissioned to design a T-shirt for an annual fundraiser and out came the gopher.\n    The gopher next made an appearance at Bell Labs, as Bob Flandrena\'s avatar in the Bell Labs mail system. Other Renee drawings became avatars for ken, r, rsc, and others. (Of course, Peter Weinberger\'s was his own iconic face.)\n    Another Bell Labs activity led to Renee creating Glenda, the Plan 9 mascot, a distant cousin of the WFMU gopher.\n    When we started the Go project we needed a logo, and Renee volunteered to draw it. It was featured on the first Go T-shirt and the Google Code site.\n    For the open source launch in 2009, Renee suggested adapting the WFMU gopher as a mascot. And the Go gopher was born:\n    (The gopher has no name, and is called just the \"Go gopher\".)\n    For the launch of the Go App Engine runtime at Google I/O 2011 we engaged Squishable to manufacture the plush gophers. This was the first time the gopher was colored blue and appeared in three dimensions. The first prototype was kinda hairy:\n    But the second one was just right:\n    Around the same time, Renee roughed out a gopher in clay. This inspired a refined sculpture that became a vinyl figurine made by Kidrobot. The vinyls were first distributed at OSCON 2011.\n    The gopher therefore exists in many forms, but has always been Renee\'s creation. It stands for the Go project and Go programmers everywhere, and is one of the most popular things in the Go world.\n    The Go gopher is a character; a unique creation. Not any old gopher, just as Snoopy is not any old cartoon dog.\n    The gopher images are Creative Commons Attributions 3.0 licensed. That means you can play with the images but you must give credit to their creator (Renee French) wherever they are used.\n    Here are a few gopher adaptations that people have used as mascots for user group mascots and similar organizations.\n    They\'re cute and we like them, but by the Creative Commons rules the groups should give Renee credit, perhaps as a mention on the user group web site.\n    The vinyl and plush gophers are copyrighted designs; accept no substitutes! But how can you get one? Their natural habitat is near high concentrations of Go programmers, and their worldwide population is growing. They may be purchased from the Google Store, although the supply can be irregular. (These elusive creatures have been spotted in all kinds of places.)\n    Perhaps the best way to get a gopher is to catch one in the wild at a Go conference. There are two big chances this year: GopherCon (Denver, April 24-26) and dotGo (Paris, October 10).\n    (Photo by Noah Lorang.)\n		\n			By Rob Pike and Andrew Gerrand\n		\n	','https://blog.golang.org/gopher','2014-03-24','2017-05-07 13:49:59.928901'),(498,'GopherCon 2014 Wrap Up','Andrew Gerrand','		GopherCon 2014 Wrap Up\n		28 May 2014\n		\n    In April this year 700 gophers descended upon Denver to attend GopherCon,\n    the world\'s first large-scale Go conference, organized entirely by the community.\n    The three day event featured 24 talks and one panel discussion in a single track over two days,\n    followed by an informal \"hack day\" full of code, conversation,\n    and more than 4 hours (!) of lightning talks.\n    A complete set of video recordings is now available\n    (the slides are here).\n    Two keynotes framed the conference:\n    Rob Pike\'s opening talk \"Hello, Gophers!\" (slides) discusses the history of Go by walking through two of the first Go programs.\n    Andrew Gerrand\'s closing talk \"Go for gophers\" (slides) explains the Go design philosophy through the lens of his personal experience learning the language.\n    One talk that resonated with members of the Go team was Peter Bourgon\'s\n    \"Best practices for Production Environments\"\n    (notes).\n    From deployment to dependency management, it answers many frequently asked questions about Go use in the real world and is a must-see for anyone serious about building systems in Go.\n    But, really, you should just watch them all.\n    They\'re great.\n    The Go Gopher was everywhere.\n    Each attendee received one of the new pink and purple gophers,\n    which now accompany the original blue one:\n    The gopher was also seen wearing a cape on the side of the incredible CoreOS bus:\n    Most of the Go team were at the conference,\n    and we were moved by the passion and dedication of the Go community.\n    It was a thrill to see the many different ways people are using the language.\n    It was also great to put faces to many of the names we know well through their contributions to the project.\n    We would like to extend our thanks and congratulations to the GopherCon organizers (Brian Ketelsen and Erik St. Martin), the excellent speakers, and the tireless volunteers that pitched in to make the event such a success.\n    We look forward to GopherCon 2015, which promises to be bigger and better still.\n    But if you really can\'t wait, we\'ll see you at dotGo in Paris on the 10th of October!\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/gophercon','2014-05-28','2017-05-07 13:49:59.968050'),(499,'Go 1.3 is released','Andrew Gerrand','		Go 1.3 is released\n		18 June 2014\n		\n    Today we are happy to announce the release of Go 1.3.\n    This release comes six months after our last major release and provides better\n    performance, improved tools, support for running Go in new environments, and more.\n    All Go users should upgrade to Go 1.3.\n    You can grab the release from our downloads page and\n    find the full list of improvements and fixes in the\n    release notes.\n    What follows are some highlights.\n    Godoc,\n    the Go documentation server, now performs static analysis.\n    When enabled with the -analysis flag, analysis results are presented\n    in both the source and package documentation views, making it easier\n    than ever to navigate and understand Go programs.\n    See the documentation for the details.\n    The gc toolchain now supports the Native Client (NaCl) execution sandbox on the\n    32- and 64-bit Intel architectures.\n    This permits the safe execution of untrusted code, useful in environments such as the\n    Playground.\n    To set up NaCl on your system see the NativeClient wiki page.\n    Also included in this release is experimental support for the DragonFly BSD,\n    Plan 9, and Solaris operating systems. To use Go on these systems you must\n    install from source.\n    Changes to the runtime have improved the\n    performance of Go binaries,\n    with an improved garbage collector, a new\n    \"contiguous\" goroutine stack management strategy,\n    a faster race detector, and improvements to the regular expression engine.\n    As part of the general overhaul of the Go\n    linker, the compilers and linkers have been refactored. The instruction\n    selection phase that was part of the linker has been moved to the compiler.\n    This can speed up incremental builds for large projects.\n    The garbage collector is now\n    precise when examining stacks (collection of the heap has been precise since Go\n    1.1), meaning that a non-pointer value such as an integer will never be\n    mistaken for a pointer and prevent unused memory from being reclaimed. This\n    change affects code that uses package unsafe; if you have unsafe code you\n    should read the release notes\n    carefully to see if your code needs updating.\n    We would like to thank the many people who contributed to this release;\n    it would not have been possible without your help.\n    So, what are you waiting for?\n    Head on over to the downloads page and start hacking.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go1.3','2014-06-18','2017-05-07 13:49:59.987228'),(500,'Go will be at OSCON 2014','Francesc Campoy','		Go will be at OSCON 2014\n		15 July 2014\n		\n    OSCON, the Open Source Convention, is taking place\n    from July 20th to the 29th in Portland, Oregon and Go will be central to many\n    talks. If you are attending make sure you add these to your personal schedule.\n    On Monday you\'ll have the chance to learn Go in these two tutorials:\n    Getting Started with Go by Steve Francia (MongoDB) at 9:00am on Monday, 07/21/2014\n    A Quick Introduction to System Tools Programming with Go by Chris McEniry (Sony Network Entertainment) at 1:30pm on Monday, 07/21/2014\n    During the rest of the week you can hear how different projects use Go:\n    A Recovering Java Developer Learns to Go by Matt Stine (Pivotal) at 1:40pm on Tuesday, 07/22/2014\n    Painless Data Storage with MongoDB and Go by Steve Francia (MongoDB) and Gustavo Niemeyer (Canonical) at 1:40pm on Tuesday, 07/22/2014\n    Gophers with Hammers: Fun with Parsing and Generating Go by Josh Bleecher Snyder (PayPal) at 2:30pm on Tuesday, 07/22/2014\n    Go for Object Oriented Programmers (or OO Programming without Objects) by Steve Francia (MongoDB) at 4:10pm on Wednesday, 07/23/2014\n    Inside the Go Tour by Francesc Campoy Flores (Google Inc.) at 11:50am on Thursday, 07/24/2014\n    And if you have any questions come to the\n    Go office hours\n    on Wednesday or come anytime by the Google booth.\n    See you at OSCON!\n		\n			By Francesc Campoy\n		\n	','https://blog.golang.org/oscon','2014-07-15','2017-05-07 13:50:00.019424'),(501,'GolangUK 2015','Francesc Campoy','		GolangUK 2015\n		9 October 2015\n		\n    On August 21st the Go community gathered in London for the first edition of\n    Golang UK. The conference featured two parallel\n    tracks and nearly 400 gophers attended.\n    The conference started with the opening keynote by David Calavera\n    called Crossing the Language Chasm (video)\n    and continued with two concurrently executed tracks.\n    Main track:\n    Stupid Gopher Tricks, by Andrew Gerrand (video)\n    Complex Concurrency Patterns in Go, by Evan Huus (video)\n    Code Analysis [no reading required], by Francesc Campoy (video)\n    Go kit: a toolkit for microservices Peter Bourgon (video)\n    Dependency Management Conundrum, by William Kennedy (video)\n    Side track:\n    Building APIs, by Mat Ryer (video)\n    Building a Bank with Go, by Matt Heath (video)\n    CockroachDB: Make Data Easy, by Ben Darnell (video)\n    Understanding memory allocation in Go, by Dean Elbaz (video)\n    Whispered Secrets, by Eleanor McHugh (video)\n    Finally Damian Gryski took the stage for the\n    closing keynote (video),\n    giving an overview of how the Go community has evolved over time and hinting\n    to what the future might look like.\n    On the day before the conference William Kennedy\n    gave a full day Go workshop.\n    It was a great conference, so congratulations to the organizers and see you next year in London!\n		\n			By Francesc Campoy\n		\n	','https://blog.golang.org/gouk15','2015-10-09','2017-05-07 13:50:00.043901'),(502,'Go Concurrency Patterns: Context','Sameer Ajmani','		Go Concurrency Patterns: Context\n		29 July 2014\n		\n  Introduction\n    In Go servers, each incoming request is handled in its own goroutine.\n    Request handlers often start additional goroutines to access backends such as\n    databases and RPC services.\n    The set of goroutines working on a request typically needs access to\n    request-specific values such as the identity of the end user, authorization\n    tokens, and the request\'s deadline.\n    When a request is canceled or times out, all the goroutines working on that\n    request should exit quickly so the system can reclaim any resources they are\n    using.\n    At Google, we developed a context package that makes it easy to pass\n    request-scoped values, cancelation signals, and deadlines across API boundaries\n    to all the goroutines involved in handling a request.\n    The package is publicly available as\n    context.\n    This article describes how to use the package and provides a complete working\n    example.\n  Context\n    The core of the context package is the Context type:\n	\n// A Context carries a deadline, cancelation signal, and request-scoped values\n// across API boundaries. Its methods are safe for simultaneous use by multiple\n// goroutines.\ntype Context interface {\n    // Done returns a channel that is closed when this Context is canceled\n    // or times out.\n    Done() &lt;-chan struct{}\n    // Err indicates why this context was canceled, after the Done channel\n    // is closed.\n    Err() error\n    // Deadline returns the time when this Context will be canceled, if any.\n    Deadline() (deadline time.Time, ok bool)\n    // Value returns the value associated with key or nil if none.\n    Value(key interface{}) interface{}\n}\n    (This description is condensed; the\n    godoc is authoritative.)\n    The Done method returns a channel that acts as a cancelation signal to\n    functions running on behalf of the Context: when the channel is closed, the\n    functions should abandon their work and return.\n    The Err method returns an error indicating why the Context was canceled.\n    The Pipelines and Cancelation article discusses the Done\n    channel idiom in more detail.\n    A Context does not have a Cancel method for the same reason the Done\n    channel is receive-only: the function receiving a cancelation signal is usually\n    not the one that sends the signal.\n    In particular, when a parent operation starts goroutines for sub-operations,\n    those sub-operations should not be able to cancel the parent.\n    Instead, the WithCancel function (described below) provides a way to cancel a\n    new Context value.\n    A Context is safe for simultaneous use by multiple goroutines.\n    Code can pass a single Context to any number of goroutines and cancel that\n    Context to signal all of them.\n    The Deadline method allows functions to determine whether they should start\n    work at all; if too little time is left, it may not be worthwhile.\n    Code may also use a deadline to set timeouts for I/O operations.\n    Value allows a Context to carry request-scoped data.\n    That data must be safe for simultaneous use by multiple goroutines.\n  Derived contexts\n    The context package provides functions to derive new Context values from\n    existing ones.\n    These values form a tree: when a Context is canceled, all Contexts derived\n    from it are also canceled.\n    Background is the root of any Context tree; it is never canceled:\n	\n// Background returns an empty Context. It is never canceled, has no deadline,\n// and has no values. Background is typically used in main, init, and tests,\n// and as the top-level Context for incoming requests.\nfunc Background() Context\n    WithCancel and WithTimeout return derived Context values that can be\n    canceled sooner than the parent Context.\n    The Context associated with an incoming request is typically canceled when the\n    request handler returns.\n    WithCancel is also useful for canceling redundant requests when using multiple\n    replicas.\n    WithTimeout is useful for setting a deadline on requests to backend servers:\n	\n// WithCancel returns a copy of parent whose Done channel is closed as soon as\n// parent.Done is closed or cancel is called.\nfunc WithCancel(parent Context) (ctx Context, cancel CancelFunc)\n// A CancelFunc cancels a Context.\ntype CancelFunc func()\n// WithTimeout returns a copy of parent whose Done channel is closed as soon as\n// parent.Done is closed, cancel is called, or timeout elapses. The new\n// Context\'s Deadline is the sooner of now+timeout and the parent\'s deadline, if\n// any. If the timer is still running, the cancel function releases its\n// resources.\nfunc WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc)\n    WithValue provides a way to associate request-scoped values with a Context:\n	\n// WithValue returns a copy of parent whose Value method returns val for key.\nfunc WithValue(parent Context, key interface{}, val interface{}) Context\n    The best way to see how to use the context package is through a worked\n    example.\n  Example: Google Web Search\n    Our example is an HTTP server that handles URLs like\n    /search?q=golang&amp;timeout=1s by forwarding the query \"golang\" to the\n    Google Web Search API and\n    rendering the results.\n    The timeout parameter tells the server to cancel the request after that\n    duration elapses.\n    The code is split across three packages:\n    server provides the main function and the handler for /search.\n    userip provides functions for extracting a user IP address from a request and associating it with a Context.\n    google provides the Search function for sending a query to Google.\n  The server program\n    The server program handles requests like\n    /search?q=golang by serving the first few Google search results for golang.\n    It registers handleSearch to handle the /search endpoint.\n    The handler creates an initial Context called ctx and arranges for it to be\n    canceled when the handler returns.\n    If the request includes the timeout URL parameter, the Context is canceled\n    automatically when the timeout elapses:\n	\nfunc handleSearch(w http.ResponseWriter, req *http.Request) {\n    // ctx is the Context for this handler. Calling cancel closes the\n    // ctx.Done channel, which is the cancellation signal for requests\n    // started by this handler.\n    var (\n        ctx    context.Context\n        cancel context.CancelFunc\n    )\n    timeout, err := time.ParseDuration(req.FormValue(\"timeout\"))\n    if err == nil {\n        // The request has a timeout, so create a context that is\n        // canceled automatically when the timeout expires.\n        ctx, cancel = context.WithTimeout(context.Background(), timeout)\n    } else {\n        ctx, cancel = context.WithCancel(context.Background())\n    }\n    defer cancel() // Cancel ctx as soon as handleSearch returns.\n    The handler extracts the query from the request and extracts the client\'s IP\n    address by calling on the userip package.\n    The client\'s IP address is needed for backend requests, so handleSearch\n    attaches it to ctx:\n	\n    // Check the search query.\n    query := req.FormValue(\"q\")\n    if query == \"\" {\n        http.Error(w, \"no query\", http.StatusBadRequest)\n        return\n    }\n    // Store the user IP in ctx for use by code in other packages.\n    userIP, err := userip.FromRequest(req)\n    if err != nil {\n        http.Error(w, err.Error(), http.StatusBadRequest)\n        return\n    }\n    ctx = userip.NewContext(ctx, userIP)\n    The handler calls google.Search with ctx and the query:\n	\n    // Run the Google search and print the results.\n    start := time.Now()\n    results, err := google.Search(ctx, query)\n    elapsed := time.Since(start)\n    If the search succeeds, the handler renders the results:\n	\n    if err := resultsTemplate.Execute(w, struct {\n        Results          google.Results\n        Timeout, Elapsed time.Duration\n    }{\n        Results: results,\n        Timeout: timeout,\n        Elapsed: elapsed,\n    }); err != nil {\n        log.Print(err)\n        return\n    }\n  Package userip\n    The userip package provides functions for\n    extracting a user IP address from a request and associating it with a Context.\n    A Context provides a key-value mapping, where the keys and values are both of\n    type interface{}.\n    Key types must support equality, and values must be safe for simultaneous use by\n    multiple goroutines.\n    Packages like userip hide the details of this mapping and provide\n    strongly-typed access to a specific Context value.\n    To avoid key collisions, userip defines an unexported type key and uses\n    a value of this type as the context key:\n	\n// The key type is unexported to prevent collisions with context keys defined in\n// other packages.\ntype key int\n// userIPkey is the context key for the user IP address.  Its value of zero is\n// arbitrary.  If this package defined other context keys, they would have\n// different integer values.\nconst userIPKey key = 0\n    FromRequest extracts a userIP value from an http.Request:\n	\nfunc FromRequest(req *http.Request) (net.IP, error) {\n    ip, _, err := net.SplitHostPort(req.RemoteAddr)\n    if err != nil {\n        return nil, fmt.Errorf(\"userip: %q is not IP:port\", req.RemoteAddr)\n    }\n    NewContext returns a new Context that carries a provided userIP value:\n	\nfunc NewContext(ctx context.Context, userIP net.IP) context.Context {\n    return context.WithValue(ctx, userIPKey, userIP)\n}\n    FromContext extracts a userIP from a Context:\n	\nfunc FromContext(ctx context.Context) (net.IP, bool) {\n    // ctx.Value returns nil if ctx has no value for the key;\n    // the net.IP type assertion returns ok=false for nil.\n    userIP, ok := ctx.Value(userIPKey).(net.IP)\n    return userIP, ok\n}\n  Package google\n    The google.Search function makes an HTTP request\n    to the Google Web Search API\n    and parses the JSON-encoded result.\n    It accepts a Context parameter ctx and returns immediately if ctx.Done is\n    closed while the request is in flight.\n    The Google Web Search API request includes the search query and the user IP as\n    query parameters:\n	\nfunc Search(ctx context.Context, query string) (Results, error) {\n    // Prepare the Google Search API request.\n    req, err := http.NewRequest(\"GET\", \"https://ajax.googleapis.com/ajax/services/search/web?v=1.0\", nil)\n    if err != nil {\n        return nil, err\n    }\n    q := req.URL.Query()\n    q.Set(\"q\", query)\n    // If ctx is carrying the user IP address, forward it to the server.\n    // Google APIs use the user IP to distinguish server-initiated requests\n    // from end-user requests.\n    if userIP, ok := userip.FromContext(ctx); ok {\n        q.Set(\"userip\", userIP.String())\n    }\n    req.URL.RawQuery = q.Encode()\n    Search uses a helper function, httpDo, to issue the HTTP request and cancel\n    it if ctx.Done is closed while the request or response is being processed.\n    Search passes a closure to httpDo handle the HTTP response:\n	\n    var results Results\n    err = httpDo(ctx, req, func(resp *http.Response, err error) error {\n        if err != nil {\n            return err\n        }\n        defer resp.Body.Close()\n        // Parse the JSON search result.\n        // https://developers.google.com/web-search/docs/#fonje\n        var data struct {\n            ResponseData struct {\n                Results []struct {\n                    TitleNoFormatting string\n                    URL               string\n                }\n            }\n        }\n        if err := json.NewDecoder(resp.Body).Decode(&amp;data); err != nil {\n            return err\n        }\n        for _, res := range data.ResponseData.Results {\n            results = append(results, Result{Title: res.TitleNoFormatting, URL: res.URL})\n        }\n        return nil\n    })\n    // httpDo waits for the closure we provided to return, so it\'s safe to\n    // read results here.\n    return results, err\n    The httpDo function runs the HTTP request and processes its response in a new\n    goroutine.\n    It cancels the request if ctx.Done is closed before the goroutine exits:\n	\nfunc httpDo(ctx context.Context, req *http.Request, f func(*http.Response, error) error) error {\n    // Run the HTTP request in a goroutine and pass the response to f.\n    tr := &amp;http.Transport{}\n    client := &amp;http.Client{Transport: tr}\n    c := make(chan error, 1)\n    go func() { c &lt;- f(client.Do(req)) }()\n    select {\n    case &lt;-ctx.Done():\n        tr.CancelRequest(req)\n        &lt;-c // Wait for f to return.\n        return ctx.Err()\n    case err := &lt;-c:\n        return err\n    }\n}\n  Adapting code for Contexts\n    Many server frameworks provide packages and types for carrying request-scoped\n    values.\n    We can define new implementations of the Context interface to bridge between\n    code using existing frameworks and code that expects a Context parameter.\n    For example, Gorilla\'s\n    github.com/gorilla/context\n    package allows handlers to associate data with incoming requests by providing a\n    mapping from HTTP requests to key-value pairs.\n    In gorilla.go, we provide a Context\n    implementation whose Value method returns the values associated with a\n    specific HTTP request in the Gorilla package.\n    Other packages have provided cancelation support similar to Context.\n    For example, Tomb provides a Kill\n    method that signals cancelation by closing a Dying channel.\n    Tomb also provides methods to wait for those goroutines to exit, similar to\n    sync.WaitGroup.\n    In tomb.go, we provide a Context implementation that\n    is canceled when either its parent Context is canceled or a provided Tomb is\n    killed.\n  Conclusion\n    At Google, we require that Go programmers pass a Context parameter as the\n    first argument to every function on the call path between incoming and outgoing\n    requests.\n    This allows Go code developed by many different teams to interoperate well.\n    It provides simple control over timeouts and cancelation and ensures that\n    critical values like security credentials transit Go programs properly.\n    Server frameworks that want to build on Context should provide implementations\n    of Context to bridge between their packages and those that expect a Context\n    parameter.\n    Their client libraries would then accept a Context from the calling code.\n    By establishing a common interface for request-scoped data and cancelation,\n    Context makes it easier for package developers to share code for creating\n    scalable services.\n		\n			By Sameer Ajmani\n		\n	','https://blog.golang.org/context','2014-07-29','2017-05-07 13:50:00.100290'),(503,'Go at OSCON','Francesc Campoy','		Go at OSCON\n		20 August 2014\n		\n  Introduction\n    What happens in Portland in July? OSCON! At\n    this year\'s conference, Go was more present than ever before, with five talks,\n    two workshops, a\n    Birds of a Feather\n    session, and a meetup.\n  Talks\n    Matt Stine talked about his experience switching\n    from Java to Go with\n    A recovering Java developer learns Go\n    while Steve Francia presented\n    Painless Data Storage with MongoDB and Go.\n    Steve also presented\n    Go for Object Oriented Programmers,\n    where he explained how some object oriented concepts can be implemented in Go.\n    Finally, Josh Bleecher Snyder talked about his\n    experience writing tools to work with Go source code in\n    Gophers with hammers,\n    and Francesc Campoy talked about all the things\n    that could have gone wrong and what the Go team did to prevent them\n    Inside the Go playground.\n  Workshops\n    At the beginning of OSCON\'s workshop day, Steve Francia presented how to build a\n    web application and a CLI tool during\n    Getting started with Go to a big\n    room full of Gophers.\n    In the afternoon, Chris McEniry gave his\n    Quick introduction to system tools programming with Go where he went over some useful skills to\n    write system tools using Go and its standard library.\n  Additional events\n    To take advantage of the increased Gopher population in Portland during OSCON, we\n    organized two extra events: the first PDXGolang\n    meetup and a\n    Birds of a Feather session.\n    At the meetup Francesc Campoy talked about\n    Go Best Practices and\n    Kelsey Hightower gave a great\n    introduction to Kubernetes,\n    a container management system for clusters written in Go by Google. If you live\n    in Portland, make sure you join the group and come\n    along to the next meeting.\n    The \"Birds of a Feather\" (or, more aptly, \"Gophers of a Feather\") was a lot of\n    fun for everyone involved. We hope to see more of you there next year.\n  In conclusion\n    Thanks to all the gophers that participated in OSCON. After the successes of\n    this year we look forward to more Go fun at OSCON 2015.\n		\n			By Francesc Campoy\n		\n	','https://blog.golang.org/osconreport','2014-08-20','2017-05-07 13:50:00.315723'),(504,'Constants','Rob Pike','		Constants\n		25 August 2014\n		\n  Introduction\n    Go is a statically typed language that does not permit operations that mix numeric types.\n    You can\'t add a float64 to an int, or even an int32 to an int.\n    Yet it is legal to write 1e6*time.Second or math.Exp(1) or even 1&lt;&lt;(\'\\t\'+2.0).\n    In Go, constants, unlike variables, behave pretty much like regular numbers.\n    This post explains why that is and what it means.\n  Background: C\n    In the early days of thinking about Go, we talked about a number of problems caused by the way C and its descendants let you mix and match numeric types.\n    Many mysterious bugs, crashes, and portability problems are caused by expressions that combine integers of different sizes and \"signedness\".\n    Although to a seasoned C programmer the result of a calculation like\n  unsigned int u = 1e9;\nlong signed int i = -1;\n... i + u ...\n    may be familiar, it isn\'t a priori obvious.\n    How big is the result?\n    What is its value?\n    Is it signed or unsigned?\n    Nasty bugs lurk here.\n    C has a set of rules called \"the usual arithmetic conversions\" and it is an indicator of their subtlety that they have changed over the years (introducing yet more bugs, retroactively).\n    When designing Go, we decided to avoid this minefield by mandating that there is no mixing of numeric types.\n    If you want to add i and u, you must be explicit about what you want the result to be.\n    Given\n  var u uint\nvar i int\n    you can write either uint(i)+u or i+int(u), with both the meaning and type of the addition clearly expressed, but unlike in C you cannot write i+u.\n    You can\'t even mix int and int32, even when int is a 32-bit type.\n    This strictness eliminates a common cause of bugs and other failures.\n    It is a vital property of Go.\n    But it has a cost: it sometimes requires programmers to decorate their code with clumsy numeric conversions to express their meaning clearly.\n    And what about constants?\n    Given the declarations above, what would make it legal to write i = 0 or u = 0?\n    What is the type of 0?\n    It would be unreasonable to require constants to have type conversions in simple contexts such as i = int(0).\n    We soon realized the answer lay in making numeric constants work differently from how they behave in other C-like languages.\n    After much thinking and experimentation, we came up with a design that we believe feels right almost always, freeing the programmer from converting constants all the time yet being able to write things like math.Sqrt(2) without being chided by the compiler.\n    In short, constants in Go just work, most of the time anyway.\n    Let\'s see how that happens.\n  Terminology\n    First, a quick definition.\n    In Go, const is a keyword introducing a name for a scalar value such as 2 or 3.14159 or \"scrumptious\".\n    Such values, named or otherwise, are called constants in Go.\n    Constants can also be created by expressions built from constants, such as 2+3 or 2+3i or math.Pi/2 or (\"go\"+\"pher\").\n    Some languages don\'t have constants, and others have a more general definition of constant or application of the word const.\n    In C and C++, for instance, const is a type qualifier that can codify more intricate properties of more intricate values.\n    But in Go, a constant is just a simple, unchanging value, and from here on we\'re talking only about Go.\n  String constants\n    There are many kinds of numeric constants—integers, floats, runes, signed, unsigned, imaginary, complex—so let\'s start with a simpler form of constant: strings.\n    String constants are easy to understand and provide a smaller space in which to explore the type issues of constants in Go.\n    A string constant encloses some text between double quotes.\n    (Go also has raw string literals, enclosed by backquotes ``, but for the purpose of this discussion they have all the same properties.)\n    Here is a string constant:\n  \"Hello, 世界\"\n    (For much more detail about the representation and interpretation of strings, see this blog post.)\n    What type does this string constant have?\n    The obvious answer is string, but that is wrong.\n    This is an untyped string constant, which is to say it is a constant textual value that does not yet have a fixed type.\n    Yes, it\'s a string, but it\'s not a Go value of type string.\n    It remains an untyped string constant even when given a name:\n  const hello = \"Hello, 世界\"\n    After this declaration, hello is also an untyped string constant.\n    An untyped constant is just a value, one not yet given a defined type that would force it to obey the strict rules that prevent combining differently typed values.\n    It is this notion of an untyped constant that makes it possible for us to use constants in Go with great freedom.\n    So what, then, is a typed string constant?\n    It\'s one that\'s been given a type, like this:\n  const typedHello string = \"Hello, 世界\"\n    Notice that the declaration of typedHello has an explicit string type before the equals sign.\n    This means that typedHello has Go type string, and cannot be assigned to a Go variable of a different type.\n    That is to say, this code works:\n	\n// +build OMIT\npackage main\nimport \"fmt\"\nconst typedHello string = \"Hello, 世界\"\nfunc main() {\n    var s string\n    s = typedHello\n    fmt.Println(s)\n}\n    but this does not:\n	\n// +build OMIT\npackage main\nimport \"fmt\"\nconst typedHello string = \"Hello, 世界\"\nfunc main() {\n    type MyString string\n    var m MyString\n    m = typedHello // Type error\n    fmt.Println(m)\n}\n    The variable m has type MyString and cannot be assigned a value of a different type.\n    It can only be assigned values of type MyString, like this:\n	\n// +build OMIT\npackage main\nimport \"fmt\"\nconst typedHello string = \"Hello, 世界\"\nfunc main() {\n	type MyString string\n	var m MyString\n    const myStringHello MyString = \"Hello, 世界\"\n    m = myStringHello // OK\n    fmt.Println(m)\n}\n    or by forcing the issue with a conversion, like this:\n	\n// +build OMIT\npackage main\nimport \"fmt\"\nconst typedHello string = \"Hello, 世界\"\nfunc main() {\n	type MyString string\n	var m MyString\n    m = MyString(typedHello)\n    fmt.Println(m)\n}\n    Returning to our untyped string constant, it has the helpful property that, since it has no type, assigning it to a typed variable does not cause a type error.\n    That is, we can write\n  m = \"Hello, 世界\"\n    or\n  m = hello\n    because, unlike the typed constants typedHello and myStringHello, the untyped constants \"Hello, 世界\" and hello have no type.\n    Assigning them to a variable of any type compatible with strings works without error.\n    These untyped string constants are strings, of course, so they can only be used where a string is allowed, but they do not have type string.\n  Default type\n    As a Go programmer, you have certainly seen many declarations like\n  str := \"Hello, 世界\"\n    and by now you might be asking, \"if the constant is untyped, how does str get a type in this variable declaration?\"\n    The answer is that an untyped constant has a default type, an implicit type that it transfers to a value if a type is needed where none is provided.\n    For untyped string constants, that default type is obviously string, so\n  str := \"Hello, 世界\"\n    or\n  var str = \"Hello, 世界\"\n    means exactly the same as\n  var str string = \"Hello, 世界\"\n    One way to think about untyped constants is that they live in a kind of ideal space of values, a space less restrictive than Go\'s full type system.\n    But to do anything with them, we need to assign them to variables, and when that happens the variable (not the constant itself) needs a type, and the constant can tell the variable what type it should have.\n    In this example, str becomes a value of type string because the untyped string constant gives the declaration its default type, string.\n    In such a declaration, a variable is declared with a type and initial value.\n    Sometimes when we use a constant, however, the destination of the value is not so clear.\n    For instance consider this statement:\n	\n// +build OMIT\npackage main\nimport \"fmt\"\nfunc main() {\n    fmt.Printf(\"%s\", \"Hello, 世界\")\n}\n    The signature of fmt.Printf is\n  func Printf(format string, a ...interface{}) (n int, err error)\n    which is to say its arguments (after the format string) are interface values.\n    What happens when fmt.Printf is called with an untyped constant is that an interface value is created\n    to pass as an argument, and the concrete type stored for that argument is the default type of the constant.\n    This process is analogous to what we saw earlier when declaring an initialized value using an untyped string constant.\n    You can see the result in this example, which uses the format %v to print the value and %T to print the type of the value being passed to fmt.Printf:\n	\n// +build OMIT\npackage main\nimport \"fmt\"\nconst hello = \"Hello, 世界\"\nfunc main() {\n    fmt.Printf(\"%T: %v\\n\", \"Hello, 世界\", \"Hello, 世界\")\n    fmt.Printf(\"%T: %v\\n\", hello, hello)\n}\n    If the constant has a type, that goes into the interface, as this example shows:\n	\n// +build OMIT\npackage main\nimport \"fmt\"\ntype MyString string\nconst myStringHello MyString = \"Hello, 世界\"\nfunc main() {\n    fmt.Printf(\"%T: %v\\n\", myStringHello, myStringHello)\n}\n    (For more information about how interface values work, see the first sections of this blog post.)\n    In summary, a typed constant obeys all the rules of typed values in Go.\n    On the other hand, an untyped constant does not carry a Go type in the same way and can be mixed and matched more freely.\n    It does, however, have a default type that is exposed when, and only when, no other type information is available.\n  Default type determined by syntax\n    The default type of an untyped constant is determined by its syntax.\n    For string constants, the only possible implicit type is string.\n    For numeric constants, the implicit type has more variety.\n    Integer constants default to int, floating-point constants float64, rune constants to rune (an alias for int32), and imaginary constants to complex128.\n    Here\'s our canonical print statement used repeatedly to show the default types in action:\n	\n// +build OMIT\npackage main\nimport \"fmt\"\nfunc main() {\n    fmt.Printf(\"%T %v\\n\", 0, 0)\n    fmt.Printf(\"%T %v\\n\", 0.0, 0.0)\n    fmt.Printf(\"%T %v\\n\", \'x\', \'x\')\n    fmt.Printf(\"%T %v\\n\", 0i, 0i)\n}\n    (Exercise: Explain the result for \'x\'.)\n  Booleans\n    Everything we said about untyped string constants can be said for untyped boolean constants.\n    The values true and false are untyped boolean constants that can be assigned to any boolean variable,\n    but once given a type, boolean variables cannot be mixed:\n	\n// +build OMIT\npackage main\nimport \"fmt\"\nfunc main() {\n    type MyBool bool\n    const True = true\n    const TypedTrue bool = true\n    var mb MyBool\n    mb = true      // OK\n    mb = True      // OK\n    mb = TypedTrue // Bad\n    fmt.Println(mb)\n}\n    Run the example and see what happens, then comment out the \"Bad\" line and run it again.\n    The pattern here follows exactly that of string constants.\n  Floats\n    Floating-point constants are just like boolean constants in most respects.\n    Our standard example works as expected in translation:\n	\n// +build OMIT\npackage main\nimport \"fmt\"\nfunc main() {\n    type MyFloat64 float64\n    const Zero = 0.0\n    const TypedZero float64 = 0.0\n    var mf MyFloat64\n    mf = 0.0       // OK\n    mf = Zero      // OK\n    mf = TypedZero // Bad\n    fmt.Println(mf)\n}\n    One wrinkle is that there are two floating-point types in Go: float32 and float64.\n    The default type for a floating-point constant is float64, although an untyped floating-point\n    constant can be assigned to a float32 value just fine:\n	\n// +build OMIT\npackage main\nimport \"fmt\"\nfunc main() {\n	const Zero = 0.0\n	const TypedZero float64 = 0.0\n    var f32 float32\n    f32 = 0.0\n    f32 = Zero      // OK: Zero is untyped\n    f32 = TypedZero // Bad: TypedZero is float64 not float32.\n    fmt.Println(f32)\n}\n    Floating-point values are a good place to introduce the concept of overflow, or the range of values.\n    Numeric constants live in an arbitrary-precision numeric space; they are just regular numbers.\n    But when they are assigned to a variable the value must be able to fit in the destination.\n    We can declare a constant with a very large value:\n	\n    const Huge = 1e1000\n    —that\'s just a number, after all—but we can\'t assign it or even print it. This statement won\'t even compile:\n	\n// +build OMIT\npackage main\nimport \"fmt\"\nfunc main() {\n	const Huge = 1e1000\n	// START OMIT\n    fmt.Println(Huge)\n	// STOP OMIT\n}\n    The error is, \"constant 1.00000e+1000 overflows float64\", which is true.\n    But Huge might be useful: we can use it in expressions with other constants and use the value of those expressions if the result\n    can be represented in the range of a float64.\n    The statement,\n	\n// +build OMIT\npackage main\nimport \"fmt\"\nfunc main() {\n	const Huge = 1e1000\n	// START OMIT\n    fmt.Println(Huge / 1e999)\n	// STOP OMIT\n}\n    prints 10, as one would expect.\n    In a related way, floating-point constants may have very high precision, so that arithmetic involving them is more accurate.\n    The constants defined in the math package are given with many more digits than are\n    available in a float64. Here is the definition of math.Pi:\n  Pi    = 3.14159265358979323846264338327950288419716939937510582097494459\n    When that value is assigned to a variable, some of the precision will be lost; the assignment will create the float64 (or float32)\n    value closest to the high-precision value. This snippet\n	\n// +build OMIT\npackage main\nimport (\n	\"fmt\"\n	\"math\"\n)\nfunc main() {\n    pi := math.Pi\n    fmt.Println(pi)\n}\n    prints 3.141592653589793.\n    Having so many digits available means that calculations like Pi/2 or other more intricate evaluations can carry more precision\n    until the result is assigned, making calculations involving constants easier to write without losing precision.\n    It also means that there is no occasion in which the floating-point corner cases like infinities,\n    soft underflows, and NaNs arise in constant expressions.\n    (Division by a constant zero is a compile-time error, and when everything is a number there\'s no such thing as \"not a number\".)\n  Complex numbers\n    Complex constants behave a lot like floating-point constants.\n    Here\'s a version of our now-familiar litany translated into complex numbers:\n	\n// +build OMIT\npackage main\nimport \"fmt\"\nfunc main() {\n    type MyComplex128 complex128\n    const I = (0.0 + 1.0i)\n    const TypedI complex128 = (0.0 + 1.0i)\n    var mc MyComplex128\n    mc = (0.0 + 1.0i) // OK\n    mc = I            // OK\n    mc = TypedI       // Bad\n    fmt.Println(mc)\n}\n    The default type of a complex number is complex128, the larger-precision version composed of two float64 values.\n    For clarity in our example, we wrote out the full expression (0.0+1.0i), but this value can be shortened to 0.0+1.0i,\n    1.0i or even 1i.\n    Let\'s play a trick.\n    We know that in Go, a numeric constant is just a number.\n    What if that number is a complex number with no imaginary part, that is, a real?\n    Here\'s one:\n	\n    const Two = 2.0 + 0i\n    That\'s an untyped complex constant.\n    Even though it has no imaginary part, the syntax of the expression defines it to have default type complex128.\n    Therefore, if we use it to declare a variable, the default type will be complex128. The snippet\n	\n// +build OMIT\npackage main\nimport \"fmt\"\nfunc main() {\n	const Two = 2.0 + 0i\n    s := Two\n    fmt.Printf(\"%T: %v\\n\", s, s)\n}\n    prints complex128: (2+0i).\n    But numerically, Two can be stored in a scalar floating-point number, a float64 or float32, with no loss of information.\n    Thus we can assign Two to a float64, either in an initialization or an assignment, without problems:\n	\n// +build OMIT\npackage main\nimport \"fmt\"\nfunc main() {\n	const Two = 2.0 + 0i\n    var f float64\n    var g float64 = Two\n    f = Two\n    fmt.Println(f, \"and\", g)\n}\n    The output is 2 and 2.\n    Even though Two is a complex constant, it can be assigned to scalar floating-point variables.\n    This ability for a constant to \"cross\" types like this will prove useful.\n  Integers\n    At last we come to integers.\n    They have more moving parts—many sizes, signed or unsigned, and more—but they play by the same rules.\n    For the last time, here is our familiar example, using just int this time:\n	\n// +build OMIT\npackage main\nimport \"fmt\"\nfunc main() {\n    type MyInt int\n    const Three = 3\n    const TypedThree int = 3\n    var mi MyInt\n    mi = 3          // OK\n    mi = Three      // OK\n    mi = TypedThree // Bad\n    fmt.Println(mi)\n}\n    The same example could be built for any of the integer types, which are:\n  int int8 int16 int32 int64\nuint uint8 uint16 uint32 uint64\nuintptr\n    (plus the aliases byte for uint8 and rune for int32).\n    That\'s a lot, but the pattern in the way constants work should be familiar enough by now that you can see how things will play out.\n    As mentioned above, integers come in a couple of forms and each form has its own default type: int for simple constants like 123 or 0xFF or -14\n    and rune for quoted characters like \'a\', \'世\' or \'\\r\'.\n    No constant form has as its default type an unsigned integer type.\n    However, the flexibility of untyped constants means we can initialize unsigned integer variables using simple constants as long as we are clear about the type.\n    It\'s analogous to how we can initialize a float64 using a complex number with zero imaginary part.\n    Here are several different ways to initialize a uint; all are equivalent, but all must mention the type explicitly for the result to be unsigned.\n  var u uint = 17\nvar u = uint(17)\nu := uint(17)\n    Similarly to the range issue mentioned in the section on floating-point values, not all integer values can fit in all integer types.\n    There are two problems that might arise: the value might be too large, or it might be a negative value being assigned to an unsigned integer type.\n    For instance, int8 has range -128 through 127, so constants outside of that range can never be assigned to a variable of type int8:\n	\n// +build OMIT\npackage main\nfunc main() {\n	// START OMIT\n    var i8 int8 = 128 // Error: too large.\n	// STOP OMIT\n	_ = i8\n}\n    Similarly, uint8, also known as byte, has range 0 through 255, so a large or negative constant cannot be assigned to a uint8:\n	\n// +build OMIT\npackage main\nfunc main() {\n	// START OMIT\n    var u8 uint8 = -1 // Error: negative value.\n	// STOP OMIT\n	_ = u8\n}\n    This type-checking can catch mistakes like this one:\n	\n// +build OMIT\npackage main\nfunc main() {\n    type Char byte\n    var c Char = \'世\' // Error: \'世\' has value 0x4e16, too large.\n	_ = c\n}\n    If the compiler complains about your use of a constant, it\'s likely a real bug like this.\n  An exercise: The largest unsigned int\n    Here is an informative little exercise.\n    How do we express a constant representing the largest value that fits in a uint?\n    If we were talking about uint32 rather than uint, we could write\n  const MaxUint32 = 1&lt;&lt;32 - 1\n    but we want uint, not uint32.\n    The int and uint types have equal unspecified numbers of bits, either 32 or 64.\n    Since the number of bits available depends on the architecture, we can\'t just write down a single value.\n    Fans of two\'s-complement arithmetic,\n    which Go\'s integers are defined to use, know that the representation of -1 has all its bits set to 1,\n    so the bit pattern of -1 is internally the same as that of the\n    largest unsigned integer.\n    We therefore might think we could write\n	\n// +build OMIT\npackage main\nfunc main() {\n	// START OMIT\n    const MaxUint uint = -1 // Error: negative value\n	// STOP OMIT\n}\n    but that is illegal because -1 cannot be represented by an unsigned variable; -1 is not in the range of unsigned values.\n    A conversion won\'t help either, for the same reason:\n	\n// +build OMIT\npackage main\nfunc main() {\n	// START OMIT\n    const MaxUint uint = uint(-1) // Error: negative value\n	// STOP OMIT\n}\n    Even though at run-time a value of -1 can be converted to an unsigned integer, the rules\n    for constant conversions forbid this kind of coercion at compile time.\n    That is to say, this works:\n	\n// +build OMIT\npackage main\nfunc main() {\n    var u uint\n    var v = -1\n    u = uint(v)\n	_ = u\n}\n    but only because v is a variable; if we made v a constant, even an untyped constant, we\'d be back in forbidden territory:\n	\n// +build OMIT\npackage main\nfunc main() {\n    var u uint\n    const v = -1\n    u = uint(v) // Error: negative value\n	_ = u\n}\n    We return to our previous approach, but instead of -1 we try ^0, the bitwise negation of an arbitrary number of zero bits.\n    But that fails too, for a similar reason:\n    In the space of numeric values,\n    ^0 represents an infinite number of ones, so we lose information if we assign that to any fixed-size integer:\n	\n// +build OMIT\npackage main\nfunc main() {\n	// START OMIT\n    const MaxUint uint = ^0 // Error: overflow\n	// STOP OMIT\n}\n    How then do we represent the largest unsigned integer as a constant?\n    The key is to constrain the operation to the number of bits in a uint and avoiding\n    values, such as negative numbers, that are not representable in a uint.\n    The simplest uint value is the typed constant uint(0).\n    If uints have 32 or 64 bits, uint(0) has 32 or 64 zero bits accordingly.\n    If we invert each of those bits, we\'ll get the correct number of one bits, which is the largest uint value.\n    Therefore we don\'t flip the bits of the untyped constant 0, we flip the bits of the typed constant uint(0).\n    Here, then, is our constant:\n	\n// +build OMIT\npackage main\nimport \"fmt\"\nfunc main() {\n    const MaxUint = ^uint(0)\n    fmt.Printf(\"%x\\n\", MaxUint)\n}\n    Whatever the number of bits it takes to represent a uint in the current execution environment\n    (on the playground, it\'s 32),\n    this constant correctly represents the largest value a variable of type uint can hold.\n    If you understand the analysis that got us to this result, you understand all the important points about constants in Go.\n  Numbers\n    The concept of untyped constants in Go means that all the numeric constants, whether integer, floating-point, complex, or even character values,\n    live in a kind of unified space.\n    It\'s when we bring them to the computational world of variables, assignments, and operations that the actual types matter.\n    But as long as we stay in the world of numeric constants, we can mix and match values as we like.\n    All these constants have numeric value 1:\n  1\n1.000\n1e3-99.0*10-9\n\'\\x01\'\n\'\\u0001\'\n\'b\' - \'a\'\n1.0+3i-3.0i\n    Therefore, although they have different implicit default types, written as untyped constants they can be assigned to a variable of any integer type:\n	\n// +build OMIT\npackage main\nimport \"fmt\"\nfunc main() {\n    var f float32 = 1\n    var i int = 1.000\n    var u uint32 = 1e3 - 99.0*10.0 - 9\n    var c float64 = \'\\x01\'\n    var p uintptr = \'\\u0001\'\n    var r complex64 = \'b\' - \'a\'\n    var b byte = 1.0 + 3i - 3.0i\n    fmt.Println(f, i, u, c, p, r, b)\n}\n    The output from this snippet is: 1 1 1 1 1 (1+0i) 1.\n    You can even do nutty stuff like\n	\n// +build OMIT\npackage main\nimport \"fmt\"\nfunc main() {\n    var f = \'a\' * 1.5\n    fmt.Println(f)\n}\n    which yields 145.5, which is pointless except to prove a point.\n    But the real point of these rules is flexibility.\n    That flexibility means that, despite the fact that in Go it is illegal in the same expression to mix floating-point and integer variables,\n    or even int and int32 variables, it is fine to write\n  sqrt2 := math.Sqrt(2)\n    or\n  const millisecond = time.Second/1e3\n    or\n  bigBufferWithHeader := make([]byte, 512+1e6)\n    and have the results mean what you expect.\n    Because in Go, numeric constants work as you expect: like numbers.\n		\n			By Rob Pike\n		\n	','https://blog.golang.org/constants','2014-08-25','2017-05-07 13:50:00.344961'),(505,'A conversation with the Go team','','		A conversation with the Go team\n		6 June 2013\n		\n    At Google I/O 2013, several members of the Go team hosted a \"Fireside chat.\"\n    Robert Griesemer, Rob Pike, David Symonds, Andrew Gerrand, Ian Lance Taylor,\n    Sameer Ajmani, Brad Fitzpatrick, and Nigel Tao took questions from the audience\n    and people around the world about various aspects of the Go project.\n    We also hosted a similar session at I/O last year:\n    Meet the Go team.\n    There were many more questions from Google Moderator than we were able to\n    answer in the short 40 minute session.\n    Here we answer some of those we missed in the live session.\n    Linking speed (and memory usage) for the gc toolchain are a known problem.\n    Are there any plans to address this during the 1.2 cycle?\n    Rob: Yes. We are always thinking about ways to improve performance of the\n    tools as well as the language and libraries.\n    I have been very pleased to see how quickly Go appears to be gaining traction.\n    Can you talk about the reactions you have experienced working with other\n    developers inside and outside Google? Are there any major sticking points remaining?\n    Robert: A lot of developers that seriously tried Go are very happy with it.\n    Many of them report a much smaller, more readable and thus maintainable code\n    base: A 50% code size reduction or more when coming from C++ seems common.\n    Developers that switched to Go from Python are invariably pleased with the\n    performance gain. The typical complaints are about small inconsistencies in the\n    language (some of which we might iron out at some point). What surprises me is\n    that almost nobody complains about the lack of generics.\n    When will Go be a first-class language for Android development?\n    Andrew: This would be great, but we don\'t have anything to announce.\n    Is there a roadmap for the next version of Go?\n    Andrew: We have no feature roadmap as such. The contributors tend to work on\n    what interests them. Active areas of development include the gc and gccgo\n    compilers, the garbage collector and runtime, and many others. We expect the\n    majority of exciting new additions will be in the form of improvements to our\n    tools. You can find design discussions and code reviews on the\n    golang-dev mailing list.\n    As for the timeline, we do have\n    concrete plans:\n    we expect to release Go 1.2 on December 1, 2013.\n    Where do you guys want to see Go used externally?\n    What would you consider a big win for Go adoption outside Google?\n    Where do you think Go has the potential to make a significant impact?\n    Rob: Where Go is deployed is up to its users, not to us. We\'re happy to see\n    it gain traction anywhere it helps. It was designed with server-side software\n    in mind, and is showing promise there, but has also shown strengths in many\n    other areas and the story is really just beginning. There are many surprises to\n    come.\n    Ian: It’s easier for startups to use Go, because they don’t have an\n    entrenched code base that they need to work with. So I see two future big wins\n    for Go. One would be a significant use of Go by an existing large software\n    company other than Google. Another would be a significant IPO or acquisition\n    of a startup that primarily uses Go. These are both indirect: clearly choice\n    of programming language is a very small factor in the success of a company.\n    But it would be another way to show that Go can be part of a successful\n    software system.\n    Have you thought any (more) about the potential of dynamically loading\n    Go packages or objects and how it could work in Go?\n    I think this could enable some really interesting and expressive constructs,\n    especially coupled with interfaces.\n    Rob: This is an active topic of discussion. We appreciate how powerful the\n    concept can be and hope we can find a way to implement it before too long.\n    There are serious challenges in the design approach to take and the need to\n    make it work portably.\n    There was a discussion a while ago about collecting some best-of-breed\n    database/sql drivers in a more central place.\n    Some people had strong opinions to the contrary though.\n    Where is database/sql and its drivers going in the next year?\n    Brad: While we could create an official subrepo (“go.db”) for database\n    drivers, we fear that would unduly bless certain drivers. At this point we’d\n    still rather see healthy competition between different drivers. The\n    SQLDrivers wiki page\n    lists some good ones.\n    The database/sql package didn’t get much attention for a while, due to lack of\n    drivers. Now that drivers exist, usage of the package is increasing and\n    correctness and performance bugs are now being reported (and fixed). Fixes will\n    continue, but no major changes to the interface of database/sql are planned.\n     There might be small extensions here and there as needed for performance or to\n    assist some drivers.\n    What is the status of versioning?\n    Is importing some code from github a best practice recommended by the Go team?\n    What happens when we publish our code that is dependent on a github repo and\n    the API of the dependee changes?\n    Ian: This is frequently discussed on the mailing list. What we do internally\n    is take a snapshot of the imported code, and update that snapshot from time to\n    time. That way, our code base won\'t break unexpectedly if the API changes.\n    But we understand that that approach doesn’t work very well for people who are\n    themselves providing a library. We’re open to good suggestions in this area.\n    Remember that this is an aspect of the tools that surround the language rather\n    than the language itself; the place to fix this is in the tools, not the\n    language.\n    What about Go and Graphical User Interfaces?\n    Rob: This is a subject close to my heart. Newsqueak, a very early precursor\n    language, was designed specifically for writing graphics programs (that\'s what\n    we used to call apps). The landscape has changed a lot but I think Go\'s\n    concurrency model has much to offer in the field of interactive graphics.\n    Andrew: There are many\n    bindings for existing graphics libraries\n    out there, and a few Go-specific projects. One of the more promising ones is\n    go.uik, but it\'s still in its early\n    days. I think there\'s a lot of potential for a great Go-specific UI toolkit for\n    writing native applications (consider handling user events by receiving from a\n    channel), but developing a production-quality package is a significant\n    undertaking. I have no doubt one will come in time.\n    In the meantime, the web is the most broadly available platform for user\n    interfaces. Go provides great support for building web apps, albeit only on the\n    back end.\n    In the mailing lists Adam Langley has stated that the TLS code has not been\n    reviewed by outside groups, and thus should not be used in production.\n    Are there plans to have the code reviewed?\n    A good secure implementation of concurrent TLS would be very nice.\n    Adam: Cryptography is notoriously easy to botch in subtle and surprising ways\n    and I’m only human. I don’t feel that I can warrant that Go’s TLS code is\n    flawless and I wouldn’t want to misrepresent it.\n    There are a couple of places where the code is known to have side-channel\n    issues: the RSA code is blinded but not constant time, elliptic curves other\n    than P-224 are not constant time and the Lucky13 attack might work. I hope to\n    address the latter two in the Go 1.2 timeframe with a constant-time P-256\n    implementation and AES-GCM.\n    Nobody has stepped forward to do a review of the TLS stack however and I’ve not\n    investigated whether we could get Matasano or the like to do it. That depends\n    on whether Google wishes to fund it.\n    What do you think about GopherCon 2014?\n    Does anyone from the team plan to attend?\n    Andrew: It\'s very exciting. I\'m sure some of us will be there.\n		\n	','https://blog.golang.org/a-conversation-with-the-go-team','2013-06-06','2017-05-07 13:50:00.404485'),(506,'Go and the Google Cloud Platform','Andrew Gerrand','		Go and the Google Cloud Platform\n		12 June 2013\n		\n  Introduction\n    In 2011 we announced the Go runtime for App Engine. Since then, we have continued to improve the Go App Engine experience, and generally improved Go support for the Google Cloud Platform. For instance, the google-api-go-client provides a Go interface to a range of Google\'s public APis, including Compute Engine, Cloud Storage, BigQuery, Drive, and many more.\n    Learn more by watching these talks from Google I/O this year:\n  High Performance Apps with Go on App Engine\n    The Go runtime for App Engine is a high performance engine for\n    running web applications. It produces fast responses,\n    starts instances in a fraction of a second, makes the most use\n    of instance hours, and allows your app to do serious processing\n    at full machine speed.\n    Come along to hear how to fully exploit the power of Go on App\n    Engine and make your web applications the best they can be.\n  All the Ships in the World\n    Visualizing Data with Google Cloud and Maps\n    Tens of thousands of ships report their position at least once\n    every 5 minutes, 24 hours a day.\n    Visualizing that quantity of data and serving it out to large\n    numbers of people takes lots of power both in the browser and on the server.\n    This session will explore the use of Maps,\n    App Engine, Go, Compute Engine, BigQuery, Cloud Storage,\n    and WebGL to do massive data visualization.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go-and-google-cloud-platform','2013-06-12','2017-05-07 13:50:00.416402'),(507,'The first Go program','Andrew Gerrand','		The first Go program\n		18 July 2013\n		\n    Brad Fitzpatrick and I (Andrew Gerrand) recently started restructuring\n    godoc, and it occurred to me that it is one\n    of the oldest Go programs.\n    Robert Griesemer started writing it back in early 2009,\n    and we\'re still using it today.\n    When I tweeted about\n    this, Dave Cheney replied with an interesting question:\n    what is the oldest Go program? Rob Pike dug into his mail and found it\n    in an old message to Robert and Ken Thompson.\n    What follows is the first Go program. It was written by Rob in February 2008,\n    when the team was just Rob, Robert, and Ken. They had a solid feature list\n    (mentioned in this blog post)\n    and a rough language specfication. Ken had just finished the first working version of\n    a Go compiler (it didn\'t produce native code, but rather transliterated Go code\n    to C for fast prototyping) and it was time to try writing a program with it.\n    Rob sent mail to the \"Go team\":\n  From: Rob \'Commander\' Pike\nDate: Wed, Feb 6, 2008 at 3:42 PM\nTo: Ken Thompson, Robert Griesemer\nSubject: slist\nit works now.\nroro=% a.out\n(defn foo (add 12 34))\nreturn: icounter = 4440\nroro=%\nhere\'s the code.\nsome ugly hackery to get around the lack of strings.\n    (The icounter line in the program output is the number of executed\n    statements, printed for debugging.)\n	\npackage main\n// fake stuff\ntype char uint8;\n// const char TESTSTRING[] = \"(defn foo (add \'a \'b))\\n\";\ntype Atom struct {\n        string  *[100]char;\n        integer int;\n        next    *Slist;  /* in hash bucket */\n}\ntype List struct {\n        car     *Slist;\n        cdr     *Slist;\n}\ntype Slist struct {\n        isatom          bool;\n        isstring        bool;\n        //union {\n        atom    Atom;\n        list    List;\n        //} u;\n        Free method();\n        Print method();\n        PrintOne method(doparen bool);\n        String method(*char &lt;-);\n        Integer method(int &lt;-);\n        Car method(*Slist &lt;-);\n        Cdr method(*Slist &lt;-);\n}\nmethod (this *Slist) Car(*Slist &lt;-) {\n        return this.list.car;\n}\nmethod (this *Slist) Cdr(*Slist &lt;-) {\n        return this.list.cdr;\n}\nmethod (this *Slist) String(*[100]char &lt;-) {\n        return this.atom.string;\n}\nmethod (this *Slist) Integer(int &lt;-) {\n        return this.atom.integer;\n}\nfunction OpenFile();\nfunction Parse(*Slist &lt;-);\n//Slist* atom(char *s, int i);\nvar token int;\nvar peekc int = -1;\nvar lineno int32 = 1;\nvar input [100*1000]char;\nvar inputindex int = 0;\nvar tokenbuf [100]char;\nvar EOF int = -1;  // BUG should be const\nfunction main(int32 &lt;-) {\n        var list *Slist;\n        OpenFile();\n        for ;; {\n                list = Parse();\n                if list == nil {\n                        break;\n                }\n                list.Print();\n                list.Free();\n                break;\n        }\n        return 0;\n}\nmethod (slist *Slist) Free(&lt;-) {\n        if slist == nil {\n                return;\n        }\n        if slist.isatom {\n//              free(slist.String());\n        } else {\n                slist.Car().Free();\n                slist.Cdr().Free();\n        }\n//      free(slist);\n}\nmethod (slist *Slist) PrintOne(&lt;- doparen bool) {\n        if slist == nil {\n                return;\n        }\n        if slist.isatom {\n                if slist.isstring {\n                        print(slist.String());\n                } else {\n                        print(slist.Integer());\n                }\n        } else {\n                if doparen {\n                        print(\"(\");\n                }\n                slist.Car().PrintOne(true);\n                if slist.Cdr() != nil {\n                        print(\" \");\n                        slist.Cdr().PrintOne(false);\n                }\n                if doparen {\n                        print(\")\");\n                }\n        }\n}\nmethod (slist *Slist) Print() {\n        slist.PrintOne(true);\n        print \"\\n\";\n}\nfunction Get(int &lt;-) {\n        var c int;\n        if peekc &gt;= 0 {\n                c = peekc;\n                peekc = -1;\n        } else {\n                c = convert(int, input[inputindex]);\n                inputindex = inputindex + 1; // BUG should be incr one expr\n                if c == \'\\n\' {\n                        lineno = lineno + 1;\n                }\n                if c == \'\\0\' {\n                        inputindex = inputindex - 1;\n                        c = EOF;\n                }\n        }\n        return c;\n}\nfunction WhiteSpace(bool &lt;- c int) {\n        return c == \' \' || c == \'\\t\' || c == \'\\r\' || c == \'\\n\';\n}\nfunction NextToken() {\n        var i, c int;\n        var backslash bool;\n        tokenbuf[0] = \'\\0\';     // clear previous token\n        c = Get();\n        while WhiteSpace(c)  {\n                c = Get();\n        }\n        switch c {\n                case EOF:\n                        token = EOF;\n                case \'(\':\n                case \')\':\n                        token = c;\n                        break;\n                case:\n                        for i = 0; i &lt; 100 - 1; {  // sizeof tokenbuf - 1\n                                tokenbuf[i] = convert(char, c);\n                                i = i + 1;\n                                c = Get();\n                                if c == EOF {\n                                        break;\n                                }\n                                if WhiteSpace(c) || c == \')\' {\n                                        peekc = c;\n                                        break;\n                                }\n                        }\n                        if i &gt;= 100 - 1 {  // sizeof tokenbuf - 1\n                                panic \"atom too long\\n\";\n                        }\n                        tokenbuf[i] = \'\\0\';\n                        if \'0\' &lt;= tokenbuf[0] &amp;&amp; tokenbuf[0] &lt;= \'9\' {\n                                token = \'0\';\n                        } else {\n                                token = \'A\';\n                        }\n        }\n}\nfunction Expect(&lt;- c int) {\n        if token != c {\n                print \"parse error: expected \", c, \"\\n\";\n                panic \"parse\";\n        }\n        NextToken();\n}\n// Parse a non-parenthesized list up to a closing paren or EOF\nfunction ParseList(*Slist &lt;-) {\n        var slist, retval *Slist;\n        slist = new(Slist);\n        slist.list.car = nil;\n        slist.list.cdr = nil;\n        slist.isatom = false;\n        slist.isstring = false;\n        retval = slist;\n        for ;; {\n                slist.list.car = Parse();\n                if token == \')\' {       // empty cdr\n                        break;\n                }\n                if token == EOF {       // empty cdr  BUG SHOULD USE ||\n                        break;\n                }\n                slist.list.cdr = new(Slist);\n                slist = slist.list.cdr;\n        }\n        return retval;\n}\nfunction atom(*Slist &lt;- i int) {  // BUG: uses tokenbuf; should take argument\n        var h, length int;\n        var slist, tail *Slist;\n        slist = new(Slist);\n        if token == \'0\' {\n                slist.atom.integer = i;\n                slist.isstring = false;\n        } else {\n                slist.atom.string = new([100]char);\n                var i int;\n                for i = 0; ; i = i + 1 {\n                        (*slist.atom.string)[i] = tokenbuf[i];\n                        if tokenbuf[i] == \'\\0\' {\n                                break;\n                        }\n                }\n                //slist.atom.string = \"hello\"; // BUG! s; //= strdup(s);\n                slist.isstring = true;\n        }\n        slist.isatom = true;\n        return slist;\n}\nfunction atoi(int &lt;-) {  // BUG: uses tokenbuf; should take argument\n        var v int = 0;\n        for i := 0; \'0\' &lt;= tokenbuf[i] &amp;&amp; tokenbuf[i] &lt;= \'9\'; i = i + 1 {\n                v = 10 * v + convert(int, tokenbuf[i] - \'0\');\n        }\n        return v;\n}\nfunction Parse(*Slist &lt;-) {\n        var slist *Slist;\n        if token == EOF || token == \')\' {\n                return nil;\n        }\n        if token == \'(\' {\n                NextToken();\n                slist = ParseList();\n                Expect(\')\');\n                return slist;\n        } else {\n                // Atom\n                switch token {\n                        case EOF:\n                                return nil;\n                        case \'0\':\n                                slist = atom(atoi());\n                        case \'\"\':\n                        case \'A\':\n                                slist = atom(0);\n                        case:\n                                slist = nil;\n                                print \"unknown token\"; //, token, tokenbuf;\n                }\n                NextToken();\n                return slist;\n        }\n        return nil;\n}\nfunction OpenFile() {\n        //strcpy(input, TESTSTRING);\n        //inputindex = 0;\n        // (defn foo (add 12 34))\\n\n        inputindex = 0;\n        peekc = -1;  // BUG\n        EOF = -1;  // BUG\n        i := 0;\n        input[i] = \'(\'; i = i + 1;\n        input[i] = \'d\'; i = i + 1;\n        input[i] = \'e\'; i = i + 1;\n        input[i] = \'f\'; i = i + 1;\n        input[i] = \'n\'; i = i + 1;\n        input[i] = \' \'; i = i + 1;\n        input[i] = \'f\'; i = i + 1;\n        input[i] = \'o\'; i = i + 1;\n        input[i] = \'o\'; i = i + 1;\n        input[i] = \' \'; i = i + 1;\n        input[i] = \'(\'; i = i + 1;\n        input[i] = \'a\'; i = i + 1;\n        input[i] = \'d\'; i = i + 1;\n        input[i] = \'d\'; i = i + 1;\n        input[i] = \' \'; i = i + 1;\n        input[i] = \'1\'; i = i + 1;\n        input[i] = \'2\'; i = i + 1;\n        input[i] = \' \'; i = i + 1;\n        input[i] = \'3\'; i = i + 1;\n        input[i] = \'4\'; i = i + 1;\n        input[i] = \')\'; i = i + 1;\n        input[i] = \')\'; i = i + 1;\n        input[i] = \'\\n\'; i = i + 1;\n        NextToken();\n}\n    The program parses and prints an\n    S-expression.\n    It takes no user input and has no imports, relying only on the built-in\n    print facility for output. \n    It was written literally the first day there was a\n    working but rudimentary compiler.\n    Much of the language wasn\'t implemented and some of it wasn\'t even specified.\n    Still, the basic flavor of the language today is recognizable in this program.\n    Type and variable declarations, control flow, and package statements haven\'t\n    changed much.\n    But there are many differences and absences.\n    Most significant are the lack of concurrency and interfaces—both\n    considered essential since day 1 but not yet designed.\n    A func was a function, and its signature specified return values\n    before arguments, separating them with &lt;-, which we now use as the channel\n    send/receive operator. For example, the WhiteSpace function takes the integer\n    c and returns a boolean.\n  function WhiteSpace(bool &lt;- c int)\n    This arrow was a stop-gap measure until a better syntax arose for declaring\n    multiple return values.\n    Methods were distinct from functions and had their own keyword.\n  method (this *Slist) Car(*Slist &lt;-) {\n    return this.list.car;\n}\n    And methods were pre-declared in the struct definition, although that changed soon.\n  type Slist struct {\n    ...\n    Car method(*Slist &lt;-);\n}\n    There were no strings, although they were in the spec.\n    To work around this, Rob had to build the input string as an uint8 array with\n    a clumsy construction. (Arrays were rudimentary and slices hadn\'t been designed\n    yet, let alone implemented, although there was the unimplemented concept of an\n    \"open array\".)\n  input[i] = \'(\'; i = i + 1;\ninput[i] = \'d\'; i = i + 1;\ninput[i] = \'e\'; i = i + 1;\ninput[i] = \'f\'; i = i + 1;\ninput[i] = \'n\'; i = i + 1;\ninput[i] = \' \'; i = i + 1;\n...\n    Both panic and print were built-in keywords, not pre-declared functions.\n  print \"parse error: expected \", c, \"\\n\";\npanic \"parse\";\n    And there are many other little differences; see if you can identify some others.\n    Less than two years after this program was written, Go was released as an\n    open source project. Looking back, it is striking how much the language has\n    grown and matured. (The last thing to change between this proto-Go and the Go\n    we know today was the elimination of semicolons.)\n    But even more striking is how much we have learned about writing Go code.\n    For instance, Rob called his method receivers this, but now we use shorter\n    context-specific names. There are hundreds of more significant examples\n    and to this day we\'re still discovering better ways to write Go code.\n    (Check out the glog package\'s clever trick for\n    handling verbosity levels.) \n    I wonder what we\'ll learn tomorrow.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/first-go-program','2013-07-18','2017-05-07 13:50:00.431176'),(508,'Go 1.2 is released','Andrew Gerrand','		Go 1.2 is released\n		1 December 2013\n		\n    We are pleased to announce the release of Go 1.2, the latest stable version of\n    the Go Programming Language.\n    Binary distributions may be downloaded from the\n    usual place or if you prefer to\n    compile from source you should use\n    the release or go1.2 tags.\n    This new release comes nearly seven months after the release of Go 1.1 in May,\n    a much shorter period than the 14 months between 1.1 and 1.0.\n    We anticipate a comparable interval between future major releases.\n    Go 1.2 includes a couple of minor\n    language changes, several improvements to the language implementation and\n    tools, some performance improvements, and many additions and\n    (backward-compatible) changes to the standard library.\n    Please read the release notes for all\n    the details, as some changes may affect the behavior of existing (buggy) programs.\n    What follows is the highlights of the release.\n    A new three-index slice syntax\n    adds the ability to specify capacity as well as length. This allows the\n    programmer to pass a slice value that can only access a limited portion of the\n    underlying array, a technique that previously required the use of the unsafe\n    package.\n    A major new feature of the tool chain is the facility to compute and display\n    test coverage results.\n    See the go test\n    and cover tool\n    documentation for details. Later this week we will publish an article that\n    discusses this new feature in detail.\n    Goroutines are now pre-emptively scheduled,\n    in that the scheduler is invoked occasionally upon entry to a function.\n    This can prevent busy goroutines from starving other goroutines on the same\n    thread.\n    An increase to the default goroutine stack size should improve the\n    performance of some programs. (The old size had a tendency to introduce\n    expensive stack-segment switching in performance-critical sections.)\n    On the other end, new restrictions on\n    stack sizes and\n    the number of operating system threads\n    should prevent misbehaving programs from consuming all the resources of a\n    machine. (These limits may be adjusted using new functions in the\n    runtime/debug package.)\n    Finally, among the many changes to the standard library,\n    significant changes include\n    the new encoding package,\n    indexed arguments in Printf format strings, and\n    some convenient additions to the template packages.\n    As part of the release, the Go Playground has been\n    updated to Go 1.2. This also affects services that use the Playground, such as\n    the Go Tour and this blog.\n    The update also adds the ability to use threads and the os, net, and\n    unsafe packages inside the sandbox, making it more like a real Go environment.\n    To everyone who helped make this release possible, from the many users who\n    submitted bug reports to the 116 (!) contributors who committed more than 1600\n    changes to the core: Your help is invaluable to the project. Thank you!\n    This blog post is the first of the\n    Go Advent Calendar,\n    a series of daily articles presented by the\n    Gopher Academy from December 1 to 25.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go12','2013-12-01','2017-05-07 13:50:00.550866'),(509,'The cover story','Rob Pike','		The cover story\n		2 December 2013\n		\n  Introduction\n    From the beginning of the project, Go was designed with tools in mind.\n    Those tools include some of the most iconic pieces of Go technology such as\n    the documentation presentation tool\n    godoc,\n    the code formatting tool\n    gofmt,\n    and the API rewriter\n    gofix.\n    Perhaps most important of all is the\n    go command,\n    the program that automatically installs, builds, and tests Go programs\n    using nothing more than the source code as the build specification.\n    The release of Go 1.2 introduces a new tool for test coverage that takes an\n    unusual approach to the way it generates coverage statistics, an approach\n    that builds on the technology laid down by godoc and friends.\n  Support for tools\n    First, some background: What does it mean for a\n    language to support good tooling?\n    It means that the language makes it easy to write good tools and that its ecosystem\n    supports the construction of tools of all flavors.\n    There are a number of properties of Go that make it suitable for tooling.\n    For starters, Go has a regular syntax that is easy to parse.\n    The grammar aims to be free of special cases that require complex machinery to analyze.\n    Where possible, Go uses lexical and syntactic constructs to make semantic properties\n    easy to understand.\n    Examples include the use of upper-case letters to define exported names\n    and the radically simplified scoping rules compared to other languages in the C tradition.\n    Finally, the standard library comes with production-quality packages to lex and parse Go source code.\n    They also include, more unusually, a production-quality package to pretty-print Go syntax trees.\n    These packages in combination form the core of the gofmt tool, but the pretty-printer is worth singling out.\n    Because it can take an arbitrary Go syntax tree and output standard-format, human-readable, correct\n    code, it creates the possibility to build tools that transform the parse tree and output modified but\n    correct and easy-to-read code.\n    One example is the gofix tool, which automates the\n    rewriting of code to use new language features or updated libraries.\n    Gofix let us make fundamental changes to the language and libraries in the\n    run-up to Go 1.0,\n    with the confidence that users could just run the tool to update their source to the newest version.\n    Inside Google, we have used gofix to make sweeping changes in a huge code repository that would be almost\n    unthinkable in the other languages we use.\n    There\'s no need any more to support multiple versions of some API; we can use gofix to update\n    the entire company in one operation.\n    It\'s not just these big tools that these packages enable, of course.\n    They also make it easy to write more modest programs such as IDE plugins, for instance.\n    All these items build on each other, making the Go environment\n    more productive by automating many tasks.\n  Test coverage\n    Test coverage is a term that describes how much of a package\'s code is exercised by running the package\'s tests.\n    If executing the test suite causes 80% of the package\'s source statements to be run, we say that the test coverage is 80%.\n    The program that provides test coverage in Go 1.2 is the latest to exploit the tooling support in the Go ecosystem.\n    The usual way to compute test coverage is to instrument the binary.\n    For instance, the GNU gcov program sets breakpoints at branches\n    executed by the binary.\n    As each branch executes, the breakpoint is cleared and the target statements of the branch are marked as \'covered\'.\n    This approach is successful and widely used. An early test coverage tool for Go even worked the same way.\n    But it has problems.\n    It is difficult to implement, as analysis of the execution of binaries is challenging.\n    It also requires a reliable way of tying the execution trace back to the source code, which can also be difficult,\n    as any user of a source-level debugger can attest.\n    Problems there include inaccurate debugging information and issues such as in-lined functions complicating\n    the analysis.\n    Most important, this approach is very non-portable.\n    It needs to be done afresh for every architecture, and to some extent for every\n    operating system since debugging support varies greatly from system to system.\n    It does work, though, and for instance if you are a user of gccgo, the gcov tool can give you test coverage\n    information.\n    However If you\'re a user of gc, the more commonly used Go compiler suite, until Go 1.2 you were out of luck.\n  Test coverage for Go\n    For the new test coverage tool for Go, we took a different approach that avoids dynamic debugging.\n    The idea is simple: Rewrite the package\'s source code before compilation to add instrumentation,\n    compile and run the modified source, and dump the statistics.\n    The rewriting is easy to arrange because the go command controls the flow\n    from source to test to execution.\n    Here\'s an example. Say we have a simple, one-file package like this:\n	\npackage size\nfunc Size(a int) string {\n    switch {\n    case a &lt; 0:\n        return \"negative\"\n    case a == 0:\n        return \"zero\"\n    case a &lt; 10:\n        return \"small\"\n    case a &lt; 100:\n        return \"big\"\n    case a &lt; 1000:\n        return \"huge\"\n    }\n    return \"enormous\"\n}\n    and this test:\n	\npackage size\nimport \"testing\"\ntype Test struct {\n    in  int\n    out string\n}\nvar tests = []Test{\n    {-1, \"negative\"},\n    {5, \"small\"},\n}\nfunc TestSize(t *testing.T) {\n    for i, test := range tests {\n        size := Size(test.in)\n        if size != test.out {\n            t.Errorf(\"#%d: Size(%d)=%s; want %s\", i, test.in, size, test.out)\n        }\n    }\n}\n    To get the test coverage for the package,\n    we run the test with coverage enabled by providing the -cover flag to go test:\n  % go test -cover\nPASS\ncoverage: 42.9% of statements\nok      size    0.026s\n%\n    Notice that the coverage is 42.9%, which isn\'t very good.\n    Before we ask how to raise that number, let\'s see how that was computed.\n    When test coverage is enabled, go test runs the \"cover\" tool, a separate program included\n    with the distribution, to rewrite the source code before compilation. Here\'s what the rewritten\n    Size function looks like:\n	\nfunc Size(a int) string {\n    GoCover.Count[0] = 1\n    switch {\n    case a &lt; 0:\n        GoCover.Count[2] = 1\n        return \"negative\"\n    case a == 0:\n        GoCover.Count[3] = 1\n        return \"zero\"\n    case a &lt; 10:\n        GoCover.Count[4] = 1\n        return \"small\"\n    case a &lt; 100:\n        GoCover.Count[5] = 1\n        return \"big\"\n    case a &lt; 1000:\n        GoCover.Count[6] = 1\n        return \"huge\"\n    }\n    GoCover.Count[1] = 1\n    return \"enormous\"\n}\n    Each executable section of the program is annotated with an assignment statement that,\n    when executed, records that that section ran.\n    The counter is tied to the original source position of the statements it counts\n    through a second read-only data structure that is also generated by the cover tool.\n    When the test run completes, the counters are collected and the percentage is computed\n    by seeing how many were set.\n    Although that annotating assignment might look expensive, it compiles to a single \"move\" instruction.\n    Its run-time overhead is therefore modest, adding only about 3% when running a typical (more realistic) test.\n    That makes it reasonable to include test coverage as part of the standard development pipeline.\n  Viewing the results\n    The test coverage for our example was poor.\n    To discover why, we ask go test to write a \"coverage profile\" for us, a file that holds\n    the collected statistics so we can study them in more detail.\n    That\'s easy to do: use the -coverprofile flag to specify a file for the output:\n  % go test -coverprofile=coverage.out \nPASS\ncoverage: 42.9% of statements\nok      size    0.030s\n%\n    (The -coverprofile flag automatically sets -cover to enable coverage analysis.)\n    The test runs just as before, but the results are saved in a file.\n    To study them, we run the test coverage tool ourselves, without go test.\n    As a start, we can ask for the coverage to be broken down by function,\n    although that\'s not going to illuminate much in this case since there\'s\n    only one function:\n  % go tool cover -func=coverage.out\nsize.go:    Size          42.9%\ntotal:      (statements)  42.9%\n%\n    A much more interesting way to see the data is to get an HTML presentation\n    of the source code decorated with coverage information.\n    This display is invoked by the -html flag:\n  $ go tool cover -html=coverage.out\n    When this command is run, a browser window pops up, showing the covered (green),\n    uncovered (red), and uninstrumented (grey) source.\n    Here\'s a screen dump:\n    With this presentation, it\'s obvious what\'s wrong: we neglected to test several\n    of the cases!\n    And we can see exactly which ones they are, which makes it easy to\n    improve our test coverage.\n  Heat maps\n    A big advantage of this source-level approach to test coverage is that it\'s\n    easy to instrument the code in different ways.\n    For instance, we can ask not only whether a statement has been executed,\n    but how many times.\n    The go test command accepts a -covermode flag to set the coverage mode\n    to one of three settings:\n    set:    did each statement run?\n    count:  how many times did each statement run?\n    atomic: like count, but counts precisely in parallel programs\n    The default is \'set\', which we\'ve already seen.\n    The atomic setting is needed only when accurate counts are required\n    when running parallel algorithms. It uses atomic operations from the\n    sync/atomic package,\n    which can be quite expensive.\n    For most purposes, though, the count mode works fine and, like\n    the default set mode, is very cheap.\n    Let\'s try counting statement execution for a standard package, the fmt formatting package.\n    We run the test and write out a coverage profile so we can present the information\n    nicely afterwards.\n  % go test -covermode=count -coverprofile=count.out fmt \nok      fmt    0.056s    coverage: 91.7% of statements\n%\n    That\'s a much better test coverage ratio than for our previous example.\n    (The coverage ratio is not affected by the coverage mode.)\n    We can display the function breakdown:\n  % go tool cover -func=count.out\nfmt/format.go: init              100.0%\nfmt/format.go: clearflags        100.0%\nfmt/format.go: init              100.0%\nfmt/format.go: computePadding     84.6%\nfmt/format.go: writePadding      100.0%\nfmt/format.go: pad               100.0%\n...\nfmt/scan.go:   advance            96.2%\nfmt/scan.go:   doScanf            96.8%\ntotal:         (statements)       91.7%\n    The big payoff happens in the HTML output:\n  % go tool cover -html=count.out\n    Here\'s what the pad function looks like in that presentation:\n    Notice how the intensity of the green changes. Brighter-green\n    statements have higher execution counts; less saturated greens\n    represent lower execution counts.\n    You can even hover the mouse over the statements to see the\n    actual counts pop up in a tool tip.\n    At the time of writing, the counts come out like this\n    (we\'ve moved the counts from the tool tips to beginning-of-line\n    markers to make them easier to show):\n  2933    if !f.widPresent || f.wid == 0 {\n2985        f.buf.Write(b)\n2985        return\n2985    }\n  56    padding, left, right := f.computePadding(len(b))\n  56    if left &gt; 0 {\n  37        f.writePadding(left, padding)\n  37    }\n  56    f.buf.Write(b)\n  56    if right &gt; 0 {\n  13        f.writePadding(right, padding)\n  13    }\n    That\'s a lot of information about the execution of the function,\n    information that might be useful in profiling.\n  Basic blocks\n    You might have noticed that the counts in the previous example\n    were not what you expected on the lines with closing braces.\n    That\'s because, as always, test coverage is an inexact science.\n    What\'s going on here is worth explaining, though. We\'d like the\n    coverage annotations to be demarcated by branches in the program,\n    the way they are when the binary is instrumented in the traditional\n    method.\n    It\'s hard to do that by rewriting the source, though, since\n    the branches don\'t appear explicitly in the source.\n    What the coverage annotation does is instrument blocks, which\n    are typically bounded by brace brackets.\n    Getting this right in general is very hard.\n    A consequence of the algorithm used is that the closing\n    brace looks like it belongs to the block it closes, while the\n    opening brace looks like it belongs outside the block.\n    A more interesting consequence is that in an expression like\n  f() &amp;&amp; g()\n    there is no attempt to separately instrument the calls to f and g, Regardless of\n    the facts it will always look like they both ran the same\n    number of times, the number of times f ran.\n    To be fair, even gcov has trouble here. That tool gets the\n    instrumentation right but the presentation is line-based and\n    can therefore miss some nuances.\n  The big picture\n    That\'s the story about test coverage in Go 1.2.\n    A new tool with an interesting implementation enables not only\n    test coverage statistics, but easy-to-interpret presentations\n    of them and even the possibility to extract profiling information.\n    Testing is an important part of software development and test\n    coverage a simple way to add discipline to your testing strategy.\n    Go forth, test, and cover.\n		\n			By Rob Pike\n		\n	','https://blog.golang.org/cover','2013-12-02','2017-05-07 13:50:00.578882'),(510,'Inside the Go Playground','Andrew Gerrand','		Inside the Go Playground\n		12 December 2013\n		\n  Introduction\n    In September 2010 we introduced the Go Playground,\n    a web service that compiles and executes arbitrary Go code and returns the\n    program output.\n    If you\'re a Go programmer then you have probably already used the playground\n    by using the Go Playground directly,\n    taking the Go Tour,\n    or running executable examples\n    from the Go documentation.\n    You may also have used it by clicking one of the \"Run\" buttons in a slide\n    deck on talks.golang.org or a post on this\n    very blog \n    (such as the recent article on Strings).\n    In this article we will take a look at how the playground is implemented\n    and integrated with these services.\n    The implementation involves a variant operating system environment and runtime\n    and our description here assumes you have some familiarity with systems\n    programming using Go.\n  Overview\n    The playground service has three parts:\n    A back end that runs on Google\'s servers. It receives RPC requests, compiles the user program using the gc tool chain, executes the user program, and returns the program output (or compilation errors) as the RPC response.\n    A front end that runs on Google App Engine. It receives HTTP requests from the client and makes corresponding RPC requests to the back end. It also does some caching.\n    A JavaScript client that implements the user interface and makes HTTP requests to the front end.\n  The back end\n    The back end program itself is trivial, so we won\'t discuss its implementation\n    here. The interesting part is how we safely execute arbitrary user code in a\n    secure environment while still providing core functionality such as time, the\n    network, and the file system.\n    To isolate user programs from Google\'s infrastructure, the back end runs \n    them under Native Client\n    (or \"NaCl\"), a technology developed by Google to permit the safe execution of\n    x86 programs inside web browsers. The back end uses a special version of the gc\n    tool chain that generates NaCl executables.\n    (This special tool chain was merged into Go 1.3.\n    To learn more, read the design document.)\n    NaCl limits the amount of CPU and RAM a program may consume, and it prevents \n    programs from accessing the network or file system.\n    This presents a problem, however.\n    Go\'s concurrency and networking support are among its key strengths,\n    and access to the file system is vital for many programs.\n    To demonstrate concurrency effectively we need time, and to demonstrate\n    networking and the file system we obviously need a network and a file system.\n    Although all these things are supported today, the first version of the\n    playground, launched in 2010, had none of them.\n    The current time was fixed at 10 November 2009, time.Sleep had no effect,\n    and most functions of the os and net packages were stubbed out to\n    return an EINVALID error.\n    A year ago we\n    implemented fake time\n    in the playground, so that programs that sleep would behave correctly.\n    A more recent update to the playground introduced a fake network stack and a\n    fake file system, making the playground\'s tool chain similar to a normal\n    Go tool chain.\n    These facilities are described in the following sections.\n  Faking time\n    Playground programs are limited in the amount of CPU time and memory they can\n    use, but they are also restricted in how much real time they can use.\n    This is because each running program consumes resources on the back end\n    and any stateful infrastructure between it and the client.\n    Limiting the run time of each playground program makes our service more\n    predictable and defends us against denial of service attacks.\n    But these restrictions become stifling when running code that uses time.\n    The Go Concurrency Patterns\n    talk demonstrates concurrency with examples that use timing functions like\n    time.Sleep and\n    time.After.\n    When run under early versions of the playground, these programs\' sleeps would\n    have no effect and their behavior would be strange (and sometimes wrong).\n    By using a clever trick we can make a Go program think that it is sleeping,\n    when really the sleeps take no time at all.\n    To explain the trick we first need to understand how the scheduler manages\n    sleeping goroutines.\n    When a goroutine calls time.Sleep (or similar) the scheduler adds a timer to\n    a heap of pending timers and puts the goroutine to sleep.  \n    Meanwhile, a special timer goroutine manages that heap.\n    When the timer goroutine starts it tells the scheduler to wake\n    it when the next pending timer is ready to fire and then sleeps.\n    When it wakes up it checks which timers have expired, wakes the appropriate\n    goroutines, and goes back to sleep.\n    The trick is to change the condition that wakes the timer goroutine.\n    Instead of waking it after a specific time period, we modify the scheduler to\n    wait for a deadlock; the state where all goroutines are blocked.\n    The playground version of the runtime maintains its own internal clock. When\n    the modified scheduler detects a deadlock it checks whether any timers are\n    pending. If so, it advances the internal clock to the trigger time of the\n    earliest timer and then wakes the timer goroutine. Execution continues and the\n    program believes that time has passed, when in fact the sleep was nearly\n    instantaneous.\n    These changes to the scheduler can be found in proc.c and time.goc.\n    Fake time fixes the issue of resource exhaustion on the back end, but what\n    about the program output? It would be odd to see a program that sleeps run to\n    completion correctly without taking any time.\n    The following program prints the current time each second and then exits after\n    three seconds. Try running it.\n	\n// +build OMIT\npackage main\nimport (\n	\"fmt\"\n	\"time\"\n)\nfunc main() {\n    stop := time.After(3 * time.Second)\n    tick := time.NewTicker(1 * time.Second)\n    defer tick.Stop()\n    for {\n        select {\n        case &lt;-tick.C:\n            fmt.Println(time.Now())\n        case &lt;-stop:\n            return\n        }\n    }\n}\n    How does this work? It is a collaboration between the back end, front end, and client.\n    We capture the timing of each write to standard output and standard error and\n    provide it to the client. Then the client can \"play back\" the writes with the\n    correct timing, so that the output appears just as if the program were running\n    locally.\n    The playground\'s runtime package provides a special\n    write function\n    that includes a small \"playback header\" before each write.\n    The playback header comprises a magic string, the current time, and the\n    length of the write data. A write with a playback header has this structure:\n  0 0 P B &lt;8-byte time&gt; &lt;4-byte data length&gt; &lt;data&gt;\n    The raw output of the program above looks like this:\n  \\x00\\x00PB\\x11\\x74\\xef\\xed\\xe6\\xb3\\x2a\\x00\\x00\\x00\\x00\\x1e2009-11-10 23:00:01 +0000 UTC\n\\x00\\x00PB\\x11\\x74\\xef\\xee\\x22\\x4d\\xf4\\x00\\x00\\x00\\x00\\x1e2009-11-10 23:00:02 +0000 UTC\n\\x00\\x00PB\\x11\\x74\\xef\\xee\\x5d\\xe8\\xbe\\x00\\x00\\x00\\x00\\x1e2009-11-10 23:00:03 +0000 UTC\n    The front end parses this output as a series of events\n    and returns a list of events to the client as a JSON object:\n  {\n    \"Errors\": \"\",\n    \"Events\": [\n        {\n            \"Delay\": 1000000000,\n            \"Message\": \"2009-11-10 23:00:01 +0000 UTC\\n\"\n        },\n        {\n            \"Delay\": 1000000000,\n            \"Message\": \"2009-11-10 23:00:02 +0000 UTC\\n\"\n        },\n        {\n            \"Delay\": 1000000000,\n            \"Message\": \"2009-11-10 23:00:03 +0000 UTC\\n\"\n        }\n    ]\n}\n    The JavaScript client (running in the user\'s web browser) then plays back the\n    events using the provided delay intervals.\n    To the user it appears that the program is running in real time.\n  Faking the file system\n    Programs built with the Go\'s NaCl tool chain cannot access the local machine\'s\n    file system. Instead, the syscall package\'s file-related functions\n    (Open, Read, Write, and so on) operate on an in-memory file system\n    that is implemented by the syscall package itself.  \n    Since package syscall is the interface between the Go code and the operating\n    system kernel, user programs see the file system exactly the same way as they\n    would a real one.\n    The following example program writes data to a file, and then copies\n    its contents to standard output. Try running it. (You can edit it, too!)\n	\n// +build OMIT\npackage main\nimport (\n	\"fmt\"\n	\"io/ioutil\"\n	\"log\"\n)\nfunc main() {\n    const filename = \"/tmp/file.txt\"\n    err := ioutil.WriteFile(filename, []byte(\"Hello, file system\\n\"), 0644)\n    if err != nil {\n        log.Fatal(err)\n    }\n    b, err := ioutil.ReadFile(filename)\n    if err != nil {\n        log.Fatal(err)\n    }\n    fmt.Printf(\"%s\", b)\n}\n    When a process starts, the file system is populated with some devices under\n    /dev and an empty /tmp directory. The program can manipulate the file\n    system as usual, but when the process exits any changes to the file system are\n    lost.\n    There is also a provision to load a zip file into the file system at init time\n    (see unzip_nacl.go).\n    So far we have only used the unzip facility to provide the data files required\n    to run the standard library tests, but we intend to provide playground programs\n    with a set of files that can be used in documentation examples, blog posts, and\n    the Go Tour.\n    The implementation can be found in the\n    fs_nacl.go and\n    fd_nacl.go files\n    (which, by virtue of their _nacl suffix, are built into package syscall only\n    when GOOS is set to nacl).\n    The file system itself is represented by the \n    fsys struct,\n    of which a global instance (named fs) is created during init time.\n    The various file-related functions then operate on fs instead of making the\n    actual system call.\n    For instance, here is the syscall.Open function:\n  func Open(path string, openmode int, perm uint32) (fd int, err error) {\n    fs.mu.Lock()\n    defer fs.mu.Unlock()\n    f, err := fs.open(path, openmode, perm&amp;0777|S_IFREG)\n    if err != nil {\n        return -1, err\n    }\n    return newFD(f), nil\n}\n    File descriptors are tracked by a global slice named\n    files.\n    Each file descriptor corresponds to a file\n    and each file provides a value that implements the fileImpl interface.\n    There are several implementations of the interface:\n    regular files and devices (such as /dev/random) are represented by fsysFile,\n    standard input, output, and error are instances of naclFile, which uses system calls to interact with the actual files (these are a playground program\'s only way to interact with the outside world),\n    network sockets have their own implementation, discussed in the next section.\n  Faking the network \n    Like the file system, the playground\'s network stack is an in-process fake\n    implemented by the syscall package. It permits playground projects to use\n    the loopback interface (127.0.0.1). Requests to other hosts will fail.\n    For an executable example, run the following program. It listens on a TCP port,\n    waits for an incoming connection, copies the data from that connection to\n    standard output, and exits. In another goroutine, it makes a connection to the\n    listening port, writes a string to the connection, and closes it.\n	\n// +build OMIT\npackage main\nimport (\n	\"io\"\n	\"log\"\n	\"net\"\n	\"os\"\n)\nfunc main() {\n    l, err := net.Listen(\"tcp\", \"127.0.0.1:4000\")\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer l.Close()\n    go dial()\n    c, err := l.Accept()\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer c.Close()\n    io.Copy(os.Stdout, c)\n}\nfunc dial() {\n    c, err := net.Dial(\"tcp\", \"127.0.0.1:4000\")\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer c.Close()\n    c.Write([]byte(\"Hello, network\\n\"))\n}\n    The interface to the network is more complex than the one for files, so the\n    implementation of the fake network is larger and more complex than the fake\n    file system. It must simulate read and write timeouts, different address types\n    and protocols, and so on.\n    The implementation can be found in net_nacl.go.\n    A good place to start reading is netFile, the network socket implementation of the fileImpl interface.\n  The front end\n    The playground front end is another simple program (shorter than 100 lines).\n    It receives HTTP requests from the client, makes RPC requests to the back end,\n    and does some caching.\n    The front end serves an HTTP handler at https://golang.org/compile.\n    The handler expects a POST request with a body field\n    (the Go program to run) and an optional version field\n    (for most clients this should be \"2\").\n    When the front end receives a compilation request it first checks\n    memcache\n    to see if it has cached the results of a previous compilation of that source.\n    If found, it returns the cached response.\n    The cache prevents popular programs such as those on the\n    Go home page from overloading the back ends.\n    If there is no cached response, the front end makes an RPC request to the back\n    end, stores the response in memcache, parses the playback events, and returns\n    a JSON object to the client as the HTTP response (as described above).\n  The client\n    The various sites that use the playground each share some common JavaScript\n    code for setting up the user interface (the code and output boxes, the run\n    button, and so on) and communicating with the playground front end.\n    This implementation is in the file\n    playground.js\n    in the go.tools repository, which can be imported from the\n    golang.org/x/tools/godoc/static package.\n    Some of it is clean and some is a bit crufty, as it is the result of\n    consolidating several divergent implementations of the client code.\n    The playground\n    function takes some HTML elements and turns them into an interactive\n    playground widget. You should use this function if you want to put the\n    playground on your own site (see \'Other clients\' below).\n    The Transport\n    interface (not formally defined, this being JavaScript)\n    abstracts the user interface from the means of talking to the web front end.\n    HTTPTransport\n    is an implementation of Transport that speaks the HTTP-based protocol\n    described earlier. \n    SocketTransport\n    is another implementation that speaks WebSocket (see \'Playing offline\' below).\n    To comply with the same-origin policy,\n    the various web servers (godoc, for instance) proxy requests to\n    /compile through to the playground service at https://golang.org/compile.\n    The common golang.org/x/tools/playground\n    package does this proxying.\n  Playing offline\n    Both the Go Tour and the\n    Present Tool can be\n    run offline. This is great for people with limited internet connectivity\n    or presenters at conferences who cannot (and should not) rely on a working\n    internet connection.\n    To run offline, the tools run their own version of the playground back end on\n    the local machine. The back end uses a regular Go tool chain with none of the\n    aforementioned modifications and uses a WebSocket to communicate with the\n    client.\n    The WebSocket back end implementation can be found in the\n    golang.org/x/too/playground/socket package.\n    The Inside Present talk discusses this code in detail.\n  Other clients\n    The playground service is used by more than just the official Go project\n    (Go by Example is one other instance)\n    and we are happy for you to use it on your own site. All we ask is that\n    you contact us first,\n    use a unique user agent in your requests (so we can identify you), and that\n    your service is of benefit to the Go community.\n  Conclusion\n    From godoc to the tour to this very blog, the playground has become an\n    essential part of our Go documentation story. With the recent additions\n    of the fake file system and network stack we are excited to expand\n    our learning materials to cover those areas.\n    But, ultimately, the playground is just the tip of the iceberg.\n    With Native Client support scheduled for Go 1.3,\n    we look forward to seeing what the community can do with it.\n    This article is part 12 of the\n    Go Advent Calendar,\n    a series of daily blog posts throughout December .\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/playground','2013-12-12','2017-05-07 13:50:00.596328'),(511,'Go on App Engine: tools, tests, and concurrency','Andrew Gerrand and Johan Euphrosine','		Go on App Engine: tools, tests, and concurrency\n		13 December 2013\n		\n  Background\n    When we launched Go for App Engine\n    in May 2011 the SDK was just a modified version of the Python SDK.\n    At the time, there was no canonical way to build or organize Go programs, so it\n    made sense to take the Python approach. Since then Go 1.0 was released,\n    including the go tool and a\n    convention for organizing Go programs.\n    In January 2013 we announced\n    better integration\n    between the Go App Engine SDK and the go tool, promoting the use of\n    conventional import paths in App Engine apps and making it possible to use \"go\n    get\" to fetch app dependencies.\n    With the recent release of App Engine 1.8.8 we are pleased to announce more\n    improvements to the developer experience for Go on App Engine.\n  The goapp tool\n    The Go App Engine SDK now includes the \"goapp\" tool, an App Engine-specific\n    version of the \"go\" tool. The new name permits users to keep both the regular\n    \"go\" tool and the \"goapp\" tool in their system PATH.\n    In addition to the existing \"go\" tool commands,\n    the \"goapp\" tool provides new commands for working with App Engine apps.\n    The \"goapp serve\"\n    command starts the local development server and the\n    \"goapp deploy\"\n    command uploads an app to App Engine.\n    The main advantages offered by the \"goapp serve\" and \"goapp deploy\" commands\n    are a simplified user interface and consistency with existing commands like\n    \"go get\" and \"go fmt\".\n    For example, to run a local instance of the app in the current directory, run:\n  $ goapp serve\n    To upload it to App Engine:\n  $ goapp deploy\n    You can also specify the Go import path to serve or deploy:\n  $ goapp serve github.com/user/myapp\n    You can even specify a YAML file to serve or deploy a specific\n    module:\n  $ goapp deploy mymodule.yaml\n    These commands can replace most uses of dev_appserver.py and appcfg.py,\n    although the Python tools are still available for their less common uses.\n  Local unit testing\n    The Go App Engine SDK now supports local unit testing, using Go\'s native\n    testing package\n    and the \"go test\" command\n    (provided as \"goapp test\" by the SDK).\n    Furthermore, you can now write tests that use App Engine services.\n    The aetest package\n    provides an appengine.Context value that delegates requests to a temporary\n    instance of the development server.\n    For more information about using \"goapp test\" and the aetest package, see the\n    Local Unit Testing for Go documentation.\n    Note that the aetest package is still in its early days;\n    we hope to add more features over time.\n  Better concurrency support\n    It is now possible to configure the number of concurrent requests served by\n    each of your app\'s dynamic instances by setting the\n    max_concurrent_requests option\n    (available to Automatic Scaling modules only).\n    Here\'s an example app.yaml file:\n  application: maxigopher\nversion: 1\nruntime: go\napi_version: go1\nautomatic_scaling:\n  max_concurrent_requests: 100\n    This configures each instance of the app to serve up to 100 requests\n    concurrently (up from the default of 10). You can configure Go instances to\n    serve up to a maximum of 500 concurrent requests.\n    This setting allows your instances to handle more simultaneous requests by\n    taking advantage of Go\'s efficient handling of concurrency, which should yield\n    better instance utilization and ultimately fewer billable instance hours.\n  Conclusion\n    With these changes Go on App Engine is more convenient and efficient than ever,\n    and we hope you enjoy the improvements. Please join the\n    google-appengine-go group\n    to raise questions or discuss these changes with the engineering team and the\n    rest of the community.\n		\n			By Andrew Gerrand and Johan Euphrosine\n		\n	','https://blog.golang.org/appengine-dec2013','2013-12-13','2017-05-07 13:50:00.625771'),(512,'Go talks at FOSDEM 2014','Andrew Gerrand','		Go talks at FOSDEM 2014\n		24 February 2014\n		\n  Introduction\n    At FOSDEM on the 2nd of February 2014 members of the Go\n    community presented a series of talks in the Go Devroom. The day was a huge\n    success, with 13 great talks presented to a consistently jam-packed room.\n    Video recordings of the talks are now available, and a selection of these\n    videos are presented below.\n    The complete series of talks is available\n    as a YouTube playlist.\n    (You can also get them directly at the\n    FOSDEM video archive.)\n  Scaling with Go: YouTube\'s Vitess\n    Google Engineer Sugu Sougoumarane described how he and his\n    team built Vitess in Go to help scale \n    YouTube.\n    Vitess is a set of servers and tools primarily developed in Go.\n    It helps scale MySQL databases for the web, and is currently used as a\n    fundamental component of YouTube\'s MySQL infrastructure.\n    The talk covers some history about how and why the team chose Go, and how it\n    paid off.\n    Sugu also talks abou tips and techniques used to scale Vitess using Go.\n    The slides for the talk are available here.\n  Camlistore\n    Camlistore is designed to be \"your personal storage\n    system for life, putting you in control, and designed to last.\" It\'s open\n    source, under nearly 4 years of active development, and extremely flexible.  In\n    this talk, Brad Fitzpatrick and Mathieu Lonjaret explain why they built it,\n    what it does, and talk about its design.\n  Write your own Go compiler\n    Elliot Stoneham explains the potential for Go as a portable language and\n    reviews the Go tools that make that such an exciting possibility.\n    He said: \"Based on my experiences writing an experimental Go to Haxe\n    translator, I\'ll talk about the practical issues of code generation and runtime\n    emulation required. I\'ll compare some of my design decisions with those of two\n    other Go compiler/translators that build on the go.tools library. My aim is to\n    encourage you to try one of these new \'mutant\' Go compilers. I hope some of you\n    will be inspired to contribute to one of them or even to write a new one of\n    your own.\"\n  More\n    There were many more great talks, so please check out the complete series \n    as a YouTube playlist.\n    In particular, the lightning talks were a lot of fun.\n    I would like to give my personal thanks to the excellent speakers, Mathieu\n    Lonjaret for managing the video gear, and to the FOSDEM staff for making all\n    this possible.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/fosdem14','2014-02-24','2017-05-07 13:50:00.653777'),(513,'Go Concurrency Patterns: Pipelines and cancellation','Sameer Ajmani','		Go Concurrency Patterns: Pipelines and cancellation\n		13 March 2014\n		\n  Introduction\n    Go\'s concurrency primitives make it easy to construct streaming data pipelines\n    that make efficient use of I/O and multiple CPUs.  This article presents\n    examples of such pipelines, highlights subtleties that arise when operations\n    fail, and introduces techniques for dealing with failures cleanly.\n  What is a pipeline?\n    There\'s no formal definition of a pipeline in Go; it\'s just one of many kinds of\n    concurrent programs.  Informally, a pipeline is a series of stages connected\n    by channels, where each stage is a group of goroutines running the same\n    function.  In each stage, the goroutines\n    receive values from upstream via inbound channels\n    perform some function on that data, usually producing new values\n    send values downstream via outbound channels\n    Each stage has any number of inbound and outbound channels, except the\n    first and last stages, which have only outbound or inbound channels,\n    respectively.  The first stage is sometimes called the source or\n    producer; the last stage, the sink or consumer.\n    We\'ll begin with a simple example pipeline to explain the ideas and techniques.\n    Later, we\'ll present a more realistic example.\n  Squaring numbers\n    Consider a pipeline with three stages.\n    The first stage, gen, is a function that converts a list of integers to a\n    channel that emits the integers in the list.  The gen function starts a\n    goroutine that sends the integers on the channel and closes the channel when all\n    the values have been sent:\n	\nfunc gen(nums ...int) &lt;-chan int {\n    out := make(chan int)\n    go func() {\n        for _, n := range nums {\n            out &lt;- n\n        }\n        close(out)\n    }()\n    return out\n}\n    The second stage, sq, receives integers from a channel and returns a\n    channel that emits the square of each received integer.  After the\n    inbound channel is closed and this stage has sent all the values\n    downstream, it closes the outbound channel:\n	\nfunc sq(in &lt;-chan int) &lt;-chan int {\n    out := make(chan int)\n    go func() {\n        for n := range in {\n            out &lt;- n * n\n        }\n        close(out)\n    }()\n    return out\n}\n    The main function sets up the pipeline and runs the final stage: it receives\n    values from the second stage and prints each one, until the channel is closed:\n	\nfunc main() {\n    // Set up the pipeline.\n    c := gen(2, 3)\n    out := sq(c)\n    // Consume the output.\n    fmt.Println(&lt;-out) // 4\n    fmt.Println(&lt;-out) // 9\n}\n    Since sq has the same type for its inbound and outbound channels, we\n    can compose it any number of times.  We can also rewrite main as a\n    range loop, like the other stages:\n	\nfunc main() {\n    // Set up the pipeline and consume the output.\n    for n := range sq(sq(gen(2, 3))) {\n        fmt.Println(n) // 16 then 81\n    }\n}\n  Fan-out, fan-in\n    Multiple functions can read from the same channel until that channel is closed;\n    this is called fan-out. This provides a way to distribute work amongst a group\n    of workers to parallelize CPU use and I/O.\n    A function can read from multiple inputs and proceed until all are closed by\n    multiplexing the input channels onto a single channel that\'s closed when all the\n    inputs are closed.  This is called fan-in.\n    We can change our pipeline to run two instances of sq, each reading from the\n    same input channel.  We introduce a new function, merge, to fan in the\n    results:\n	\nfunc main() {\n    in := gen(2, 3)\n    // Distribute the sq work across two goroutines that both read from in.\n    c1 := sq(in)\n    c2 := sq(in)\n    // Consume the merged output from c1 and c2.\n    for n := range merge(c1, c2) {\n        fmt.Println(n) // 4 then 9, or 9 then 4\n    }\n}\n    The merge function converts a list of channels to a single channel by starting\n    a goroutine for each inbound channel that copies the values to the sole outbound\n    channel.  Once all the output goroutines have been started, merge starts one\n    more goroutine to close the outbound channel after all sends on that channel are\n    done.\n    Sends on a closed channel panic, so it\'s important to ensure all sends\n    are done before calling close.  The\n    sync.WaitGroup type\n    provides a simple way to arrange this synchronization:\n	\nfunc merge(cs ...&lt;-chan int) &lt;-chan int {\n    var wg sync.WaitGroup\n    out := make(chan int)\n    // Start an output goroutine for each input channel in cs.  output\n    // copies values from c to out until c is closed, then calls wg.Done.\n    output := func(c &lt;-chan int) {\n        for n := range c {\n            out &lt;- n\n        }\n        wg.Done()\n    }\n    wg.Add(len(cs))\n    for _, c := range cs {\n        go output(c)\n    }\n    // Start a goroutine to close out once all the output goroutines are\n    // done.  This must start after the wg.Add call.\n    go func() {\n        wg.Wait()\n        close(out)\n    }()\n    return out\n}\n  Stopping short\n    There is a pattern to our pipeline functions:\n    stages close their outbound channels when all the send operations are done.\n    stages keep receiving values from inbound channels until those channels are closed.\n    This pattern allows each receiving stage to be written as a range loop and\n    ensures that all goroutines exit once all values have been successfully sent\n    downstream.\n    But in real pipelines, stages don\'t always receive all the inbound\n    values.  Sometimes this is by design: the receiver may only need a\n    subset of values to make progress.  More often, a stage exits early\n    because an inbound value represents an error in an earlier stage. In\n    either case the receiver should not have to wait for the remaining\n    values to arrive, and we want earlier stages to stop producing values\n    that later stages don\'t need.\n    In our example pipeline, if a stage fails to consume all the inbound values, the\n    goroutines attempting to send those values will block indefinitely:\n	\n    // Consume the first value from output.\n    out := merge(c1, c2)\n    fmt.Println(&lt;-out) // 4 or 9\n    return\n    // Since we didn\'t receive the second value from out,\n    // one of the output goroutines is hung attempting to send it.\n}\n    This is a resource leak: goroutines consume memory and runtime resources, and\n    heap references in goroutine stacks keep data from being garbage collected.\n    Goroutines are not garbage collected; they must exit on their own.\n    We need to arrange for the upstream stages of our pipeline to exit even when the\n    downstream stages fail to receive all the inbound values.  One way to do this is\n    to change the outbound channels to have a buffer.  A buffer can hold a fixed\n    number of values; send operations complete immediately if there\'s room in the\n    buffer:\n  c := make(chan int, 2) // buffer size 2\nc &lt;- 1  // succeeds immediately\nc &lt;- 2  // succeeds immediately\nc &lt;- 3  // blocks until another goroutine does &lt;-c and receives 1\n    When the number of values to be sent is known at channel creation time, a buffer\n    can simplify the code.  For example, we can rewrite gen to copy the list of\n    integers into a buffered channel and avoid creating a new goroutine:\n	\nfunc gen(nums ...int) &lt;-chan int {\n    out := make(chan int, len(nums))\n    for _, n := range nums {\n        out &lt;- n\n    }\n    close(out)\n    return out\n}\n    Returning to the blocked goroutines in our pipeline, we might consider adding a\n    buffer to the outbound channel returned by merge:\n	\nfunc merge(cs ...&lt;-chan int) &lt;-chan int {\n    var wg sync.WaitGroup\n    out := make(chan int, 1) // enough space for the unread inputs\n    // ... the rest is unchanged ...\n    While this fixes the blocked goroutine in this program, this is bad code.  The\n    choice of buffer size of 1 here depends on knowing the number of values merge\n    will receive and the number of values downstream stages will consume.  This is\n    fragile: if we pass an additional value to gen, or if the downstream stage\n    reads any fewer values, we will again have blocked goroutines.\n    Instead, we need to provide a way for downstream stages to indicate to the\n    senders that they will stop accepting input.\n  Explicit cancellation\n    When main decides to exit without receiving all the values from\n    out, it must tell the goroutines in the upstream stages to abandon\n    the values they\'re trying it send.  It does so by sending values on a\n    channel called done.  It sends two values since there are\n    potentially two blocked senders:\n	\nfunc main() {\n    in := gen(2, 3)\n    // Distribute the sq work across two goroutines that both read from in.\n    c1 := sq(in)\n    c2 := sq(in)\n    // Consume the first value from output.\n    done := make(chan struct{}, 2)\n    out := merge(done, c1, c2)\n    fmt.Println(&lt;-out) // 4 or 9\n    // Tell the remaining senders we\'re leaving.\n    done &lt;- struct{}{}\n    done &lt;- struct{}{}\n}\n    The sending goroutines replace their send operation with a select statement\n    that proceeds either when the send on out happens or when they receive a value\n    from done.  The value type of done is the empty struct because the value\n    doesn\'t matter: it is the receive event that indicates the send on out should\n    be abandoned.  The output goroutines continue looping on their inbound\n    channel, c, so the upstream stages are not blocked. (We\'ll discuss in a moment\n    how to allow this loop to return early.)\n	\nfunc merge(done &lt;-chan struct{}, cs ...&lt;-chan int) &lt;-chan int {\n    var wg sync.WaitGroup\n    out := make(chan int)\n    // Start an output goroutine for each input channel in cs.  output\n    // copies values from c to out until c is closed or it receives a value\n    // from done, then output calls wg.Done.\n    output := func(c &lt;-chan int) {\n        for n := range c {\n            select {\n            case out &lt;- n:\n            case &lt;-done:\n            }\n        }\n        wg.Done()\n    }\n    // ... the rest is unchanged ...\n    This approach has a problem: each downstream receiver needs to know the number\n    of potentially blocked upstream senders and arrange to signal those senders on\n    early return.  Keeping track of these counts is tedious and error-prone.\n    We need a way to tell an unknown and unbounded number of goroutines to\n    stop sending their values downstream.  In Go, we can do this by\n    closing a channel, because\n    a receive operation on a closed channel can always proceed immediately, yielding the element type\'s zero value.\n    This means that main can unblock all the senders simply by closing\n    the done channel.  This close is effectively a broadcast signal to\n    the senders.  We extend each of our pipeline functions to accept\n    done as a parameter and arrange for the close to happen via a\n    defer statement, so that all return paths from main will signal\n    the pipeline stages to exit.\n	\nfunc main() {\n    // Set up a done channel that\'s shared by the whole pipeline,\n    // and close that channel when this pipeline exits, as a signal\n    // for all the goroutines we started to exit.\n    done := make(chan struct{})\n    defer close(done)\n    in := gen(done, 2, 3)\n    // Distribute the sq work across two goroutines that both read from in.\n    c1 := sq(done, in)\n    c2 := sq(done, in)\n    // Consume the first value from output.\n    out := merge(done, c1, c2)\n    fmt.Println(&lt;-out) // 4 or 9\n    // done will be closed by the deferred call.\n}\n    Each of our pipeline stages is now free to return as soon as done is closed.\n    The output routine in merge can return without draining its inbound channel,\n    since it knows the upstream sender, sq, will stop attempting to send when\n    done is closed.  output ensures wg.Done is called on all return paths via\n    a defer statement:\n	\nfunc merge(done &lt;-chan struct{}, cs ...&lt;-chan int) &lt;-chan int {\n    var wg sync.WaitGroup\n    out := make(chan int)\n    // Start an output goroutine for each input channel in cs.  output\n    // copies values from c to out until c or done is closed, then calls\n    // wg.Done.\n    output := func(c &lt;-chan int) {\n        defer wg.Done()\n        for n := range c {\n            select {\n            case out &lt;- n:\n            case &lt;-done:\n                return\n            }\n        }\n    }\n    // ... the rest is unchanged ...\n    Similarly, sq can return as soon as done is closed.  sq ensures its out\n    channel is closed on all return paths via a defer statement:\n	\nfunc sq(done &lt;-chan struct{}, in &lt;-chan int) &lt;-chan int {\n    out := make(chan int)\n    go func() {\n        defer close(out)\n        for n := range in {\n            select {\n            case out &lt;- n * n:\n            case &lt;-done:\n                return\n            }\n        }\n    }()\n    return out\n}\n    Here are the guidelines for pipeline construction:\n    stages close their outbound channels when all the send operations are done.\n    stages keep receiving values from inbound channels until those channels are closed or the senders are unblocked.\n    Pipelines unblock senders either by ensuring there\'s enough buffer for all the\n    values that are sent or by explicitly signalling senders when the receiver may\n    abandon the channel.\n  Digesting a tree\n    Let\'s consider a more realistic pipeline.\n    MD5 is a message-digest algorithm that\'s useful as a file checksum.  The command\n    line utility md5sum prints digest values for a list of files.\n  % md5sum *.go\nd47c2bbc28298ca9befdfbc5d3aa4e65  bounded.go\nee869afd31f83cbb2d10ee81b2b831dc  parallel.go\nb88175e65fdcbc01ac08aaf1fd9b5e96  serial.go\n    Our example program is like md5sum but instead takes a single directory as an\n    argument and prints the digest values for each regular file under that\n    directory, sorted by path name.\n  % go run serial.go .\nd47c2bbc28298ca9befdfbc5d3aa4e65  bounded.go\nee869afd31f83cbb2d10ee81b2b831dc  parallel.go\nb88175e65fdcbc01ac08aaf1fd9b5e96  serial.go\n    The main function of our program invokes a helper function MD5All, which\n    returns a map from path name to digest value, then sorts and prints the results:\n	\nfunc main() {\n    // Calculate the MD5 sum of all files under the specified directory,\n    // then print the results sorted by path name.\n    m, err := MD5All(os.Args[1])\n    if err != nil {\n        fmt.Println(err)\n        return\n    }\n    var paths []string\n    for path := range m {\n        paths = append(paths, path)\n    }\n    sort.Strings(paths)\n    for _, path := range paths {\n        fmt.Printf(\"%x  %s\\n\", m[path], path)\n    }\n}\n    The MD5All function is the focus of our discussion.  In\n    serial.go, the implementation uses no concurrency and\n    simply reads and sums each file as it walks the tree.\n	\n// MD5All reads all the files in the file tree rooted at root and returns a map\n// from file path to the MD5 sum of the file\'s contents.  If the directory walk\n// fails or any read operation fails, MD5All returns an error.\nfunc MD5All(root string) (map[string][md5.Size]byte, error) {\n    m := make(map[string][md5.Size]byte)\n    err := filepath.Walk(root, func(path string, info os.FileInfo, err error) error {\n        if err != nil {\n            return err\n        }\n        if !info.Mode().IsRegular() {\n            return nil\n        }\n        data, err := ioutil.ReadFile(path)\n        if err != nil {\n            return err\n        }\n        m[path] = md5.Sum(data)\n        return nil\n    })\n    if err != nil {\n        return nil, err\n    }\n    return m, nil\n}\n  Parallel digestion\n    In parallel.go, we split MD5All into a two-stage\n    pipeline.  The first stage, sumFiles, walks the tree, digests each file in\n    a new goroutine, and sends the results on a channel with value type result:\n	\ntype result struct {\n    path string\n    sum  [md5.Size]byte\n    err  error\n}\n    sumFiles returns two channels: one for the results and another for the error\n    returned by filepath.Walk.  The walk function starts a new goroutine to\n    process each regular file, then checks done.  If done is closed, the walk\n    stops immediately:\n	\nfunc sumFiles(done &lt;-chan struct{}, root string) (&lt;-chan result, &lt;-chan error) {\n    // For each regular file, start a goroutine that sums the file and sends\n    // the result on c.  Send the result of the walk on errc.\n    c := make(chan result)\n    errc := make(chan error, 1)\n    go func() {\n        var wg sync.WaitGroup\n        err := filepath.Walk(root, func(path string, info os.FileInfo, err error) error {\n            if err != nil {\n                return err\n            }\n            if !info.Mode().IsRegular() {\n                return nil\n            }\n            wg.Add(1)\n            go func() {\n                data, err := ioutil.ReadFile(path)\n                select {\n                case c &lt;- result{path, md5.Sum(data), err}:\n                case &lt;-done:\n                }\n                wg.Done()\n            }()\n            // Abort the walk if done is closed.\n            select {\n            case &lt;-done:\n                return errors.New(\"walk canceled\")\n            default:\n                return nil\n            }\n        })\n        // Walk has returned, so all calls to wg.Add are done.  Start a\n        // goroutine to close c once all the sends are done.\n        go func() {\n            wg.Wait()\n            close(c)\n        }()\n        // No select needed here, since errc is buffered.\n        errc &lt;- err\n    }()\n    return c, errc\n}\n    MD5All receives the digest values from c.  MD5All returns early on error,\n    closing done via a defer:\n	\nfunc MD5All(root string) (map[string][md5.Size]byte, error) {\n    // MD5All closes the done channel when it returns; it may do so before\n    // receiving all the values from c and errc.\n    done := make(chan struct{})\n    defer close(done)\n    c, errc := sumFiles(done, root)\n    m := make(map[string][md5.Size]byte)\n    for r := range c {\n        if r.err != nil {\n            return nil, r.err\n        }\n        m[r.path] = r.sum\n    }\n    if err := &lt;-errc; err != nil {\n        return nil, err\n    }\n    return m, nil\n}\n  Bounded parallelism\n    The MD5All implementation in parallel.go\n    starts a new goroutine for each file. In a directory with many large\n    files, this may allocate more memory than is available on the machine.\n    We can limit these allocations by bounding the number of files read in\n    parallel.  In bounded.go, we do this by\n    creating a fixed number of goroutines for reading files.  Our pipeline\n    now has three stages: walk the tree, read and digest the files, and\n    collect the digests.\n    The first stage, walkFiles, emits the paths of regular files in the tree:\n	\nfunc walkFiles(done &lt;-chan struct{}, root string) (&lt;-chan string, &lt;-chan error) {\n    paths := make(chan string)\n    errc := make(chan error, 1)\n    go func() {\n        // Close the paths channel after Walk returns.\n        defer close(paths)\n        // No select needed for this send, since errc is buffered.\n        errc &lt;- filepath.Walk(root, func(path string, info os.FileInfo, err error) error {\n            if err != nil {\n                return err\n            }\n            if !info.Mode().IsRegular() {\n                return nil\n            }\n            select {\n            case paths &lt;- path:\n            case &lt;-done:\n                return errors.New(\"walk canceled\")\n            }\n            return nil\n        })\n    }()\n    return paths, errc\n}\n    The middle stage starts a fixed number of digester goroutines that receive\n    file names from paths and send results on channel c:\n	\nfunc digester(done &lt;-chan struct{}, paths &lt;-chan string, c chan&lt;- result) {\n    for path := range paths {\n        data, err := ioutil.ReadFile(path)\n        select {\n        case c &lt;- result{path, md5.Sum(data), err}:\n        case &lt;-done:\n            return\n        }\n    }\n}\n    Unlike our previous examples, digester does not close its output channel, as\n    multiple goroutines are sending on a shared channel.  Instead, code in MD5All\n    arranges for the channel to be closed when all the digesters are done:\n	\n    // Start a fixed number of goroutines to read and digest files.\n    c := make(chan result)\n    var wg sync.WaitGroup\n    const numDigesters = 20\n    wg.Add(numDigesters)\n    for i := 0; i &lt; numDigesters; i++ {\n        go func() {\n            digester(done, paths, c)\n            wg.Done()\n        }()\n    }\n    go func() {\n        wg.Wait()\n        close(c)\n    }()\n    We could instead have each digester create and return its own output\n    channel, but then we would need additional goroutines to fan-in the\n    results.\n    The final stage receives all the results from c then checks the\n    error from errc.  This check cannot happen any earlier, since before\n    this point, walkFiles may block sending values downstream:\n	\n    m := make(map[string][md5.Size]byte)\n    for r := range c {\n        if r.err != nil {\n            return nil, r.err\n        }\n        m[r.path] = r.sum\n    }\n    // Check whether the Walk failed.\n    if err := &lt;-errc; err != nil {\n        return nil, err\n    }\n    return m, nil\n}\n  Conclusion\n    This article has presented techniques for constructing streaming data pipelines\n    in Go.  Dealing with failures in such pipelines is tricky, since each stage in\n    the pipeline may block attempting to send values downstream, and the downstream\n    stages may no longer care about the incoming data.  We showed how closing a\n    channel can broadcast a \"done\" signal to all the goroutines started by a\n    pipeline and defined guidelines for constructing pipelines correctly.\n    Further reading:\n    Go Concurrency Patterns (video) presents the basics of Go\'s concurrency primitives and several ways to apply them.\n    Advanced Go Concurrency Patterns (video) covers more complex uses of Go\'s primitives, especially select.\n    Douglas McIlroy\'s paper Squinting at Power Series shows how Go-like concurrency provides elegant support for complex calculations.\n		\n			By Sameer Ajmani\n		\n	','https://blog.golang.org/pipelines','2014-03-13','2017-05-07 13:50:00.679934'),(514,'Introducing HTTP Tracing','Jaana Burcu Dogan','		Introducing HTTP Tracing\n		4 October 2016\n		\n  Introduction\n    In Go 1.7 we introduced HTTP tracing, a facility to gather fine-grained\n    information throughout the lifecycle of an HTTP client request.\n    Support for HTTP tracing is provided by the net/http/httptrace\n    package. The collected information can be used for debugging latency issues,\n    service monitoring, writing adaptive systems, and more.\n  HTTP events\n    The httptrace package provides a number of hooks to gather information\n    during an HTTP round trip about a variety of events. These events include:\n    Connection creation\n    Connection reuse\n    DNS lookups\n    Writing the request to the wire\n    Reading the response\n  Tracing events\n    You can enable HTTP tracing by putting an\n    *httptrace.ClientTrace\n    containing hook functions into a request\'s context.Context.\n    Various http.RoundTripper\n    implementations report the internal events by\n    looking for context\'s *httptrace.ClientTrace and calling the relevant hook functions.\n    The tracing is scoped to the request\'s context and users should\n    put a *httptrace.ClientTrace to the request context before they start a request.\n	\n    req, _ := http.NewRequest(\"GET\", \"http://example.com\", nil)\n    trace := &amp;httptrace.ClientTrace{\n        DNSDone: func(dnsInfo httptrace.DNSDoneInfo) {\n            fmt.Printf(\"DNS Info: %+v\\n\", dnsInfo)\n        },\n        GotConn: func(connInfo httptrace.GotConnInfo) {\n            fmt.Printf(\"Got Conn: %+v\\n\", connInfo)\n        },\n    }\n    req = req.WithContext(httptrace.WithClientTrace(req.Context(), trace))\n    if _, err := http.DefaultTransport.RoundTrip(req); err != nil {\n        log.Fatal(err)\n    }\n    During a round trip, http.DefaultTransport will invoke each hook\n    as an event happens. The program above will print the DNS\n    information as soon as the DNS lookup is complete. It will similarly print\n    connection information when a connection is established to the request\'s host.\n  Tracing with http.Client\n    The tracing mechanism is designed to trace the events in the lifecycle\n    of a single http.Transport.RoundTrip. However, a client may\n    make multiple round trips to complete an HTTP request. For example, in the case\n    of a URL redirection, the registered hooks will be called as many times as the\n    client follows HTTP redirects, making multiple requests.\n    Users are responsible for recognizing such events at the http.Client level.\n    The program below identifies the current request by using an\n    http.RoundTripper wrapper.\n	\npackage main\nimport (\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n    \"net/http/httptrace\"\n)\n// transport is an http.RoundTripper that keeps track of the in-flight\n// request and implements hooks to report HTTP tracing events.\ntype transport struct {\n    current *http.Request\n}\n// RoundTrip wraps http.DefaultTransport.RoundTrip to keep track\n// of the current request.\nfunc (t *transport) RoundTrip(req *http.Request) (*http.Response, error) {\n    t.current = req\n    return http.DefaultTransport.RoundTrip(req)\n}\n// GotConn prints whether the connection has been used previously\n// for the current request.\nfunc (t *transport) GotConn(info httptrace.GotConnInfo) {\n    fmt.Printf(\"Connection reused for %v? %v\\n\", t.current.URL, info.Reused)\n}\nfunc main() {\n    t := &amp;transport{}\n    req, _ := http.NewRequest(\"GET\", \"https://google.com\", nil)\n    trace := &amp;httptrace.ClientTrace{\n        GotConn: t.GotConn,\n    }\n    req = req.WithContext(httptrace.WithClientTrace(req.Context(), trace))\n    client := &amp;http.Client{Transport: t}\n    if _, err := client.Do(req); err != nil {\n        log.Fatal(err)\n    }\n}\n    The program will follow the redirect of google.com to www.google.com and will output:\n  Connection reused for https://google.com? false\nConnection reused for https://www.google.com/? false\n    The Transport in the net/http package supports tracing of both HTTP/1\n    and HTTP/2 requests.\n    If you are an author of a custom http.RoundTripper implementation,\n    you can support tracing by checking the request context for an\n    *httptest.ClientTrace and invoking the relevant hooks as the events occur.\n  Conclusion\n    HTTP tracing is a valuable addition to Go for those who are interested\n    in debugging HTTP request latency and writing tools for network debugging\n    for outbound traffic.\n    By enabling this new facility, we hope to see HTTP debugging, benchmarking\n    and visualization tools from the community — such as\n    httpstat.\n		\n			By Jaana Burcu Dogan\n		\n	','https://blog.golang.org/http-tracing','2016-10-04','2017-05-07 13:50:00.733252'),(515,'Go 1.1 is released','Andrew Gerrand','		Go 1.1 is released\n		13 May 2013\n		\n    It is our great pleasure to announce the release of Go 1.1.\n    In March last year we released Go 1.0, and since then we have released three minor \"point releases\". The point releases were made to fix only critical issues, so the Go 1.0.3 you use today is still, in essence, the Go 1.0 we released in March 2012.\n    Go 1.1 includes many improvements over 1.0.\n    The most significant improvements are performance-related. We have made optimizations in the compiler and linker, garbage collector, goroutine scheduler, map implementation, and parts of the standard library. It is likely that your Go code will run noticeably faster when built with Go 1.1.\n    There are some minor changes to the language itself, two of which are worth singling out here: the changes to return requirements will lead to more succinct and correct programs, and the introduction of method values provides an expressive way to bind a method to its receiver as a function value.\n    Concurrent programming is safer in Go 1.1 with the addition of a race detector for finding memory synchronization errors in your programs. We will discuss the race detector more in an upcoming article, but for now the manual is a great place to get started.\n    The tools and standard library have been improved and expanded. You can read the full story in the release notes.\n    As per our compatibility guidelines, Go 1.1 remains compatible with Go 1.0 and we recommend all Go users upgrade to the new release.\n    All this would not have been possible without the help of our contributors from the open source community. Since Go 1.0, the core received more than 2600 commits from 161 people outside Google. Thank you everyone for your time and effort. In particular, we would like to thank Shenghou Ma, Rémy Oudompheng, Dave Cheney, Mikio Hara, Alex Brainman, Jan Ziak, and Daniel Morsing for their outstanding contributions.\n    To grab the new release, follow the usual installation instructions. Happy hacking!\n    Thanks to Renée French for the gopher!\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go-11-is-released','2013-05-13','2017-05-07 13:50:01.126350'),(516,'Advanced Go Concurrency Patterns','Andrew Gerrand','		Advanced Go Concurrency Patterns\n		23 May 2013\n		\n    At Google I/O a year ago Rob Pike presented Go Concurrency Patterns, an introduction to Go\'s concurrency model. Last week, at I/O 2013, Go team member Sameer Ajmani continued the story with Advanced Go Concurrency Patterns, an in-depth look at a real concurrent programming problem. The talk shows how to detect and avoid deadlocks and race conditions, and demonstrates the implementation of deadlines, cancellation, and more. For those who want to take their Go programming to the next level, this is a must-see.\n    The slides are available here (use the left and right arrows to navigate).\n    The slides were produced with the present tool, and the runnable code snippets are powered by the Go Playground. The source code for this talk is in the go.talks sub-repository.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/advanced-go-concurrency-patterns','2013-05-23','2017-05-07 13:50:01.216620'),(517,'Arrays, slices (and strings): The mechanics of \'append\'','Rob Pike','		Arrays, slices (and strings): The mechanics of \'append\'\n		26 September 2013\n		\n  Introduction\n    One of the most common features of procedural programming languages is\n    the concept of an array.\n    Arrays seem like simple things but there are many questions that must be\n    answered when adding them to a language, such as:\n    fixed-size or variable-size?\n    is the size part of the type?\n    what do multidimensional arrays look like?\n    does the empty array have meaning?\n    The answers to these questions affect whether arrays are just\n    a feature of the language or a core part of its design.\n    In the early development of Go, it took about a year to decide the answers\n    to these questions before the design felt right.\n    The key step was the introduction of slices, which built on fixed-size\n    arrays to give a flexible, extensible data structure.\n    To this day, however, programmers new to Go often stumble over the way slices\n    work, perhaps because experience from other languages has colored their thinking.\n    In this post we\'ll attempt to clear up the confusion.\n    We\'ll do so by building up the pieces to explain how the append built-in function\n    works, and why it works the way it does.\n  Arrays\n    Arrays are an important building block in Go, but like the foundation of a building\n    they are often hidden below more visible components.\n    We must talk about them briefly before we move on to the more interesting,\n    powerful, and prominent idea of slices.\n    Arrays are not often seen in Go programs because\n    the size of an array is part of its type, which limits its expressive power.\n    The declaration\n	\nvar buffer [256]byte\n    declares the variable buffer, which holds 256 bytes.\n    The type of buffer includes its size, [256]byte.\n    An array with 512 bytes would be of the distinct type [512]byte.\n    The data associated with an array is just that: an array of elements.\n    Schematically, our buffer looks like this in memory,\n  buffer: byte byte byte ... 256 times ... byte byte byte\n    That is, the variable holds 256 bytes of data and nothing else. We can\n    access its elements with the familiar indexing syntax, buffer[0], buffer[1],\n    and so on through buffer[255]. (The index range 0 through 255 covers\n    256 elements.) Attempting to index buffer with a value outside this\n    range will crash the program.\n    There is a built-in function called len that returns the number of elements\n    of an array or slice and also of a few other data types.\n    For arrays, it\'s obvious what len returns.\n    In our example, len(buffer) returns the fixed value 256.\n    Arrays have their place—they are a good representation of a transformation\n    matrix for instance—but their most common purpose in Go is to hold storage\n    for a slice.\n  Slices: The slice header\n    Slices are where the action is, but to use them well one must understand\n    exactly what they are and what they do.\n    A slice is a data structure describing a contiguous section of an array\n    stored separately from the slice variable itself.\n    A slice is not an array.\n    A slice describes a piece of an array.\n    Given our buffer array variable from the previous section, we could create\n    a slice that describes elements 100 through 150 (to be precise, 100 through 149,\n    inclusive) by slicing the array:\n	\nvar slice []byte = buffer[100:150]\n    In that snippet we used the full variable declaration to be explicit.\n    The variable slice has type []byte, pronounced \"slice of bytes\",\n    and is initialized from the array, called\n    buffer, by slicing elements 100 (inclusive) through 150 (exclusive).\n    The more idiomatic syntax would drop the type, which is set by the initializing expression:\n  var slice = buffer[100:150]\n    Inside a function we could use the short declaration form,\n  slice := buffer[100:150]\n    What exactly is this slice variable?\n    It\'s not quite the full story, but for now think of a\n    slice as a little data structure with two elements: a length and a pointer to an element\n    of an array.\n    You can think of it as being built like this behind the scenes:\n  type sliceHeader struct {\n    Length        int\n    ZerothElement *byte\n}\nslice := sliceHeader{\n    Length:        50,\n    ZerothElement: &amp;buffer[100],\n}\n    Of course, this is just an illustration.\n    Despite what this snippet says that sliceHeader struct is not visible\n    to the programmer, and the type\n    of the element pointer depends on the type of the elements,\n    but this gives the general idea of the mechanics.\n    So far we\'ve used a slice operation on an array, but we can also slice a slice, like this:\n  slice2 := slice[5:10]\n    Just as before, this operation creates a new slice, in this case with elements\n    5 through 9 (inclusive) of the original slice, which means elements\n    105 through 109 of the original array.\n    The underlying sliceHeader struct for the slice2 variable looks like\n    this:\n  slice2 := sliceHeader{\n    Length:        5,\n    ZerothElement: &amp;buffer[105],\n}\n    Notice that this header still points to the same underlying array, stored in\n    the buffer variable.\n    We can also reslice, which is to say slice a slice and store the result back in\n    the original slice structure. After\n  slice = slice[5:10]\n    the sliceHeader structure for the slice variable looks just like it did for the slice2\n    variable.\n    You\'ll see reslicing used often, for example to truncate a slice. This statement drops\n    the first and last elements of our slice:\n  slice = slice[1:len(slice)-1]\n    [Exercise: Write out what the sliceHeader struct looks like after this assignment.]\n    You\'ll often hear experienced Go programmers talk about the \"slice header\"\n    because that really is what\'s stored in a slice variable.\n    For instance, when you call a function that takes a slice as an argument, such as\n    bytes.IndexRune, that header is\n    what gets passed to the function.\n    In this call,\n  slashPos := bytes.IndexRune(slice, \'/\')\n    the slice argument that is passed to the IndexRune function is, in fact,\n    a \"slice header\".\n    There\'s one more data item in the slice header, which we talk about below,\n    but first let\'s see what the existence of the slice header means when you\n    program with slices.\n  Passing slices to functions\n    It\'s important to understand that even though a slice contains a pointer,\n    it is itself a value.\n    Under the covers, it is a struct value holding a pointer and a length.\n    It is not a pointer to a struct.\n    This matters.\n    When we called IndexRune in the previous example,\n    it was passed a copy of the slice header.\n    That behavior has important ramifications.\n    Consider this simple function:\n	\nfunc AddOneToEachElement(slice []byte) {\n    for i := range slice {\n        slice[i]++\n    }\n}\n    It does just what its name implies, iterating over the indices of a slice\n    (using a for range loop), incrementing its elements.\n    Try it:\n	\n// +build OMIT\n// Copyright 2013 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\npackage main\nimport (\n	\"fmt\"\n)\nvar buffer [256]byte\nvar slice []byte = buffer[100:150]\nfunc AddOneToEachElement(slice []byte) {\n	for i := range slice {\n		slice[i]++\n	}\n}\nfunc main() {\n    slice := buffer[10:20]\n    for i := 0; i &lt; len(slice); i++ {\n        slice[i] = byte(i)\n    }\n    fmt.Println(\"before\", slice)\n    AddOneToEachElement(slice)\n    fmt.Println(\"after\", slice)\n}\n    (You can edit and re-execute these runnable snippets if you want to explore.)\n    Even though the slice header is passed by value, the header includes\n    a pointer to elements of an array, so both the original slice header\n    and the copy of the header passed to the function describe the same\n    array.\n    Therefore, when the function returns, the modified elements can\n    be seen through the original slice variable.\n    The argument to the function really is a copy, as this example shows:\n	\n// +build OMIT\n// Copyright 2013 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\npackage main\nimport (\n	\"fmt\"\n)\nvar buffer [256]byte\nvar slice []byte = buffer[100:150]\nfunc SubtractOneFromLength(slice []byte) []byte {\n    slice = slice[0 : len(slice)-1]\n    return slice\n}\nfunc main() {\n    fmt.Println(\"Before: len(slice) =\", len(slice))\n    newSlice := SubtractOneFromLength(slice)\n    fmt.Println(\"After:  len(slice) =\", len(slice))\n    fmt.Println(\"After:  len(newSlice) =\", len(newSlice))\n}\n    Here we see that the contents of a slice argument can be modified by a function,\n    but its header cannot.\n    The length stored in the slice variable is not modified by the call to the function,\n    since the function is passed a copy of the slice header, not the original.\n    Thus if we want to write a function that modifies the header, we must return it as a result\n    parameter, just as we have done here.\n    The slice variable is unchanged but the returned value has the new length,\n    which is then stored in newSlice,\n  Pointers to slices: Method receivers\n    Another way to have a function modify the slice header is to pass a pointer to it.\n    Here\'s a variant of our previous example that does this:\n	\n// +build OMIT\n// Copyright 2013 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\npackage main\nimport (\n	\"fmt\"\n)\nvar buffer [256]byte\nvar slice []byte = buffer[100:150]\nfunc PtrSubtractOneFromLength(slicePtr *[]byte) {\n    slice := *slicePtr\n    *slicePtr = slice[0 : len(slice)-1]\n}\nfunc main() {\n    fmt.Println(\"Before: len(slice) =\", len(slice))\n    PtrSubtractOneFromLength(&amp;slice)\n    fmt.Println(\"After:  len(slice) =\", len(slice))\n}\n    It seems clumsy in that example, especially dealing with the extra level of indirection\n    (a temporary variable helps),\n    but there is one common case where you see pointers to slices.\n    It is idiomatic to use a pointer receiver for a method that modifies a slice.\n    Let\'s say we wanted to have a method on a slice that truncates it at the final slash.\n    We could write it like this:\n	\n// +build OMIT\n// Copyright 2013 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\npackage main\nimport (\n	\"bytes\"\n	\"fmt\"\n)\ntype path []byte\nfunc (p *path) TruncateAtFinalSlash() {\n    i := bytes.LastIndex(*p, []byte(\"/\"))\n    if i &gt;= 0 {\n        *p = (*p)[0:i]\n    }\n}\nfunc main() {\n    pathName := path(\"/usr/bin/tso\") // Conversion from string to path.\n    pathName.TruncateAtFinalSlash()\n    fmt.Printf(\"%s\\n\", pathName)\n}\n    If you run this example you\'ll see that it works properly, updating the slice in the caller.\n    [Exercise: Change the type of the receiver to be a value rather\n    than a pointer and run it again. Explain what happens.]\n    On the other hand, if we wanted to write a method for path that upper-cases\n    the ASCII letters in the path (parochially ignoring non-English names), the method could\n    be a value because the value receiver will still point to the same underlying array.\n	\n// +build OMIT\n// Copyright 2013 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\npackage main\nimport (\n	\"fmt\"\n)\ntype path []byte\nfunc (p path) ToUpper() {\n    for i, b := range p {\n        if \'a\' &lt;= b &amp;&amp; b &lt;= \'z\' {\n            p[i] = b + \'A\' - \'a\'\n        }\n    }\n}\nfunc main() {\n    pathName := path(\"/usr/bin/tso\")\n    pathName.ToUpper()\n    fmt.Printf(\"%s\\n\", pathName)\n}\n    Here the ToUpper method uses two variables in the for range construct\n    to capture the index and slice element.\n    This form of loop avoids writing p[i] multiple times in the body.\n    [Exercise: Convert the ToUpper method to use a pointer receiver and see if its behavior changes.]\n    [Advanced exercise: Convert the ToUpper method to handle Unicode letters, not just ASCII.]\n  Capacity\n    Look at the following function that extends its argument slice of ints by one element:\n	\nfunc Extend(slice []int, element int) []int {\n    n := len(slice)\n    slice = slice[0 : n+1]\n    slice[n] = element\n    return slice\n}\n    (Why does it need to return the modified slice?) Now run it:\n	\n// +build OMIT\n// Copyright 2013 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\npackage main\nimport (\n	\"fmt\"\n)\nfunc Extend(slice []int, element int) []int {\n	n := len(slice)\n	slice = slice[0 : n+1]\n	slice[n] = element\n	return slice\n}\nfunc main() {\n    var iBuffer [10]int\n    slice := iBuffer[0:0]\n    for i := 0; i &lt; 20; i++ {\n        slice = Extend(slice, i)\n        fmt.Println(slice)\n    }\n}\n    See how the slice grows until... it doesn\'t.\n    It\'s time to talk about the third component of the slice header: its capacity.\n    Besides the array pointer and length, the slice header also stores its capacity:\n  type sliceHeader struct {\n    Length        int\n    Capacity      int\n    ZerothElement *byte\n}\n    The Capacity field records how much space the underlying array actually has; it is the maximum\n    value the Length can reach.\n    Trying to grow the slice beyond its capacity will step beyond the limits of the array and will trigger a panic.\n    After our example slice is created by\n  slice := iBuffer[0:0]\n    its header looks like this:\n  slice := sliceHeader{\n    Length:        0,\n    Capacity:      10,\n    ZerothElement: &amp;iBuffer[0],\n}\n    The Capacity field is equal to the length of the underlying array,\n    minus the index in the array of the first element of the slice (zero in this case).\n    If you want to inquire what the capacity is for a slice, use the built-in function cap:\n  if cap(slice) == len(slice) {\n    fmt.Println(\"slice is full!\")\n}\n  Make\n    What if we want to grow the slice beyond its capacity?\n    You can\'t!\n    By definition, the capacity is the limit to growth.\n    But you can achieve an equivalent result by allocating a new array, copying the data over, and modifying\n    the slice to describe the new array.\n    Let\'s start with allocation.\n    We could use the new built-in function to allocate a bigger array\n    and then slice the result,\n    but it is simpler to use the make built-in function instead.\n    It allocates a new array and\n    creates a slice header to describe it, all at once.\n    The make function takes three arguments: the type of the slice, its initial length, and its capacity, which is the\n    length of the array that make allocates to hold the slice data.\n    This call creates a slice of length 10 with room for 5 more (15-10), as you can see by running it:\n	\n// +build OMIT\n// Copyright 2013 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\npackage main\nimport (\n	\"fmt\"\n)\nfunc main() {\n    slice := make([]int, 10, 15)\n    fmt.Printf(\"len: %d, cap: %d\\n\", len(slice), cap(slice))\n}\n    This snippet doubles the capacity of our int slice but keeps its length the same:\n	\n// +build OMIT\n// Copyright 2013 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\npackage main\nimport (\n	\"fmt\"\n)\nfunc main() {\n    slice := make([]int, 10, 15)\n    fmt.Printf(\"len: %d, cap: %d\\n\", len(slice), cap(slice))\n    newSlice := make([]int, len(slice), 2*cap(slice))\n    for i := range slice {\n        newSlice[i] = slice[i]\n    }\n    slice = newSlice\n    fmt.Printf(\"len: %d, cap: %d\\n\", len(slice), cap(slice))\n}\n    After running this code the slice has much more room to grow before needing another reallocation.\n    When creating slices, it\'s often true that the length and capacity will be same.\n    The make built-in has a shorthand for this common case.\n    The length argument defaults to the capacity, so you can leave it out\n    to set them both to the same value.\n    After\n  gophers := make([]Gopher, 10)\n    the gophers slice has both its length and capacity set to 10.\n  Copy\n    When we doubled the capacity of our slice in the previous section,\n    we wrote a loop to copy the old data to the new slice.\n    Go has a built-in function, copy, to make this easier.\n    Its arguments are two slices, and it copies the data from the right-hand argument to the left-hand argument.\n    Here\'s our example rewritten to use copy:\n	\n// +build OMIT\n// Copyright 2013 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\npackage main\nimport (\n	\"fmt\"\n)\nfunc main() {\n	slice := make([]int, 10, 15)\n	fmt.Printf(\"len: %d, cap: %d\\n\", len(slice), cap(slice))\n    newSlice := make([]int, len(slice), 2*cap(slice))\n    copy(newSlice, slice)\n	slice = newSlice\n	fmt.Printf(\"len: %d, cap: %d\\n\", len(slice), cap(slice))\n}\n    The copy function is smart.\n    It only copies what it can, paying attention to the lengths of both arguments.\n    In other words, the number of elements it copies is the minimum of the lengths of the two slices.\n    This can save a little bookkeeping.\n    Also, copy returns an integer value, the number of elements it copied, although it\'s not always worth checking.\n    The copy function also gets things right when source and destination overlap, which means it can be used to shift\n    items around in a single slice.\n    Here\'s how to use copy to insert a value into the middle of a slice.\n	\n// Insert inserts the value into the slice at the specified index,\n// which must be in range.\n// The slice must have room for the new element.\nfunc Insert(slice []int, index, value int) []int {\n    // Grow the slice by one element.\n    slice = slice[0 : len(slice)+1]\n    // Use copy to move the upper part of the slice out of the way and open a hole.\n    copy(slice[index+1:], slice[index:])\n    // Store the new value.\n    slice[index] = value\n    // Return the result.\n    return slice\n}\n    There are a couple of things to notice in this function.\n    First, of course, it must return the updated slice because its length has changed.\n    Second, it uses a convenient shorthand.\n    The expression\n  slice[i:]\n    means exactly the same as\n  slice[i:len(slice)]\n    Also, although we haven\'t used the trick yet, we can leave out the first element of a slice expression too;\n    it defaults to zero. Thus\n  slice[:]\n    just means the slice itself, which is useful when slicing an array.\n    This expression is the shortest way to say \"a slice describing all the elements of the array\":\n  array[:]\n    Now that\'s out of the way, let\'s run our Insert function.\n	\n// +build OMIT\n// Copyright 2013 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\npackage main\nimport (\n	\"fmt\"\n)\n// Insert inserts the value into the slice at the specified index,\n// which must be in range.\n// The slice must have room for the new element.\nfunc Insert(slice []int, index, value int) []int {\n	// Grow the slice by one element.\n	slice = slice[0 : len(slice)+1]\n	// Use copy to move the upper part of the slice out of the way and open a hole.\n	copy(slice[index+1:], slice[index:])\n	// Store the new value.\n	slice[index] = value\n	// Return the result.\n	return slice\n}\nfunc main() {\n    slice := make([]int, 10, 20) // Note capacity &gt; length: room to add element.\n    for i := range slice {\n        slice[i] = i\n    }\n    fmt.Println(slice)\n    slice = Insert(slice, 5, 99)\n    fmt.Println(slice)\n}\n  Append: An example\n    A few sections back, we wrote an Extend function that extends a slice by one element.\n    It was buggy, though, because if the slice\'s capacity was too small, the function would\n    crash.\n    (Our Insert example has the same problem.)\n    Now we have the pieces in place to fix that, so let\'s write a robust implementation of\n    Extend for integer slices.\n	\nfunc Extend(slice []int, element int) []int {\n    n := len(slice)\n    if n == cap(slice) {\n        // Slice is full; must grow.\n        // We double its size and add 1, so if the size is zero we still grow.\n        newSlice := make([]int, len(slice), 2*len(slice)+1)\n        copy(newSlice, slice)\n        slice = newSlice\n    }\n    slice = slice[0 : n+1]\n    slice[n] = element\n    return slice\n}\n    In this case it\'s especially important to return the slice, since when it reallocates\n    the resulting slice describes a completely different array.\n    Here\'s a little snippet to demonstrate what happens as the slice fills up:\n	\n// +build OMIT\n// Copyright 2013 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\npackage main\nimport (\n	\"fmt\"\n)\n// Extend extends the slice by adding the element to the end.\nfunc Extend(slice []int, element int) []int {\n	n := len(slice)\n	if n == cap(slice) {\n		// Slice is full; must grow.\n		// We double its size and add 1, so if the size is zero we still grow.\n		newSlice := make([]int, len(slice), 2*len(slice)+1)\n		copy(newSlice, slice)\n		slice = newSlice\n	}\n	slice = slice[0 : n+1]\n	slice[n] = element\n	return slice\n}\nfunc main() {\n    slice := make([]int, 0, 5)\n    for i := 0; i &lt; 10; i++ {\n        slice = Extend(slice, i)\n        fmt.Printf(\"len=%d cap=%d slice=%v\\n\", len(slice), cap(slice), slice)\n        fmt.Println(\"address of 0th element:\", &amp;slice[0])\n    }\n}\n    Notice the reallocation when the initial array of size 5 is filled up.\n    Both the capacity and the address of the zeroth element change when the new array is allocated.\n    With the robust Extend function as a guide we can write an even nicer function that lets\n    us extend the slice by multiple elements.\n    To do this, we use Go\'s ability to turn a list of function arguments into a slice when the\n    function is called.\n    That is, we use Go\'s variadic function facility.\n    Let\'s call the function Append.\n    For the first version, we can just call Extend repeatedly so the mechanism of the variadic function is clear.\n    The signature of Append is this:\n  func Append(slice []int, items ...int) []int\n    What that says is that Append takes one argument, a slice, followed by zero or more\n    int arguments.\n    Those arguments are exactly a slice of int as far as the implementation\n    of Append is concerned, as you can see:\n	\n// Append appends the items to the slice.\n// First version: just loop calling Extend.\nfunc Append(slice []int, items ...int) []int {\n    for _, item := range items {\n        slice = Extend(slice, item)\n    }\n    return slice\n}\n    Notice the for range loop iterating over the elements of the items argument, which has implied type []int.\n    Also notice the use of the blank identifier _ to discard the index in the loop, which we don\'t need in this case.\n    Try it:\n	\n// +build OMIT\n// Copyright 2013 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\npackage main\nimport (\n	\"fmt\"\n)\n// Extend extends the slice by adding the element to the end.\nfunc Extend(slice []int, element int) []int {\n	n := len(slice)\n	if n == cap(slice) {\n		// Slice is full; must grow.\n		// We double its size and add 1, so if the size is zero we still grow.\n		newSlice := make([]int, len(slice), 2*len(slice)+1)\n		copy(newSlice, slice)\n		slice = newSlice\n	}\n	slice = slice[0 : n+1]\n	slice[n] = element\n	return slice\n}\n// Append appends the items to the slice.\n// First version: just loop calling Extend.\nfunc Append(slice []int, items ...int) []int {\n	for _, item := range items {\n		slice = Extend(slice, item)\n	}\n	return slice\n}\nfunc main() {\n    slice := []int{0, 1, 2, 3, 4}\n    fmt.Println(slice)\n    slice = Append(slice, 5, 6, 7, 8)\n    fmt.Println(slice)\n}\n    Another new technique is in this example is that we initialize the slice by writing a composite literal,\n    which consists of the type of the slice followed by its elements in braces:\n	\n    slice := []int{0, 1, 2, 3, 4}\n    The Append function is interesting for another reason.\n    Not only can we append elements, we can append a whole second slice\n    by \"exploding\" the slice into arguments using the ... notation at the call site:\n	\n// +build OMIT\n// Copyright 2013 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\npackage main\nimport (\n	\"fmt\"\n)\n// Extend extends the slice by adding the element to the end.\nfunc Extend(slice []int, element int) []int {\n	n := len(slice)\n	if n == cap(slice) {\n		// Slice is full; must grow.\n		// We double its size and add 1, so if the size is zero we still grow.\n		newSlice := make([]int, len(slice), 2*len(slice)+1)\n		copy(newSlice, slice)\n		slice = newSlice\n	}\n	slice = slice[0 : n+1]\n	slice[n] = element\n	return slice\n}\n// Append appends the items to the slice.\n// First version: just loop calling Extend.\nfunc Append(slice []int, items ...int) []int {\n	for _, item := range items {\n		slice = Extend(slice, item)\n	}\n	return slice\n}\nfunc main() {\n    slice1 := []int{0, 1, 2, 3, 4}\n    slice2 := []int{55, 66, 77}\n    fmt.Println(slice1)\n    slice1 = Append(slice1, slice2...) // The \'...\' is essential!\n    fmt.Println(slice1)\n}\n    Of course, we can make Append more efficient by allocating no more than once,\n    building on the innards of Extend:\n	\n// Append appends the elements to the slice.\n// Efficient version.\nfunc Append(slice []int, elements ...int) []int {\n    n := len(slice)\n    total := len(slice) + len(elements)\n    if total &gt; cap(slice) {\n        // Reallocate. Grow to 1.5 times the new size, so we can still grow.\n        newSize := total*3/2 + 1\n        newSlice := make([]int, total, newSize)\n        copy(newSlice, slice)\n        slice = newSlice\n    }\n    slice = slice[:total]\n    copy(slice[n:], elements)\n    return slice\n}\n    Here, notice how we use copy twice, once to move the slice data to the newly\n    allocated memory, and then to copy the appending items to the end of the old data.\n    Try it; the behavior is the same as before:\n	\n// +build OMIT\n// Copyright 2013 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\npackage main\nimport (\n	\"fmt\"\n)\n// Append appends the elements to the slice.\n// Efficient version.\nfunc Append(slice []int, elements ...int) []int {\n	n := len(slice)\n	total := len(slice) + len(elements)\n	if total &gt; cap(slice) {\n		// Reallocate. Grow to 1.5 times the new size, so we can still grow.\n		newSize := total*3/2 + 1\n		newSlice := make([]int, total, newSize)\n		copy(newSlice, slice)\n		slice = newSlice\n	}\n	slice = slice[:total]\n	copy(slice[n:], elements)\n	return slice\n}\nfunc main() {\n    slice1 := []int{0, 1, 2, 3, 4}\n    slice2 := []int{55, 66, 77}\n    fmt.Println(slice1)\n    slice1 = Append(slice1, slice2...) // The \'...\' is essential!\n    fmt.Println(slice1)\n}\n  Append: The built-in function\n    And so we arrive at the motivation for the design of the append built-in function.\n    It does exactly what our Append example does, with equivalent efficiency, but it\n    works for any slice type.\n    A weakness of Go is that any generic-type operations must be provided by the\n    run-time. Some day that may change, but for now, to make working with slices\n    easier, Go provides a built-in generic append function.\n    It works the same as our int slice version, but for any slice type.\n    Remember, since the slice header is always updated by a call to append, you need\n    to save the returned slice after the call.\n    In fact, the compiler won\'t let you call append without saving the result.\n    Here are some one-liners intermingled with print statements. Try them, edit them and explore:\n	\n// +build OMIT\n// Copyright 2013 The Go Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\npackage main\nimport (\n	\"fmt\"\n)\nfunc main() {\n    // Create a couple of starter slices.\n    slice := []int{1, 2, 3}\n    slice2 := []int{55, 66, 77}\n    fmt.Println(\"Start slice: \", slice)\n    fmt.Println(\"Start slice2:\", slice2)\n    // Add an item to a slice.\n    slice = append(slice, 4)\n    fmt.Println(\"Add one item:\", slice)\n    // Add one slice to another.\n    slice = append(slice, slice2...)\n    fmt.Println(\"Add one slice:\", slice)\n    // Make a copy of a slice (of int).\n    slice3 := append([]int(nil), slice...)\n    fmt.Println(\"Copy a slice:\", slice3)\n    // Copy a slice to the end of itself.\n    fmt.Println(\"Before append to self:\", slice)\n    slice = append(slice, slice...)\n    fmt.Println(\"After append to self:\", slice)\n}\n    It\'s worth taking a moment to think about the final one-liner of that example in detail to understand\n    how the design of slices makes it possible for this simple call to work correctly.\n    There are lots more examples of append, copy, and other ways to use slices\n    on the community-built\n    \"Slice Tricks\" Wiki page.\n  Nil\n    As an aside, with our newfound knowledge we can see what the representation of a nil slice is.\n    Naturally, it is the zero value of the slice header:\n  sliceHeader{\n    Length:        0,\n    Capacity:      0,\n    ZerothElement: nil,\n}\n    or just\n  sliceHeader{}\n    The key detail is that the element pointer is nil too. The slice created by\n  array[0:0]\n    has length zero (and maybe even capacity zero) but its pointer is not nil, so\n    it is not a nil slice.\n    As should be clear, an empty slice can grow (assuming it has non-zero capacity), but a nil\n    slice has no array to put values in and can never grow to hold even one element.\n    That said, a nil slice is functionally equivalent to a zero-length slice, even though it points\n    to nothing.\n    It has length zero and can be appended to, with allocation.\n    As an example, look at the one-liner above that copies a slice by appending\n    to a nil slice.\n  Strings\n    Now a brief section about strings in Go in the context of slices.\n    Strings are actually very simple: they are just read-only slices of bytes with a bit\n    of extra syntactic support from the language.\n    Because they are read-only, there is no need for a capacity (you can\'t grow them),\n    but otherwise for most purposes you can treat them just like read-only slices\n    of bytes.\n    For starters, we can index them to access individual bytes:\n  slash := \"/usr/ken\"[0] // yields the byte value \'/\'.\n    We can slice a string to grab a substring:\n  usr := \"/usr/ken\"[0:4] // yields the string \"/usr\"\n    It should be obvious now what\'s going on behind the scenes when we slice a string.\n    We can also take a normal slice of bytes and create a string from it with the simple conversion:\n  str := string(slice)\n    and go in the reverse direction as well:\n  slice := []byte(usr)\n    The array underlying a string is hidden from view; there is no way to access its contents\n    except through the string. That means that when we do either of these conversions, a\n    copy of the array must be made.\n    Go takes care of this, of course, so you don\'t have to.\n    After either of these conversions, modifications to\n    the array underlying the byte slice don\'t affect the corresponding string.\n    An important consequence of this slice-like design for strings is that\n    creating a substring is very efficient.\n    All that needs to happen\n    is the creation of a two-word string header. Since the string is read-only, the original\n    string and the string resulting from the slice operation can share the same array safely.\n    A historical note: The earliest implementation of strings always allocated, but when slices\n    were added to the language, they provided a model for efficient string handling. Some of\n    the benchmarks saw huge speedups as a result.\n    There\'s much more to strings, of course, and a\n    separate blog post covers them in greater depth.\n  Conclusion\n    To understand how slices work, it helps to understand how they are implemented.\n    There is a little data structure, the slice header, that is the item associated with the slice\n    variable, and that header describes a section of a separately allocated array.\n    When we pass slice values around, the header gets copied but the array it points\n    to is always shared.\n    Once you appreciate how they work, slices become not only easy to use, but\n    powerful and expressive, especially with the help of the copy and append\n    built-in functions.\n  More reading\n    There\'s lots to find around the intertubes about slices in Go.\n    As mentioned earlier,\n    the \"Slice Tricks\" Wiki page\n    has many examples.\n    The Go Slices blog post\n    describes the memory layout details with clear diagrams.\n    Russ Cox\'s Go Data Structures article includes\n    a discussion of slices along with some of Go\'s other internal data structures.\n    There is much more material available, but the best way to learn about slices is to use them.\n		\n			By Rob Pike\n		\n	','https://blog.golang.org/slices','2013-09-26','2017-05-07 13:50:01.238574'),(518,'HTTP/2 Server Push','Jaana Burcu Dogan, Tom Bergan','		HTTP/2 Server Push\n		24 March 2017\n		\n  Introduction\n    HTTP/2 is designed to address many of the failings of HTTP/1.x.\n    Modern web pages use many resources: HTML, stylesheets,\n    scripts, images, and so on. In HTTP/1.x, each of these resources must\n    be requested explicitly. This can be a slow process.\n    The browser starts by fetching the HTML, then learns of more resources\n    incrementally as it parses and evaluates the page. Since the server\n    must wait for the browser to make each request, the network is often\n    idle and underutilized.\n    To improve latency, HTTP/2 introduced server push, which allows the\n    server to push resources to the browser before they are explicitly\n    requested. A server often knows many of the additional resources a\n    page will need and can start pushing those resources as it responds\n    to the initial request. This allows the server to fully utilize an\n    otherwise idle network and improve page load times.\n    At the protocol level, HTTP/2 server push is driven by PUSH_PROMISE\n    frames. A PUSH_PROMISE describes a request that the server predicts the\n    browser will make in the near future. As soon as the browser receives\n    a PUSH_PROMISE, it knows that the server will deliver the resource.\n    If the browser later discovers that it needs this resource, it will\n    wait for the push to complete rather than sending a new request.\n    This reduces the time the browser spends waiting on the network.\n  Server Push in net/http\n    Go 1.8 introduced support for pushing responses from an http.Server.\n    This feature is available if the running server is an HTTP/2 server\n    and the incoming connection uses HTTP/2. In any HTTP handler,\n    you can assert if the http.ResponseWriter supports server push by checking\n    if it implements the new http.Pusher interface.\n    For example, if the server knows that app.js will be required to\n    render the page, the handler can initiate a push if http.Pusher\n    is available:\n	\n    http.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) {\n        if pusher, ok := w.(http.Pusher); ok {\n            // Push is supported.\n            if err := pusher.Push(\"/app.js\", nil); err != nil {\n                log.Printf(\"Failed to push: %v\", err)\n            }\n        }\n        // ...\n    })\n    The Push call creates a synthetic request for /app.js,\n    synthesizes that request into a PUSH_PROMISE frame, then forwards\n    the synthetic request to the server\'s request handler, which will\n    generate the pushed response. The second argument to Push specifies\n    additional headers to include in the PUSH_PROMISE. For example,\n    if the response to /app.js varies on Accept-Encoding,\n    then the PUSH_PROMISE should include an Accept-Encoding value:\n	\n    http.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) {\n        if pusher, ok := w.(http.Pusher); ok {\n            // Push is supported.\n            options := &amp;http.PushOptions{\n                Header: http.Header{\n                    \"Accept-Encoding\": r.Header[\"Accept-Encoding\"],\n                },\n            }\n            if err := pusher.Push(\"/app.js\", options); err != nil {\n                log.Printf(\"Failed to push: %v\", err)\n            }\n        }\n        // ...\n    })\n    A fully working example is available at:\n  $ go get golang.org/x/blog/content/h2push/server\n    If you run the server and load https://localhost:8080,\n    your browser\'s developer tools should show that app.js and\n    style.css were pushed by the server.\n  Start Your Pushes Before You Respond\n    It\'s a good idea to call the Push method before sending any bytes\n    of the response. Otherwise it is possible to accidentally generate\n    duplicate responses. For example, suppose you write part of an HTML\n    response:\n  &lt;html&gt;\n&lt;head&gt;\n    &lt;link rel=\"stylesheet\" href=\"a.css\"&gt;...\n    Then you call Push(\"a.css\", nil). The browser may parse this fragment\n    of HTML before it receives your PUSH_PROMISE, in which case the browser\n    will send a request for a.css in addition to receiving your\n    PUSH_PROMISE. Now the server will generate two responses for a.css.\n    Calling Push before writing the response avoids this possibility entirely.\n  When To Use Server Push\n    Consider using server push any time your network link is idle.\n    Just finished sending the HTML for your web app? Don\'t waste time waiting,\n    start pushing the resources your client will need. Are you inlining\n    resources into your HTML file to reduce latency? Instead of inlining,\n    try pushing. Redirects are another good time to use push because there\n    is almost always a wasted round trip while the client follows the redirect.\n    There are many possible scenarios for using push -- we are only getting started.\n    We would be remiss if we did not mention a few caveats. First, you can only\n    push resources your server is authoritative for -- this means you cannot\n    push resources that are hosted on third-party servers or CDNs. Second,\n    don\'t push resources unless you are confident they are actually needed\n    by the client, otherwise your push wastes bandwidth. A corollary is to\n    avoid pushing resources when it\'s likely that the client already has\n    those resources cached. Third, the naive approach of pushing all\n    resources on your page often makes performance worse. When in doubt, measure.\n    The following links make for good supplemental reading:\n    HTTP/2 Push: The Details\n    Innovating with HTTP/2 Server Push\n    Cache-Aware Server Push in H2O\n    The PRPL Pattern\n    Rules of Thumb for HTTP/2 Push\n    Server Push in the HTTP/2 spec\n  Conclusion\n    With Go 1.8, the standard library provides out-of-the-box support for HTTP/2\n    Server Push, giving you more flexibility to optimize your web applications.\n    Go to our HTTP/2 Server Push demo\n    page to see it in action.\n		\n			By Jaana Burcu Dogan, Tom Bergan\n		\n	','https://blog.golang.org/h2push','2017-03-24','2017-05-07 13:50:01.294126'),(519,'Four years of Go','Andrew Gerrand','		Four years of Go\n		10 November 2013\n		\n    Today marks the fourth anniversary of Go as an open source project.\n    Rather than talk about our technical progress (there\'ll be much to talk about\n    when we release Go 1.2 in a couple of weeks) we thought we would instead take\n    this occasion to look at how the Go community has grown.\n    Let\'s start with a chart:\n    This chart shows the growth of Google searches for the term\n    \"golang\"\n    over the past four years.\n    Notice the knee in the curve around March 2012, when Go 1.0 was released.\n    If these searches are a decent proxy for interest, then it\'s clear that\n    interest in Go has grown remarkably since launch, and particularly so in the\n    last 2 years.\n    But where is the interest coming from?\n    The open source community has embraced Go, with our community wiki listing hundreds of Go projects. Some popular ones:\n    Docker is a tool for packaging and running applications in lightweight containers. Docker makes it easy to isolate, package, and deploy applications, and is beloved by system administrators. Its creator Solomon Hykes cited Go\'s standard library, concurrency primitives, and ease of deployment as key factors, and said \"To put it simply, if Docker had not been written in Go, it would not have been as successful.\"\n    Packer is a tool for automating the creation of machine images for deployment to virtual machines or cloud services. Its author, Mitchell Hashimoto, is now working on another Go project, serf, a decentralized discovery service. Like Docker, these projects help with management of large-scale, cluster-based services.\n    Bitly\'s NSQ is a realtime distributed messaging platform designed for fault-tolerance and high-availability, and is used in production at bitly and a bunch of other companies.\n    Canonical\'s JuJu infrastructure automation system was rewritten in Go. Project lead Gustavo Niemeyer said \"It\'s not a single aspect of Go that makes it a compelling choice, but rather the careful organization of well-crafted small pieces.\"\n    The raft package provides an implementation of the Raft distributed consensus protocol. It is the basis of Go projects like etcd and SkyDNS.\n    Other popular projects include biogo, the Gorilla Web Toolkit, groupcache, Mozilla\'s heka, the kv and ql lightweight storage systems, and the Sky behavioral database. \n    But this is just the tip of the iceberg. The number of high-quality open source Go projects is phenomenal. Prolific Go hacker Keith Rarick put it well: \"The state of the Go ecosystem after only four years is astounding. Compare Go in 2013 to Python in 1995 or Java in 1999. Or C++ in 1987!\"\n    Businesses are enjoying Go, too. The Go Users wiki page lists dozens of success stories (and if you use Go, please add yourself to it). Some examples:\n    CloudFlare built their distributed DNS service entirely with Go, and are in the process of migrating their gigabytes-per-minute logging infrastructure to the language. Programmer John Graham-Cumming said \"We\'ve found Go to be the perfect match for our needs: the combination of familiar syntax, a powerful type system, a strong network library and built-in concurrency means that more and more projects are being built here in Go.\" \n    SoundCloud is an audio distribution service that has \"dozens of systems in Go, touching almost every part of the site, and in many cases powering features from top to bottom.\" Engineer Peter Bourgon said \"Go demonstrates that the cruft that burdens other languages and ecosystems—stuff that developers have learned to deal with, often in anger—is simply not a necessary part of modern programming. With Go, I have a straightforward and non-adversarial relationship with my tools, from development to production.\"\n    The ngrok service allows web developers to provide remote access to their development environments. Its author Alan Shreve said that \"ngrok\'s success as a project is due in no small part to choosing Go as the implementation language,\" citing Go\'s HTTP libraries, efficiency, cross-platform compatibility, and ease of deployment as the major benefits.\n    Poptip provides social analytics services, and product engineer Andy Bonventre said \"What started as an experiment in writing a single service in Go turned into moving almost our entire infrastructure over to it. What I love about Go the most is not necessarily the features of the language, but the focus on tooling, testing, and other elements that make writing large applications much more manageable.\"\n    Music collaboration startup Splice chose to build their service with Go. Co-founder Matt Aimonetti said \"We seriously studied and considered many programming languages, but Go\'s simplicity, efficiency, philosophy and community won us over.\"\n    And, of course, engineering teams across Google are moving to Go. Engineer Matt Welsh recently shared his experience rewriting a large production service in Go. Other notable public examples include YouTube\'s vitess project and dl.google.com. We hope to share more stories like these soon.\n    In September 2012, Apcera CEO Derek Collison predicted that \"Go will become the dominant language for systems work in [Infastructure-as-a-Service], Orchestration, and [Platform-as-a-Service] in 24 months.\" Looking at the list above, it\'s easy to believe that prediction.\n    So how can you get involved? Whether you\'re a seasoned Go programmer or just Go-curious, there are many ways to get started in the Go community:\n    Join your nearest Go User Group, where your local gophers meet to share their knowledge and experience. These groups are popping up all over the world. I have personally spoken at Go groups in Amsterdam, Berlin, Gothenburg, London, Moscow, Munich, New York City, Paris, San Francisco, Seoul, Stockholm, Sydney, Tokyo, and Warsaw; but there are many more!\n    Create or contribute to an open source Go project (or to Go itself). (And if you\'re building something, we\'d love to hear from you on the Go mailing list.)\n    If you\'re in Europe in February 2014, come along to the Go Devroom at FOSDEM 2014.\n    Attend GopherCon, the first major Go conference, in Denver in April 2014. The event is organized by the Gopher Academy, who also run a Go job board.\n    The Go team has been amazed by the growth of the Go community over the past\n    four years. We are thrilled to see so many great things being built with Go,\n    and deeply grateful to work with our wonderful and dedicated contributors.\n    Thank you, everyone.\n    Here\'s to four more years!\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/4years','2013-11-10','2017-05-07 13:50:01.321438'),(520,'Building StatHat with Go','Patrick Crosby','		Building StatHat with Go\n		19 December 2011\n		\n  Introduction\n    My name is Patrick Crosby and I\'m the founder of a company called Numerotron. We recently released StatHat.  This post is about why we chose to develop StatHat in Go, including details about how we are using Go.\n    StatHat is a tool to track statistics and events in your code.  Everyone from HTML designers to backend engineers can use StatHat easily, as it supports sending stats from HTML, JavaScript, Go, and twelve other languages.\n    You send your numbers to StatHat; it generates beautiful, fully-embeddable graphs of your data.  StatHat will alert you when specified triggers occur, send you daily email reports, and much more.  So instead of spending time writing tracking or reporting tools for your application, you can concentrate on the code.  While you do the real work, StatHat remains intensely vigilant, like an eagle in its mountaintop nest, or a babysitter on meth.\n    Here\'s an example of a StatHat graph of the temperature in NYC, Chicago, and San Francisco:\n  Architecture Overview\n    StatHat consists of two main services:  incoming statistic/event API calls and the web application for viewing and analyzing stats.  We wanted to keep these as separate as possible to isolate the data collection from the data interaction.   We did this for many reasons, but one major reason is that we anticipate handling a ton of automated incoming API HTTP requests and would thus have different optimization strategies for the API service than a web application interacting with humans.\n    The web application service is multi-tiered.  The web server processes all requests and sends them to an interactor layer.  For simple tasks, the interactor will handle generating any necessary data.  For complex tasks, the interactor relies on multiple application servers to handle tasks like generating graphs or analyzing data sets.  After the interactor is finished, the web server sends the result to a presenter.  The presenter responds to the HTTP request with either HTML or JSON.  We can horizontally scale the web, API, application servers, and databases as the demand for services grows and changes over time.  There is no single point of failure as each application server has multiple copies running.  The interactor layer allows us to have different interfaces to the system:  http, command line, automated tests, mobile API.  StatHat uses MySQL for data storage.\n  Choosing Go\n    When we designed StatHat, we had the following check list for our development tools:\n    same programming language for backend and frontend systems\n    good, fast HTML templating system\n    fast start-up, recompilation, testing for lots of tinkering\n    lots of connections on one machine\n    language tools for handling application-level concurrency\n    good performance\n    robust RPC layer to talk between tiers\n    lots of libraries\n    open source\n    We evaluated many popular and not-so-popular web technologies and ended up choosing to develop it in Go.\n    When Go was released in November 2009, I immediately installed it and loved the fast compilation times, goroutines, channels, garbage collection, and all the packages that were available.  I was especially pleased with how few lines of code my applications were using.  I soon experimented with making a web app called Langalot that concurrently searched through five foreign language dictionaries as you typed in a query.  It was blazingly fast.  I put it online and it\'s been running since February, 2010.\n    The following sections detail how Go meets StatHat\'s requirements and our experience using Go to solve our problems.\n  Runtime\n    We use the standard Go http package for our API and web app servers.  All requests first go through Nginx and any non-file requests are proxied to the Go-powered http servers.  The backend servers are all written in Go and use the rpc package to communicate with the frontend.\n  Templating\n    We built a template system using the standard template package.  Our system adds layouts, some common formatting functions, and the ability to recompile templates on-the-fly during development.  We are very pleased with the performance and functionality of the Go templates.\n  Tinkering\n    In a previous job, I worked on a video game called Throne of Darkness that was written in C++.  We had a few header files that, when modified, required a full rebuild of the entire system, 20-30 minutes long.  If anyone ever changed Character.h, he would be subject to the wrath of every other programmer.  Besides this suffering, it also slowed down development time significantly.\n    Since then, I\'ve always tried to choose technologies that allowed fast, frequent tinkering.  With Go, compilation time is a non-issue.  We can recompile the entire system in seconds, not minutes.  The development web server starts instantly, tests complete in a few seconds.  As mentioned previously, templates are recompiled as they change.  The result is that the StatHat system is very easy to work with, and the compiler is not a bottleneck.\n  RPC\n    Since StatHat is a multi-tiered system, we wanted an RPC layer so that all communication was standard.  With Go, we are using the rpc package and the gob package for encoding Go objects.  In Go, the RPC server just takes any Go object and registers its exported methods.  There is no need for an intermediary interface description language.  We\'ve found it very easy to use and many of our core application servers are under 300 lines of code.\n  Libraries\n    We don\'t want to spend time rewriting libraries for things like SSL, database drivers, JSON/XML parsers.  Although Go is a young language, it has a lot of system packages and a growing number of user-contributed packages.  With only a few exceptions, we have found Go packages for everything we have needed.\n  Open source\n    In our experience, it has been invaluable to work with open source tools. If something is going awry, it is immensely helpful to be able to examine the source through every layer and not have any black boxes.  Having the code for the language, web server, packages, and tools allows us to understand how every piece of the system works.  Everything in Go is open source.  In the Go codebase, we frequently read the tests as they often give great examples of how to use packages and language features.\n  Performance\n    People rely on StatHat for up to the minute analysis of their data and we need the system to be as responsive as possible.  In our tests, Go\'s performance blew away most of the competition.  We tested it against Rails, Sinatra, OpenResty, and Node.  StatHat has always monitored itself by tracking all kinds of performance metrics about requests, the duration of certain tasks, the amount of memory in use.  Because of this, we were able to easily evaluate different technologies.  We\'ve also taken advantage of the benchmark performance testing features of the Go testing package.\n  Application-Level Concurrency\n    In a former life, I was the CTO at OkCupid.  My experience there using OKWS taught me the importance of async programming, especially when it comes to dynamic web applications.  There is no reason you should ever do something like this synchronously:  load a user from the database, then find their stats, then find their alerts.  These should all be done concurrently, yet surprisingly, many popular frameworks have no async support.  Go supports this at the language level without any callback spaghetti.  StatHat uses goroutines extensively to run multiple functions concurrently and channels for sharing data between goroutines.\n  Hosting and Deployment\n    StatHat runs on Amazon\'s EC2 servers.  Our servers are divided into several types:\n    API\n    Web\n    Application servers\n    Database\n    There are at least two of each type of server, and they are in different zones for high availability.  Adding a new server to the mix takes just a couple of minutes.\n    To deploy, we first build the entire system into a time-stamped directory. Our packaging script builds the Go applications, compresses the CSS and JS files, and copies all the scripts and configuration files.  This directory is then distributed to all the servers, so they all have an identical distribution.  A script on each server queries its EC2 tags and determines what it is responsible for running and starts/stops/restarts any services. We frequently only deploy to a subset of the servers.\n  More\n    For more information on StatHat, please visit stathat.com. We are releasing some of the Go code we\'ve written. Go to www.stathat.com/src for all of the open source StatHat projects.\n    To learn more about Go, visit golang.org.\n		\n			By Patrick Crosby\n		\n	','https://blog.golang.org/building-stathat-with-go','2011-12-19','2017-05-07 13:50:01.401558'),(521,'Two recent Go talks','Andrew Gerrand','		Two recent Go talks\n		2 January 2013\n		\n  Introduction\n    Late last year I wrote a couple of Go talks and presented them at Strange Loop, Øredev, and various other venues. The talks are designed to give insight into the practice of Go programming, each describing the construction of a real program and demonstrating the power and depth of the Go language and its libraries and tools.\n    The following videos are, in my opinion, the best recordings of these talks.\n  Go: a simple programming environment\n    Go is a general-purpose language that bridges the gap between efficient statically typed languages and productive dynamic language. But it’s not just the language that makes Go special – Go has broad and consistent standard libraries and powerful but simple tools.\n    This talk gives an introduction to Go, followed by a tour of some real programs that demonstrate the power, scope, and simplicity of the Go programming environment.\n    See the slide deck (use the left and right arrows to navigate).\n  Go: code that grows with grace\n    One of Go\'s key design goals is code adaptability; that it should be easy to take a simple design and build upon it in a clean and natural way. In this talk I describe a simple \"chat roulette\" server that matches pairs of incoming TCP connections, and then use Go\'s concurrency mechanisms, interfaces, and standard library to extend it with a web interface and other features. While the function of the program changes dramatically, Go\'s flexibility preserves the original design as it grows.\n    See the slide deck (use the left and right arrows to navigate).\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/two-recent-go-talks','2013-01-02','2017-05-07 13:50:01.498083'),(522,'The App Engine SDK and workspaces (GOPATH)','Andrew Gerrand','		The App Engine SDK and workspaces (GOPATH)\n		9 January 2013\n		\n  Introduction\n    When we released Go 1 we introduced the go tool and, with it, the concept of workspaces. Workspaces (specified by the GOPATH environment variable) are a convention for organizing code that simplifies fetching, building, and installing Go packages. If you\'re not familiar with workspaces, please read this article or watch this screencast before reading on.\n    Until recently, the tools in the App Engine SDK were not aware of workspaces. Without workspaces the \"go get\" command cannot function, and so app authors had to install and update their app dependencies manually. It was a pain.\n    This has all changed with version 1.7.4 of the App Engine SDK. The dev_appserver and appcfg tools are now workspace-aware. When running locally or uploading an app, these tools now search for dependencies in the workspaces specified by the GOPATH environment variable. This means you can now use \"go get\" while building App Engine apps, and switch between normal Go programs and App Engine apps without changing your environment or habits.\n    For example, let\'s say you want to build an app that uses OAuth 2.0 to authenticate with a remote service. A popular OAuth 2.0 library for Go is the oauth2 package, which you can install to your workspace with this command:\n  go get golang.org/x/oauth2\n    When writing your App Engine app, import the oauth package just as you would in a regular Go program:\n  import \"golang.org/x/oauth2\"\n    Now, whether running your app with the dev_appserver or deploying it with appcfg, the tools will find the oauth package in your workspace. It just works.\n  Hybrid stand-alone/App Engine apps\n    The Go App Engine SDK builds on Go\'s standard net/http package to serve web requests and, as a result, many Go web servers can be run on App Engine with only a few changes. For example, godoc is included in the Go distribution as a stand-alone program, but it can also run as an App Engine app (godoc serves golang.org from App Engine).\n    But wouldn\'t it be nice if you could write a program that is both a stand-alone web server and an App Engine app? By using build constraints, you can.\n    Build constraints are line comments that determine whether a file should be included in a package. They are most often used in code that handles a variety of operating systems or processor architectures. For instance, the path/filepath package includes the file symlink.go, which specifies a build constraint to ensure that it is not built on Windows systems (which do not have symbolic links):\n  // +build !windows\n    The App Engine SDK introduces a new build constraint term: \"appengine\". Files that specify\n  // +build appengine\n    will be built by the App Engine SDK and ignored by the go tool. Conversely, files that specify\n  // +build !appengine\n    are ignored by the App Engine SDK, while the go tool will happily build them.\n    The goprotobuf library uses this mechanism to provide two implementations of a key part of its encode/decode machinery: pointer_unsafe.go is the faster version that cannot be used on App Engine because it uses the unsafe package, while pointer_reflect.go is a slower version that avoids unsafe by using the reflect package instead.\n    Let\'s take a simple Go web server and turn it into a hybrid app. This is main.go:\n  package main\nimport (\n    \"fmt\"\n    \"net/http\"\n)\nfunc main() {\n    http.HandleFunc(\"/\", handler)\n    http.ListenAndServe(\"localhost:8080\", nil)\n}\nfunc handler(w http.ResponseWriter, r *http.Request) {\n    fmt.Fprint(w, \"Hello!\")\n}\n    Build this with the go tool and you\'ll get a stand-alone web server executable.\n    The App Engine infrastructure provides its own main function that runs its equivalent to ListenAndServe. To convert main.go to an App Engine app, drop the call to ListenAndServe and register the handler in an init function (which runs before main). This is app.go:\n  package main\nimport (\n    \"fmt\"\n    \"net/http\"\n)\nfunc init() {\n    http.HandleFunc(\"/\", handler)\n}\nfunc handler(w http.ResponseWriter, r *http.Request) {\n    fmt.Fprint(w, \"Hello!\")\n}\n    To make this a hybrid app, we need to split it into an App Engine-specific part, an stand-alone binary-specific part, and the parts common to both versions. In this case, there is no App Engine-specific part, so we split it into just two files:\n    app.go specifies and registers the handler function. It is identical to the code listing above, and requires no build constraints as it should be included in all versions of the program.\n    main.go runs the web server. It includes the \"!appengine\" build constraint, as it must only included when building the stand-alone binary.\n  // +build !appengine\npackage main\nimport \"net/http\"\nfunc main() {\n    http.ListenAndServe(\"localhost:8080\", nil)        \n}\n    To see a more complex hybrid app, take a look at the present tool.\n  Conclusions\n    We hope these changes will make it easier to work on apps with external dependencies, and to maintain code bases that contain both stand-alone programs and App Engine apps.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/the-app-engine-sdk-and-workspaces-gopath','2013-01-09','2017-05-07 13:50:01.525917'),(523,'Concurrency is not parallelism','Andrew Gerrand','		Concurrency is not parallelism\n		16 January 2013\n		\n    If there\'s one thing most people know about Go, is that it is designed for concurrency. No introduction to Go is complete without a demonstration of its goroutines and channels.\n    But when people hear the word concurrency they often think of parallelism, a related but quite distinct concept. In programming, concurrency is the composition of independently executing processes, while parallelism is the simultaneous execution of (possibly related) computations. Concurrency is about dealing with lots of things at once. Parallelism is about doing lots of things at once.\n    To clear up this conflation, Rob Pike gave a talk at Heroku\'s Waza conference entitled Concurrency is not parallelism, and a video recording of the talk was released a few months ago.\n    The slides are available at talks.golang.org (use the left and right arrow keys to navigate).\n    To learn about Go\'s concurrency primitives, watch Go concurrency patterns (slides).\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/concurrency-is-not-parallelism','2013-01-16','2017-05-07 13:50:01.565706'),(524,'Two recent Go articles','Andrew Gerrand','		Two recent Go articles\n		6 March 2013\n		\n  Introduction\n    In today\'s blog post I\'d like to highlight a couple of recent articles about Go.\n  Go at Google\n    In October last year, Rob Pike presented a keynote at the ACM SPLASH conference in Tucson. The talk, titled Go at Google, was a comprehensive discussion of the motivations behind Go. Rob later expanded on his talk to produce an essay titled Go at Google: Language Design in the Service of Software Engineering. Here is the abstract:\n  The Go programming language was conceived in late 2007 as an\nanswer to some of the problems we were seeing developing\nsoftware infrastructure at Google. The computing landscape\ntoday is almost unrelated to the environment in which the\nlanguages being used, mostly C++, Java, and Python, had been\ncreated. The problems introduced by multicore processors,\nnetworked systems, massive computation clusters, and the web\nprogramming model were being worked around rather than\naddressed head-on. Moreover, the scale has changed: today\'s\nserver programs comprise tens of millions of lines of code,\nare worked on by hundreds or even thousands of programmers,\nand are updated literally every day.  To make matters worse,\nbuild times, even on large compilation clusters, have\nstretched to many minutes, even hours.\nGo was designed and developed to make working in this\nenvironment more productive. Besides its better-known\naspects such as built-in concurrency and garbage collection,\nGo\'s design considerations include rigorous dependency\nmanagement, the adaptability of software architecture as\nsystems grow, and robustness across the boundaries between\ncomponents.\n    This article explains how these issues were addressed while building an efficient, compiled programming language that feels lightweight and pleasant. Examples and explanations will be taken from the real-world problems faced at Google.\n    If you have wondered about the design decisions behind Go, you may find your questions answered by the essay. It is recommended reading for both new and experienced Go programmers.\n  Go at the Google Developers Academy\n    At Google I/O 2012 the Google Developers team launched the Google Developers Academy, a program that provides training materials on Google technologies. Go is one of those technologies and we\'re pleased to announce the first GDA article featuring Go front and center:\n    Getting Started with Go, App Engine and Google+ API is an introduction to writing web applications in Go. It demonstrates how to build and deploy App Engine applications and make calls to the Google+ API using the Google APIs Go Client. This is a great entry point for Go programmers eager to get started with Google\'s developer ecosystem.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/two-recent-go-articles','2013-03-06','2017-05-07 13:50:01.604535'),(525,'The path to Go 1','Andrew Gerrand','		The path to Go 1\n		14 March 2013\n		\n    In July 2012, Rob Pike and I presented a talk at OSCON titled The path to Go 1. In it we explain how Go 1 came to be, and outline the process by which Go was refined and stabilized to become the clean, consistent programming environment that it is today. We present the major highlights of the release and discuss the details behind some specific libraries and tools.\n    The slides for the talk are available here.\n    It\'s almost a year since we cut Go 1.0 and we are now busy preparing Go 1.1. The release will include performance improvements to the gc compiler, garbage collector, and goroutine scheduler, some standard library additions, and many bug fixes and other improvements. Stay tuned, as we hope to release Go 1.1 in the coming weeks.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/the-path-to-go-1','2013-03-14','2017-05-07 13:50:01.629657'),(526,'Introducing the Go Playground','Andrew Gerrand','		Introducing the Go Playground\n		15 September 2010\n		\n    If you visit golang.org today you\'ll see our new look. We have given the site a new coat of paint and reorganized its content to make it easier to find. These changes are also reflected in the web interface of godoc, the Go documentation tool.\n    But the real news is a prominent new feature: the Go Playground.\n    The Playground allows anyone with a web browser to write Go code that we immediately compile, link, and run on our servers. There are a few example programs to get you started (see the \"Examples\" drop-down). We hope that this will give curious programmers an opportunity to try the language before installing it, and experienced Go users a convenient place in which to experiment. Beyond the front page, this functionality has the potential to make our reference and tutorial materials more engaging. We hope to extend its use in the near future.\n    Of course, there are some limitations to the kinds of programs you can run in the Playground. We can\'t simply accept arbitrary code and run it on our servers without restrictions. The programs build and run in a sandbox with a reduced standard library; the only communication your program has to the outside world is via standard output, and there are limits to CPU and memory use. As such, consider this just a taste of the wonderful world of Go; to have the full experience you\'ll need to download it yourself.\n    If you\'ve been meaning to try Go but never got around to it, why not visit golang.org to try it right now?\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/introducing-go-playground','2010-09-15','2017-05-07 13:50:01.743710'),(527,'Learn Go from your browser','Andrew Gerrand','		Learn Go from your browser\n		4 October 2011\n		\n    We are excited to announce A Tour of Go, a guided tour of the Go programming language you can run from your browser.\n    The tour is hands-on, demonstrating the language through code samples that you can modify, compile, and run from the tour itself. (The technology behind the Go Playground does the work.)\n    The tour has three sections. The first section covers basic syntax and data structures; the second discusses methods and interfaces; and the third introduces Go\'s concurrency primitives. Each section concludes with a few exercises so you can practice what you\'ve learned.\n    So, what are you waiting for? Get started now!\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/learn-go-from-your-browser','2011-10-04','2017-05-07 13:50:01.792746'),(528,'Spotlight on external Go libraries','Andrew Gerrand','		Spotlight on external Go libraries\n		3 June 2011\n		\n    While the Go authors have been working hard at improving Go\'s standard library, the greater community has created a growing ecosystem of external libraries. In this post we look at some popular Go libraries and how they can be used.\n    Mgo (pronounced \"mango\") is a MongoDB database driver. MongoDB is a document-oriented database with a long list of features suitable for a broad range of uses. The mgo package provides a rich, idiomatic Go API for working with MongoDB, from basic operations such as inserting and updating records to the more advanced MapReduce and GridFS features. Mgo has a bunch of cool features including automated cluster discovery and result pre-fetching - see the mgo homepage for details and example code. For working with large data sets Go, MongoDB, and mgo are a powerful combination.\n    Authcookie is a web library for generating and verifying user authentication cookies. It allows web servers to hand out cryptographically secure tokens tied to a specific user that will expire after a specified time period. It has a simple API that makes it straightforward to add authentication to existing web applications. See the README file for details and example code.\n    Go-charset provides support for converting between Go\'s standard UTF-8 encoding and a variety of character sets. The go-charset package implements a translating io.Reader and io.Writer so you can wrap existing Readers and Writers (such as network connections or file descriptors), making it easy to communicate with systems that use other character encodings.\n    Go-socket.io is a Go implementation of Socket.IO, a client/server API that allows web servers to push messages to web browsers. Depending on the capabilities of the user\'s browser, Socket.IO uses the best transport for the connection, be it modern websockets, AJAX long polling, or some other mechanism. Go-socket.io bridges the gap between Go servers and rich JavaScript clients for a wide range of browsers. To get a feel for go-socket.io see the chat server example.\n    It\'s worth mentioning that these packages are goinstallable. With an up-to-date Go installation you can install them all with a single command:\n  goinstall launchpad.net/mgo \\\n    github.com/dchest/authcookie \\\n    go-charset.googlecode.com/hg/charset \\\n    github.com/madari/go-socket.io\n    Once goinstalled, the packages can be imported using those same paths:\n  import (\n    \"launchpad.net/mgo\"\n    \"github.com/dchest/authcookie\"\n    \"go-charset.googlecode.com/hg/charset\"\n    \"github.com/madari/go-socket.io\"\n)\n    Also, as they are now a part of the local Go system, we can inspect their documentation with godoc:\n  godoc launchpad.net/mgo Database # see docs for Database type\n    Of course, this is just the tip of the iceberg; there are more great Go libraries listed on the package dashboard and many more to come.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/spotlight-on-external-go-libraries','2011-06-03','2017-05-07 13:50:01.830097'),(529,'Getting to know the Go community','Andrew Gerrand','		Getting to know the Go community\n		21 December 2011\n		\n    Over the past couple of years Go has attracted a lot of users and contributors, and I\'ve had a great time meeting and talking with many of you. However, for every Gopher I know there are dozens I know nothing about. In order to address this imbalance I\'ve prepared a survey for Go users everywhere.\n    The survey is short. It asks about you, your involvement with Go, and and your interest in Go-related events. Among other things, this data will help myself and the rest of the Go team plan future Go events and schedule conference appearances.\n    Please take a minute to complete the survey now.\n    Thanks!\n    Andrew\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/getting-to-know-go-community','2011-12-21','2017-05-07 13:50:01.883968'),(530,'Go turns three','Russ Cox','		Go turns three\n		10 November 2012\n		\n    The Go open source project is\n    three years old today.\n    It\'s great to look at how far Go has come in those three years.\n    When we launched, Go was an idea backed by two implementations that worked on Linux and OS X.\n    The syntax, semantics, and libraries changed regularly as we reacted to feedback from users\n    and experience with the language.\n    Since the open source launch,\n    we\'ve been joined by\n    hundreds of external contributors,\n    who have extended and improved Go in myriad ways,\n    including writing a Windows port from scratch.\n    We added a package management system\n    goinstall,\n    which eventually became the\n    go command.\n    We also added\n    support for Go on App Engine.\n    Over the past year we\'ve also given many talks, created an interactive introductory tour\n    and recently we added support for executable examples in package documentation.\n    Perhaps the most important development in the past year\n    was the launch of the first stable version,\n    Go 1.\n    People who write Go 1 programs can now be confident that their programs will\n    continue to compile and run without change, in many environments,\n    on a time scale of years.\n    As part of the Go 1 launch we spent months cleaning up the\n    language and libraries\n    to make it something that will age well.\n    We\'re working now toward the release of Go 1.1 in 2013. There will be some\n    new functionality, but that release will focus primarily on making Go perform\n    even better than it does today.\n    We\'re especially happy about the community that has grown around Go:\n    the mailing list and IRC channels seem like they are overflowing with discussion,\n    and a handful of Go books were published this year. The community is thriving.\n    Use of Go in production environments has also taken off, especially since Go 1.\n    We use Go at Google in a variety of ways, many of them invisible to the outside world.\n    A few visible ones include\n    serving Chrome and other downloads,\n    scaling MySQL database at YouTube,\n    and of course running the\n    Go home page\n    on App Engine.\n    Last year\'s\n    Thanksgiving Doodle\n    and the recent\n    Jam with Chrome\n    site are also served by Go programs.\n    Other companies and projects are using Go too, including\n    BBC Worldwide,\n    Canonical,\n    CloudFlare,\n    Heroku,\n    Novartis,\n    SoundCloud,\n    SmugMug,\n    StatHat,\n    Tinkercad,\n    and\n    many others.\n    Here\'s to many more years of productive programming in Go.\n		\n			By Russ Cox\n		\n	','https://blog.golang.org/go-turns-three','2012-11-10','2017-05-07 13:50:01.902409'),(531,'Go maps in action','Andrew Gerrand','		Go maps in action\n		6 February 2013\n		\n  Introduction\n    One of the most useful data structures in computer science is the hash table. Many hash table implementations exist with varying properties, but in general they offer fast lookups, adds, and deletes. Go provides a built-in map type that implements a hash table.\n  Declaration and initialization\n    A Go map type looks like this:\n  map[KeyType]ValueType\n    where KeyType may be any type that is comparable (more on this later), and ValueType may be any type at all, including another map!\n    This variable m is a map of string keys to int values:\n  var m map[string]int\n    Map types are reference types, like pointers or slices, and so the value of m above is nil; it doesn\'t point to an initialized map. A nil map behaves like an empty map when reading, but attempts to write to a nil map will cause a runtime panic; don\'t do that. To initialize a map, use the built in make function:\n  m = make(map[string]int)\n    The make function allocates and initializes a hash map data structure and returns a map value that points to it. The specifics of that data structure are an implementation detail of the runtime and are not specified by the language itself. In this article we will focus on the use of maps, not their implementation.\n  Working with maps\n    Go provides a familiar syntax for working with maps. This statement sets the key \"route\" to the value 66:\n  m[\"route\"] = 66\n    This statement retrieves the value stored under the key \"route\" and assigns it to a new variable i:\n  i := m[\"route\"]\n    If the requested key doesn\'t exist, we get the value type\'s zero value. In this case the value type is int, so the zero value is 0:\n  j := m[\"root\"]\n// j == 0\n    The built in len function returns on the number of items in a map:\n  n := len(m)\n    The built in delete function removes an entry from the map:\n  delete(m, \"route\")\n    The delete function doesn\'t return anything, and will do nothing if the specified key doesn\'t exist.\n    A two-value assignment tests for the existence of a key:\n  i, ok := m[\"route\"]\n    In this statement, the first value (i) is assigned the value stored under the key \"route\". If that key doesn\'t exist, i is the value type\'s zero value (0). The second value (ok) is a bool that is true if the key exists in the map, and false if not.\n    To test for a key without retrieving the value, use an underscore in place of the first value:\n  _, ok := m[\"route\"]\n    To iterate over the contents of a map, use the range keyword:\n  for key, value := range m {\n    fmt.Println(\"Key:\", key, \"Value:\", value)\n}\n    To initialize a map with some data, use a map literal:\n  commits := map[string]int{\n    \"rsc\": 3711,\n    \"r\":   2138,\n    \"gri\": 1908,\n    \"adg\": 912,\n}\n    The same syntax may be used to initialize an empty map, which is functionally identical to using the make function:\n  m = map[string]int{}\n  Exploiting zero values\n    It can be convenient that a map retrieval yields a zero value when the key is not present.\n    For instance, a map of boolean values can be used as a set-like data structure (recall that the zero value for the boolean type is false). This example traverses a linked list of Nodes and prints their values. It uses a map of Node pointers to detect cycles in the list.\n	\n    type Node struct {\n        Next  *Node\n        Value interface{}\n    }\n    var first *Node\n    visited := make(map[*Node]bool)\n    for n := first; n != nil; n = n.Next {\n        if visited[n] {\n            fmt.Println(\"cycle detected\")\n            break\n        }\n        visited[n] = true\n        fmt.Println(n.Value)\n    }\n    The expression visited[n] is true if n has been visited, or false if n is not present. There\'s no need to use the two-value form to test for the presence of n in the map; the zero value default does it for us.\n    Another instance of helpful zero values is a map of slices. Appending to a nil slice just allocates a new slice, so it\'s a one-liner to append a value to a map of slices; there\'s no need to check if the key exists. In the following example, the slice people is populated with Person values. Each Person has a Name and a slice of Likes. The example creates a map to associate each like with a slice of people that like it.\n	\n    type Person struct {\n        Name  string\n        Likes []string\n    }\n    var people []*Person\n    likes := make(map[string][]*Person)\n    for _, p := range people {\n        for _, l := range p.Likes {\n            likes[l] = append(likes[l], p)\n        }\n    }\n    To print a list of people who like cheese:\n	\n    for _, p := range likes[\"cheese\"] {\n        fmt.Println(p.Name, \"likes cheese.\")\n    }\n    To print the number of people who like bacon:\n	\n    fmt.Println(len(likes[\"bacon\"]), \"people like bacon.\")\n    Note that since both range and len treat a nil slice as a zero-length slice, these last two examples will work even if nobody likes cheese or bacon (however unlikely that may be).\n  Key types\n    As mentioned earlier, map keys may be of any type that is comparable. The language spec defines this precisely, but in short, comparable types are boolean, numeric, string, pointer, channel, and interface types, and structs or arrays that contain only those types. Notably absent from the list are slices, maps, and functions; these types cannot be compared using ==, and may not be used as map keys.\n    It\'s obvious that strings, ints, and other basic types should be available as map keys, but perhaps unexpected are struct keys. Struct can be used to key data by multiple dimensions. For example, this map of maps could be used to tally web page hits by country:\n  hits := make(map[string]map[string]int)\n    This is map of string to (map of string to int). Each key of the outer map is the path to a web page with its own inner map. Each inner map key is a two-letter country code. This expression retrieves the number of times an Australian has loaded the documentation page:\n  n := hits[\"/doc/\"][\"au\"]\n    Unfortunately, this approach becomes unwieldy when adding data, as for any given outer key you must check if the inner map exists, and create it if needed:\n  func add(m map[string]map[string]int, path, country string) {\n    mm, ok := m[path]\n    if !ok {\n        mm = make(map[string]int)\n        m[path] = mm\n    }\n    mm[country]++\n}\nadd(hits, \"/doc/\", \"au\")\n    On the other hand, a design that uses a single map with a struct key does away with all that complexity:\n  type Key struct {\n    Path, Country string\n}\nhits := make(map[Key]int)\n    When an Vietnamese person visits the home page, incrementing (and possibly creating) the appropriate counter is a one-liner:\n  hits[Key{\"/\", \"vn\"}]++\n    And it\'s similarly straightforward to see how many Swiss people have read the spec:\n  n := hits[Key{\"/ref/spec\", \"ch\"}]\n  Concurrency\n    Maps are not safe for concurrent use: it\'s not defined what happens when you read and write to them simultaneously. If you need to read from and write to a map from concurrently executing goroutines, the accesses must be mediated by some kind of synchronization mechanism. One common way to protect maps is with sync.RWMutex.\n    This statement declares a counter variable that is an anonymous struct containing a map and an embedded sync.RWMutex.\n  var counter = struct{\n    sync.RWMutex\n    m map[string]int\n}{m: make(map[string]int)}\n    To read from the counter, take the read lock:\n  counter.RLock()\nn := counter.m[\"some_key\"]\ncounter.RUnlock()\nfmt.Println(\"some_key:\", n)\n    To write to the counter, take the write lock:\n  counter.Lock()\ncounter.m[\"some_key\"]++\ncounter.Unlock()\n  Iteration order\n    When iterating over a map with a range loop, the iteration order is not specified and is not guaranteed to be the same from one iteration to the next. Since Go 1 the runtime randomizes map iteration order, as programmers relied on the stable iteration order of the previous implementation. If you require a stable iteration order you must maintain a separate data structure that specifies that order. This example uses a separate sorted slice of keys to print a map[int]string in key order:\n  import \"sort\"\nvar m map[int]string\nvar keys []int\nfor k := range m {\n    keys = append(keys, k)\n}\nsort.Ints(keys)\nfor _, k := range keys {\n    fmt.Println(\"Key:\", k, \"Value:\", m[k])\n}\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go-maps-in-action','2013-02-06','2017-05-07 13:50:02.023044'),(532,'Generating code','Rob Pike','		Generating code\n		22 December 2014\n		\n    A property of universal computation—Turing completeness—is that a computer program can write a computer program.\n    This is a powerful idea that is not appreciated as often as it might be, even though it happens frequently.\n    It\'s a big part of the definition of a compiler, for instance.\n    It\'s also how the go test command works: it scans the packages to be tested,\n    writes out a Go program containing a test harness customized for the package,\n    and then compiles and runs it.\n    Modern computers are so fast this expensive-sounding sequence can complete in a fraction of a second.\n    There are lots of other examples of programs that write programs.\n    Yacc, for instance, reads in a description of a grammar and writes out a program to parse that grammar.\n    The protocol buffer \"compiler\" reads an interface description and emits structure definitions,\n    methods, and other support code.\n    Configuration tools of all sorts work like this too, examining metadata or the environment\n    and emitting scaffolding customized to the local state.\n    Programs that write programs are therefore important elements in software engineering,\n    but programs like Yacc that produce source code need to be integrated into the build\n    process so their output can be compiled.\n    When an external build tool like Make is being used, this is usually easy to do.\n    But in Go, whose go tool gets all necessary build information from the Go source, there is a problem.\n    There is simply no mechanism to run Yacc from the go tool alone.\n    Until now, that is.\n    The latest Go release, 1.4,\n    includes a new command that makes it easier to run such tools.\n    It\'s called go generate, and it works by scanning for special comments in Go source code\n    that identify general commands to run.\n    It\'s important to understand that go generate is not part of go build.\n    It contains no dependency analysis and must be run explicitly before running go build.\n    It is intended to be used by the author of the Go package, not its clients.\n    The go generate command is easy to use.\n    As a warmup, here\'s how to use it to generate a Yacc grammar.\n    Say you have a Yacc input file called gopher.y that defines a grammar for your new language.\n    To produce the Go source file implementing the grammar,\n    you would normally invoke the standard Go version of Yacc like this:\n  go tool yacc -o gopher.go -p parser gopher.y\n    The -o option names the output file while -p specifies the package name.\n    To have go generate drive the process, in any one of the regular (non-generated) .go files\n    in the same directory, add this comment anywhere in the file:\n  //go:generate go tool yacc -o gopher.go -p parser gopher.y\n    This text is just the command above prefixed by a special comment recognized by go generate.\n    The comment must start at the beginning of the line and have no spaces between the // and the go:generate.\n    After that marker, the rest of the line specifies a command for go generate to run.\n    Now run it. Change to the source directory and run go generate, then go build and so on:\n  $ cd $GOPATH/myrepo/gopher\n$ go generate\n$ go build\n$ go test\n    That\'s it.\n    Assuming there are no errors, the go generate command will invoke yacc to create gopher.go,\n    at which point the directory holds the full set of Go source files, so we can build, test, and work normally.\n    Every time gopher.y is modified, just rerun go generate to regenerate the parser.\n    For more details about how go generate works, including options, environment variables,\n    and so on, see the design document.\n    Go generate does nothing that couldn\'t be done with Make or some other build mechanism,\n    but it comes with the go tool—no extra installation required—and fits nicely into the Go ecosystem.\n    Just keep in mind that it is for package authors, not clients,\n    if only for the reason that the program it invokes might not be available on the target machine.\n    Also, if the containing package is intended for import by go get,\n    once the file is generated (and tested!) it must be checked into the\n    source code repository to be available to clients.\n    Now that we have it, let\'s use it for something new.\n    As a very different example of how go generate can help, there is a new program available in the\n    golang.org/x/tools repository called stringer.\n    It automatically writes string methods for sets of integer constants.\n    It\'s not part of the released distribution, but it\'s easy to install:\n  $ go get golang.org/x/tools/cmd/stringer\n    Here\'s an example from the documentation for\n    stringer.\n    Imagine we have some code that contains a set of integer constants defining different types of pills:\n  package painkiller\ntype Pill int\nconst (\n    Placebo Pill = iota\n    Aspirin\n    Ibuprofen\n    Paracetamol\n    Acetaminophen = Paracetamol\n)\n    For debugging, we\'d like these constants to pretty-print themselves, which means we want a method with signature,\n  func (p Pill) String() string\n    It\'s easy to write one by hand, perhaps like this:\n  func (p Pill) String() string {\n    switch p {\n    case Placebo:\n        return \"Placebo\"\n    case Aspirin:\n        return \"Aspirin\"\n    case Ibuprofen:\n        return \"Ibuprofen\"\n    case Paracetamol: // == Acetaminophen\n        return \"Paracetamol\"\n    }\n    return fmt.Sprintf(\"Pill(%d)\", p)\n}\n    There are other ways to write this function, of course.\n    We could use a slice of strings indexed by Pill, or a map, or some other technique.\n    Whatever we do, we need to maintain it if we change the set of pills, and we need to make sure it\'s correct.\n    (The two names for paracetamol make this trickier than it might otherwise be.)\n    Plus the very question of which approach to take depends on the types and values:\n    signed or unsigned, dense or sparse, zero-based or not, and so on.\n    The stringer program takes care of all these details.\n    Although it can be run in isolation, it is intended to be driven by go generate.\n    To use it, add a generate comment to the source, perhaps near the type definition:\n  //go:generate stringer -type=Pill\n    This rule specifies that go generate should run the stringer tool to generate a String method for type Pill.\n    The output is automatically written to pill_string.go (a default we could override with the\n    -output flag).\n    Let\'s run it:\n  $ go generate\n$ cat pill_string.go\n// generated by stringer -type Pill pill.go; DO NOT EDIT\npackage pill\nimport \"fmt\"\nconst _Pill_name = \"PlaceboAspirinIbuprofenParacetamol\"\nvar _Pill_index = [...]uint8{0, 7, 14, 23, 34}\nfunc (i Pill) String() string {\n    if i &lt; 0 || i+1 &gt;= Pill(len(_Pill_index)) {\n        return fmt.Sprintf(\"Pill(%d)\", i)\n    }\n    return _Pill_name[_Pill_index[i]:_Pill_index[i+1]]\n}\n$\n    Every time we change the definition of Pill or the constants, all we need to do is run\n  $ go generate\n    to update the String method.\n    And of course if we\'ve got multiple types set up this way in the same package,\n    that single command will update all their String methods with a single command.\n    There\'s no question the generated method is ugly.\n    That\'s OK, though, because humans don\'t need to work on it; machine-generated code is often ugly.\n    It\'s working hard to be efficient.\n    All the names are smashed together into a single string,\n    which saves memory (only one string header for all the names, even if there are zillions of them).\n    Then an array, _Pill_index, maps from value to name by a simple, efficient technique.\n    Note too that _Pill_index is an array (not a slice; one more header eliminated) of uint8,\n    the smallest integer sufficient to span the space of values.\n    If there were more values, or there were negatives ones,\n    the generated type of _Pill_index might change to uint16 or int8: whatever works best.\n    The approach used by the methods printed by stringer varies according to the properties of the constant set.\n    For instance, if the constants are sparse, it might use a map.\n    Here\'s a trivial example based on a constant set representing powers of two:\n  const _Power_name = \"p0p1p2p3p4p5...\"\nvar _Power_map = map[Power]string{\n    1:    _Power_name[0:2],\n    2:    _Power_name[2:4],\n    4:    _Power_name[4:6],\n    8:    _Power_name[6:8],\n    16:   _Power_name[8:10],\n    32:   _Power_name[10:12],\n    ...,\n}\nfunc (i Power) String() string {\n    if str, ok := _Power_map[i]; ok {\n        return str\n    }\n    return fmt.Sprintf(\"Power(%d)\", i)\n}\n    In short, generating the method automatically allows us to do a better job than we would expect a human to do.\n    There are lots of other uses of go generate already installed in the Go tree.\n    Examples include generating Unicode tables in the unicode package,\n    creating efficient methods for encoding and decoding arrays in encoding/gob,\n    producing time zone data in the time package, and so on.\n    Please use go generate creatively.\n    It\'s there to encourage experimentation.\n    And even if you don\'t, use the new stringer tool to write your String methods for your integer constants.\n    Let the machine do the work.\n		\n			By Rob Pike\n		\n	','https://blog.golang.org/generate','2014-12-22','2017-05-07 13:50:02.058006'),(533,'Go updates in App Engine 1.7.1','Andrew Gerrand','		Go updates in App Engine 1.7.1\n		22 August 2012\n		\n    This week we released version 1.7.1 of the App Engine SDK. It includes some significant updates specific to the App Engine runtime for Go.\n    The memcache package has had some additions to its Codec convenience type. The SetMulti, AddMulti, CompareAndSwap, and CompareAndSwapMulti methods make it easier to store and update encoded data in the Memcache Service.\n    The bulkloader tool can now be used with Go apps, allowing users to upload and download datastore records in bulk. This is useful for backups and offline processing, and a great help when migrating Python or Java apps to the Go runtime.\n    The Images Service is now available to Go users. The new appengine/image package supports serving images directly from Blobstore and resizing or cropping those images on the fly. Note that this is not the full image service as provided by the Python and Java SDKs, as much of the equivalent functionality is available in the standard Go image package and external packages such as graphics-go.\n    The new runtime.RunInBackground function allows backend requests to spawn a new request independent of the initial request. These can run in the background as long as the backend stays alive.\n    Finally, we have filled in some missing functionality: the xmpp package now supports sending presence updates and chat invitations and retrieving the presence state of another user, and the user package supports authenticating clients with OAuth.\n    You can grab the new SDK from the App Engine downloads page and browse the updated documentation.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go-updates-in-app-engine-171','2012-08-22','2017-05-07 13:50:02.246535'),(534,'Writing scalable App Engine applications','David Symonds','		Writing scalable App Engine applications\n		1 November 2011\n		\n    Back in May, we announced the Go runtime for App Engine. Since then, we\'ve opened it up for everyone to use, added many new APIs, and improved performance. We have been thrilled by all the interesting ways that people are using Go on App Engine.\n    One of the key benefits of the Go runtime, apart from working in a fantastic language, is that it has high performance. Go applications compile to native code, with no interpreter or virtual machine getting between your program and the machine.\n    Making your web application fast is important because it is well known that a web site\'s latency has a measurable impact on user happiness, and Google web search uses it as a ranking factor. Also announced in May was that App Engine would be leaving its Preview status and transitioning to a new pricing model, providing another reason to write efficient App Engine applications.\n    To make it easier for Go developers using App Engine to write highly efficient, scalable applications, we recently updated some existing App Engine articles to include snippets of Go source code and to link to relevant Go documentation.\n    Best practices for writing scalable applications\n    Managing Your App\'s Resource Usage\n		\n			By David Symonds\n		\n	','https://blog.golang.org/writing-scalable-app-engine','2011-11-01','2017-05-07 13:50:02.342480'),(535,'Go: one year ago today','Andrew Gerrand','		Go: one year ago today\n		10 November 2010\n		\n    On the 10th of November 2009 we launched the Go project: an open-source programming language with a focus on simplicity and efficiency. The intervening year has seen a great many developments both in the Go project itself and in its community.\n    We set out to build a language for systems programming - the kinds of programs one might typically write in C or C++ - and we were surprised by Go’s utility as a general purpose language. We had anticipated interest from C, C++, and Java programmers, but the flurry of interest from users of dynamically-typed languages like Python and JavaScript was unexpected.  Go’s combination of native compilation, static typing, memory management, and lightweight syntax seemed to strike a chord with a broad cross-section of the programming community.\n    That cross-section grew to become a dedicated community of enthusiastic Go coders. Our mailing list has over 3,800 members, with around 1,500 posts each month. The project has over 130 contributors (people who have submitted code or documentation), and of the 2,800 commits since launch almost one third were contributed by programmers outside the core team. To get all that code into shape, nearly 14,000 emails were exchanged on our development mailing list.\n    Those numbers reflect a labor whose fruits are evident in the project’s code base. The compilers have improved substantially, with faster and more efficient code generation, more than one hundred reported bugs fixed, and support for a widening range of operating systems and architectures. The Windows port is approaching completion thanks to a dedicated group of contributors (one of whom became our first non-Google committer to the project). The ARM port has also made great progress, recently reaching the milestone of passing all tests.\n    The Go tool set has been expanded and improved. The Go documentation tool, godoc, now supports the documentation of other source trees (you can browse and search your own code) and provides a \"code walk\" interface for presenting tutorial materials (among many more improvements). Goinstall , a new package management tool, allows users to install and update external packages with a single command. Gofmt, the Go pretty-printer, now makes syntactic simplifications where possible. Goplay, a web-based “compile-as-you-type” tool, is a convenient way to experiment with Go for those times when you don’t have access to the Go Playground.\n    The standard library has grown by over 42,000 lines of code and includes 20 new packages.  Among the additions are the jpeg, jsonrpc, mime, netchan, and smtp packages, as well as a slew of new cryptography packages. More generally, the standard library has been continuously refined and revised as our understanding of Go’s idioms deepens.\n    The debugging story has gotten better, too. Recent improvements to the DWARF output of the gc compilers make the GNU debugger, GDB, useful for Go binaries, and we’re actively working on making that debugging information more complete. (See the  recent blog post for details.) \n    It’s now easier than ever to link against existing libraries written in languages other than Go.  Go support is in the most recent SWIG release, version 2.0.1, making it easier to link against C and C++ code, and our cgo tool has seen many fixes and improvements.\n    Gccgo, the Go front end for the GNU C Compiler, has kept pace with the gc compiler as a parallel Go implementation. It now has a working garbage collector, and has been accepted into the GCC core.  We’re now working toward making gofrontend available as a BSD-licensed Go compiler front end, fully decoupled from GCC.\n    Outside the Go project itself Go is starting to be used to build real software. There are more than 200 Go programs and libraries listed on our Project dashboard, and hundreds more on Google Code and Github. On our mailing list and IRC channel you can find coders from around the world who use Go for their programming projects. (See our guest blog post from last month for a real-world example.) Internally at Google there are several teams that choose Go for building production software, and we have received reports from other companies that are developing sizable systems in Go. We have also been in touch with several educators who are using Go as a teaching language.\n    The language itself has grown and matured, too. In the past year we have received many feature requests. But Go is a small language, and we’ve worked hard to ensure that any new feature strikes the right compromise between simplicity and utility. Since the launch we have made a number of language changes, many of which were driven by feedback from the community.\n    Semicolons are now optional in almost all instances. spec\n    The new built-in functions copy and append make management of slices more efficient and straightforward. spec\n    The upper and lower bounds may be omitted when making a sub-slice. This means that s[:] is shorthand for s[0:len(s)]. spec\n    The new built-in function recover complements panic and defer as an error handling mechanism.  blog, spec\n    The new complex number types (complex, complex64, and complex128) simplify certain mathematical operations. spec, spec\n    The composite literal syntax permits the omission of redundant type information (when specifying two-dimensional arrays, for example). release.2010-10-27, spec\n    A general syntax for variable function arguments (...T) and their propagation (v...) is now specified. spec,  spec, release.2010-09-29\n    Go is certainly ready for production use, but there is still room for improvement. Our focus for the immediate future is making Go programs faster and more efficient in the context of high performance systems. This means improving the garbage collector, optimizing generated code, and improving the core libraries. We’re also exploring some further additions to the type system to make generic programming easier. A lot has happened in a year; it’s been both thrilling and satisfying.  We hope that this coming year will be even more fruitful than the last.\n    If you’ve been meaning to get [back] into Go, now is a great time to do so! Check out the Documentation and Getting Started pages for more information, or just go nuts in the Go Playground.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go-one-year-ago-today','2010-11-10','2017-05-07 13:50:02.447827'),(536,'\"First Class Functions in Go\"','Andrew Gerrand','		\"First Class Functions in Go\"\n		30 June 2011\n		\n    Programmers new to Go are often surprised by its support for function types, functions as values, and closures. The First Class Functions in Go code walk demonstrates these features with a simulation of the dice game Pig. It is a pretty program that uses the language to great effect, and a fun read for Go beginners and veterans alike.\n    More resources are available at golang.org.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/first-class-functions-in-go-and-new-go','2011-06-30','2017-05-07 13:50:02.490524'),(537,'Debugging Go programs with the GNU Debugger','Andrew Gerrand','		Debugging Go programs with the GNU Debugger\n		30 October 2011\n		\n    Last year we reported that Go\'s gc/ld toolchain produces DWARFv3 debugging information that can be read by the GNU Debugger (GDB). Since then, work has continued steadily on improving support for debugging Go code with GDB.\n    Among the improvements are the ability to inspect goroutines and to print native Go data types, including structs, slices, strings, maps, interfaces, and channels.\n    To learn more about Go and GDB, see the Debugging with GDB article.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/debugging-go-programs-with-gnu-debugger','2011-10-30','2017-05-07 13:50:02.586963'),(538,'The Go Programming Language turns two','Andrew Gerrand','		The Go Programming Language turns two\n		10 November 2011\n		\n    Two years ago a small team at Google went public with their fledgling project - the Go Programming Language. They presented a language spec, two compilers, a modest standard library, some novel tools, and plenty of accurate (albeit succinct) documentation. They watched with excitement as programmers around the world began to play with Go. The team continued to iterate and improve on what they had built, and were gradually joined by dozens - and then hundreds - of programmers from the open source community.\n    The Go Authors went on to produce lots of libraries, new tools, and reams of documentation. They celebrated a successful year in the public eye with a blog post last November that concluded \"Go is certainly ready for production use, but there is still room for improvement. Our focus for the immediate future is making Go programs faster and more efficient in the context of high performance systems.\"\n    Today is the second anniversary of Go\'s release, and Go is faster and more stable than ever. Careful tuning of Go\'s code generators, concurrency primitives, garbage collector, and core libraries have increased the performance of Go programs, and native support for profiling and debugging makes it easier to detect and remove performance issues in user code. Go is also now easier to learn with A Tour of Go, an interactive tutorial you can take from the comfort of your web browser.\n    This year we introduced the experimental Go runtime for Google\'s App Engine platform, and we have been steadily increasing the Go runtime\'s support for App Engine\'s APIs. Just this week we released version 1.6.0 of the Go App Engine SDK, which includes support for backends (long-running processes), finer control over datastore indexes, and various other improvements. Today, the Go runtime is near feature parity with - and is a viable alternative to - the Python and Java runtimes. In fact, we now serve golang.org by running a version of godoc on the App Engine service.\n    While 2010 was a year of discovery and experimentation, 2011 was a year of fine tuning and planning for the future. This year we issued several \"release\" versions of Go that were more reliable and better supported than weekly snapshots. We also introduced gofix to take the pain out of migrating to newer releases. Furthermore, last month we announced a plan for Go version 1 - a release that will be supported for years to come. Work toward Go 1 is already underway and you can observe our progress by the latest weekly snapshot at weekly.golang.org.\n    The plan is to launch Go 1 in early 2012. We hope to bring the Go App Engine runtime out of \"experimental\" status at the same time.\n    But that\'s not all. 2011 was an exciting year for the gopher, too. He has manifested himself as a plush toy (a highly prized gift at Google I/O and other Go talks) and in vinyl form (received by every attendee at OSCON and now available at the Google Store).\n    And, most surprisingly, at Halloween he made an appearance with his gopher girlfriend!\n    Photograph by Chris Nokleberg.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go-programming-language-turns-two','2011-11-10','2017-05-07 13:50:02.660170'),(539,'Get thee to a Go meetup','Andrew Gerrand','		Get thee to a Go meetup\n		27 February 2013\n		\n    Last week, David Symonds and I each gave talks at Sydney\'s Go meetup, golang-syd. Dave spoke about Go API design in the context of Protocol Buffers, and I discussed some neat tricks in the construction of a small command-line program. The presentations were short but provoked some good questions and interesting discussion. Of the 50-odd attendees, most were regular Go programmers and a fair chunk write Go code professionally. It was a fun night.\n    It would have been great to see you there but, statistically, you\'re not from Sydney. Despair not, however, as there are likely some people in your area who either run a Go meetup or want to start one.\n    The Go wiki lists Go user groups around the world, so if there\'s one nearby you should consider going along to the next event.\n    If not, why not start your own? To gauge interest, ask around in the Go+ Community and the golang-nuts mailing list, and take a look at this list of people waiting for a Go meetup in their area. Once you have a few people interested - and at least one person willing to present something - pick a venue and set a date. If you build it, they will come.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/getthee-to-go-meetup','2013-02-27','2017-05-07 13:50:02.702225'),(540,'Go version 1 is released','Andrew Gerrand','		Go version 1 is released\n		28 March 2012\n		\n    Today marks a major milestone in the development of the Go programming language. We\'re announcing Go version 1, or Go 1 for short, which defines a language and a set of core libraries to provide a stable foundation for creating reliable products, projects, and publications.\n    Go 1 is the first release of Go that is available in supported binary distributions. They are available for Linux, FreeBSD, Mac OS X and, we are thrilled to announce, Windows.\n    The driving motivation for Go 1 is stability for its users. People who write Go 1 programs can be confident that those programs will continue to compile and run without change, in many environments, on a time scale of years. Similarly, authors who write books about Go 1 can be sure that their examples and explanations will be helpful to readers today and into the future.\n    Forward compatibility is part of stability. Code that compiles in Go 1 should, with few exceptions, continue to compile and run throughout the lifetime of that version, even as we issue updates and bug fixes such as Go version 1.1, 1.2, and so on. The Go 1 compatibility document explains the compatibility guidelines in more detail.\n    Go 1 is a representation of Go as it is used today, not a major redesign. In its planning, we focused on cleaning up problems and inconsistencies and improving portability. There had long been many changes to Go that we had designed and prototyped but not released because they were backwards-incompatible. Go 1 incorporates these changes, which provide significant improvements to the language and libraries but sometimes introduce incompatibilities for old programs. Fortunately, the go fix tool can automate much of the work needed to bring programs up to the Go 1 standard.\n    Go 1 introduces changes to the language (such as new types for Unicode characters and errors) and the standard library (such as the new time package and renamings in the strconv package). Also, the package hierarchy has been rearranged to group related items together, such as moving the networking facilities, for instance the rpc package, into subdirectories of net. A complete list of changes is documented in the Go 1 release notes. That document is an essential reference for programmers migrating code from earlier versions of Go.\n    We also restructured the Go tool suite around the new go command, a program for fetching, building, installing and maintaining Go code. The go command eliminates the need for Makefiles to write Go code because it uses the Go program source itself to derive the build instructions. No more build scripts! \n    Finally, the release of Go 1 triggers a new release of the Google App Engine SDK. A similar process of revision and stabilization has been applied to the App Engine libraries, providing a base for developers to build programs for App Engine that will run for years.\n    Go 1 is the result of a major effort by the core Go team and our many contributors from the open source community. We thank everyone who helped make this happen.\n    There has never been a better time to be a Go programmer. Everything you need to get started is at golang.org.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go-version-1-is-released','2012-03-28','2017-05-07 13:50:02.734767'),(541,'Gccgo in GCC 4.7.1','Ian Lance Taylor','		Gccgo in GCC 4.7.1\n		11 July 2012\n		\n    The Go language has always been defined by a spec, not an implementation.  The Go team has written two different compilers that implement that spec: gc and gccgo.  Having two different implementations helps ensure that the spec is complete and correct: when the compilers disagree, we fix the spec, and change one or both compilers accordingly.  Gc is the original compiler, and the go tool uses it by default.  Gccgo is a different implementation with a different focus, and in this post we’ll take a closer look at it.\n    Gccgo is distributed as part of GCC, the GNU Compiler Collection.  GCC supports several different frontends for different languages; gccgo is a Go frontend connected to the GCC backend.  The Go frontend is separate from the GCC project and is designed to be able to connect to other compiler backends, but currently only supports GCC.\n    Compared to gc, gccgo is slower to compile code but supports more powerful optimizations, so a CPU-bound program built by gccgo will usually run faster.  All the optimizations implemented in GCC over the years are available, including inlining, loop optimizations, vectorization, instruction scheduling, and more.  While it does not always produce better code, in some cases programs compiled with gccgo can run 30% faster.\n    The gc compiler supports only the most popular processors: x86 (32-bit and 64-bit) and ARM.  Gccgo, however, supports all the processors that GCC supports.  Not all those processors have been thoroughly tested for gccgo, but many have, including x86 (32-bit and 64-bit), SPARC, MIPS, PowerPC and even Alpha.  Gccgo has also been tested on operating systems that the gc compiler does not support, notably Solaris.\n    Gccgo provides the standard, complete Go library.  Many of the core features of the Go runtime are the same in both gccgo and gc, including the goroutine scheduler, channels, the memory allocator, and the garbage collector.  Gccgo supports splitting goroutine stacks as the gc compiler does, but currently only on x86 (32-bit or 64-bit) and only when using the gold linker (on other processors, each goroutine will have a large stack, and a deep series of function calls may run past the end of the stack and crash the program).\n    Gccgo distributions do not yet include a version of the go command.  However, if you install the go command from a standard Go release, it already supports gccgo via the -compiler option: go build -compiler gccgo myprog.  The tools used for calls between Go and C/C++, cgo and SWIG, also support gccgo.\n    We have put the Go frontend under the same BSD license as the rest of the Go\n    tools.  You can download the source code for the frontend at the\n    gofrontend project. Note that when the Go frontend is linked with the GCC backend to make gccgo, GCC’s GPL license takes precedence.\n    The latest release of GCC, 4.7.1, includes gccgo with support for Go 1.  If you need better performance for CPU-bound Go programs, or you need to support processors or operating systems that the gc compiler does not support, gccgo might be the answer.\n		\n			By Ian Lance Taylor\n		\n	','https://blog.golang.org/gccgo-in-gcc-471','2012-07-11','2017-05-07 13:50:02.757858'),(542,'Third-party libraries: goprotobuf and beyond','Andrew Gerrand','		Third-party libraries: goprotobuf and beyond\n		20 April 2010\n		\n    On March 24, Rob Pike announced goprotobuf, the Go bindings of Google\'s data interchange format Protocol Buffers, called protobufs for short.  With this announcement, Go joins C++, Java, and Python as languages providing official protobuf implementations. This marks an important milestone in enabling the interoperability between existing systems and those built in Go.\n    The goprotobuf project consists of two parts: a \'protocol compiler plugin\' that generates Go source files that, once compiled, can access and manage protocol buffers; and a Go package that implements run-time support for encoding (marshaling), decoding (unmarshaling), and accessing protocol buffers.\n    To use goprotobuf, you first need to have both Go and protobuf installed. You can then install the \'proto\' package with goinstall:\n  goinstall goprotobuf.googlecode.com/hg/proto\n    And then install the protobuf compiler plugin:\n  cd $GOROOT/src/pkg/goprotobuf.googlecode.com/hg/compiler\nmake install\n    For more detail see the project\'s README file.\n    This is one of a growing list of third-party Go projects. Since the announcement of goprotobuf, the X Go bindings have been spun off from the standard library to the x-go-binding project, and work has begun on a Freetype port, freetype-go. Other popular third-party projects include the lightweight web framework web.go, and the Go GTK bindings gtk-go.\n    We wish to encourage the development of other useful packages by the open source community. If you\'re working on something, don\'t keep it to yourself - let us know through our mailing list golang-nuts.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/third-party-libraries-goprotobuf-and','2010-04-20','2017-05-07 13:50:02.796928'),(543,'go fmt your code','Andrew Gerrand','		go fmt your code\n		23 January 2013\n		\n  Introduction\n    Gofmt is a tool that automatically formats Go source code.\n    Gofmt\'d code is:\n    easier to write: never worry about minor formatting concerns while hacking away,\n    easier to read: when all code looks the same you need not mentally convert others\' formatting style into something you can understand.\n    easier to maintain: mechanical changes to the source don\'t cause unrelated changes to the file\'s formatting; diffs show only the real changes.\n    uncontroversial: never have a debate about spacing or brace position ever again!\n  Format your code\n    We recently conducted a survey of Go packages in the wild and found that about 70% of them are formatted according to gofmt\'s rules. This was more than expected - and thanks to everyone who uses gofmt - but it would be great to close the gap.\n    To format your code, you can use the gofmt tool directly:\n  gofmt -w yourcode.go\n    Or you can use the \"go fmt\" command:\n  go fmt path/to/your/package\n    To help keep your code in the canonical style, the Go repository contains hooks for editors and version control systems that make it easy to run gofmt on your code.\n    For Vim users, the Vim plugin for Go includes the :Fmt command that runs gofmt on the current buffer.\n    For emacs users, go-mode.el provides a gofmt-before-save hook that can be installed by adding this line to your .emacs file:\n  (add-hook \'before-save-hook #\'gofmt-before-save)\n    For Eclipse or Sublime Text users, the GoClipse and GoSublime projects add a gofmt facility to those editors.\n    And for Git aficionados, the misc/git/pre-commit script is a pre-commit hook that prevents incorrectly-formatted Go code from being committed. If you use Mercurial, the hgstyle plugin provides a gofmt pre-commit hook.\n  Mechanical source transformation\n    One of the greatest virtues of machine-formatted code is that it can be transformed mechanically without generating unrelated formatting noise in the diffs. Mechanical transformation is invaluable when working with large code bases, as it is both more comprehensive and less error prone than making wide-sweeping changes by hand. Indeed, when working at scale (like we do at Google) it often isn\'t practical to make these kinds of changes manually.\n    The easiest way to mechanically manipulate Go code is with gofmt\'s -r flag. The flag specifies a rewrite rule of the form\n  pattern -&gt; replacement\n    where both pattern and replacement are valid Go expressions. In the pattern, single-character lowercase identifiers serve as wildcards matching arbitrary sub-expressions, and those expressions are substituted for the same identifiers in the replacement.\n    For example, this recent change to the Go core rewrote some uses of bytes.Compare to use the more efficient bytes.Equal. The contributor made the change using just two gofmt invocations:\n  gofmt -r \'bytes.Compare(a, b) == 0 -&gt; bytes.Equal(a, b)\'\ngofmt -r \'bytes.Compare(a, b) != 0 -&gt; !bytes.Equal(a, b)\'\n    Gofmt also enables gofix, which can make arbitrarily complex source transformations. Gofix was an invaluable tool during the early days when we regularly made breaking changes to the language and libraries. For example, before Go 1 the built-in error interface didn\'t exist and the convention was to use the os.Error type. When we introduced error, we provided a gofix module that rewrote all references to os.Error and its associated helper functions to use error and the new errors package. It would have been daunting to attempt by hand, but with the code in a standard format it was relatively easy to prepare, execute, and review this change which touched almost all Go code in existence.\n    For more about gofix, see this article.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go-fmt-your-code','2013-01-23','2017-05-07 13:50:02.825262'),(544,'Introducing the Go Race Detector','Dmitry Vyukov and Andrew Gerrand','		Introducing the Go Race Detector\n		26 June 2013\n		\n  Introduction\n    Race conditions are among the\n    most insidious and elusive programming errors. They typically cause erratic and\n    mysterious failures, often long after the code has been deployed to production.\n    While Go\'s concurrency mechanisms make it easy to write clean concurrent code,\n    they don\'t prevent race conditions. Care, diligence, and testing are required.\n    And tools can help.\n    We\'re happy to announce that Go 1.1 includes a\n    race detector,\n    a new tool for finding race conditions in Go code.\n    It is currently available for Linux, OS X, and Windows systems\n    with 64-bit x86 processors.\n    The race detector is based on the C/C++\n    ThreadSanitizer runtime library,\n    which has been used to detect many errors in Google\'s internal code base and in\n    Chromium.\n    The technology was integrated with Go in September 2012; since then it has detected\n    42 races\n    in the standard library. It is now part of our continuous build process,\n    where it continues to catch race conditions as they arise.\n  How it works\n    The race detector is integrated with the go tool chain. When the\n    -race command-line flag is set, the compiler instruments all memory accesses\n    with code that records when and how the memory was accessed, while the runtime\n    library watches for unsynchronized accesses to shared variables.\n    When such \"racy\" behavior is detected, a warning is printed.\n    (See this article\n    for the details of the algorithm.)\n    Because of its design, the race detector can detect race conditions only when\n    they are actually triggered by running code, which means it\'s important to run\n    race-enabled binaries under realistic workloads.\n    However, race-enabled binaries can use ten times the CPU and memory, so it is\n    impractical to enable the race detector all the time.\n    One way out of this dilemma is to run some tests with the race detector\n    enabled. Load tests and integration tests are good candidates, since they tend\n    to exercise concurrent parts of the code.\n    Another approach using production workloads is to deploy a single race-enabled\n    instance within a pool of running servers.\n  Using the race detector\n    The race detector is fully integrated with the Go tool chain.\n    To build your code with the race detector enabled, just add the\n    -race flag to the command line:\n  $ go test -race mypkg    // test the package\n$ go run -race mysrc.go  // compile and run the program\n$ go build -race mycmd   // build the command\n$ go install -race mypkg // install the package\n    To try out the race detector for yourself, fetch and run this example program:\n  $ go get -race golang.org/x/blog/support/racy\n$ racy\n  Examples\n    Here are two examples of real issues caught by the race detector.\n  Example 1: Timer.Reset\n    The first example is a simplified version of an actual bug found by the race\n    detector. It uses a timer to print a message after a random duration between 0\n    and 1 second. It does so repeatedly for five seconds.\n    It uses time.AfterFunc to create a\n    Timer for the first message and then\n    uses the Reset method to\n    schedule the next message, re-using the Timer each time.\n	\n// +build OMIT\npackage main\nimport (\n	\"fmt\"\n	\"math/rand\"\n	\"time\"\n)\nfunc main() {\n    start := time.Now()\n    var t *time.Timer\n    t = time.AfterFunc(randomDuration(), func() {\n        fmt.Println(time.Now().Sub(start))\n        t.Reset(randomDuration())\n    })\n    time.Sleep(5 * time.Second)\n}\nfunc randomDuration() time.Duration {\n    return time.Duration(rand.Int63n(1e9))\n}\n    This looks like reasonable code, but under certain circumstances it fails in a surprising way:\n  panic: runtime error: invalid memory address or nil pointer dereference\n[signal 0xb code=0x1 addr=0x8 pc=0x41e38a]\ngoroutine 4 [running]:\ntime.stopTimer(0x8, 0x12fe6b35d9472d96)\n    src/pkg/runtime/ztime_linux_amd64.c:35 +0x25\ntime.(*Timer).Reset(0x0, 0x4e5904f, 0x1)\n    src/pkg/time/sleep.go:81 +0x42\nmain.func·001()\n    race.go:14 +0xe3\ncreated by time.goFunc\n    src/pkg/time/sleep.go:122 +0x48\n    What\'s going on here? Running the program with the race detector enabled is more illuminating:\n  ==================\nWARNING: DATA RACE\nRead by goroutine 5:\n  main.func·001()\n     race.go:14 +0x169\nPrevious write by goroutine 1:\n  main.main()\n      race.go:15 +0x174\nGoroutine 5 (running) created at:\n  time.goFunc()\n      src/pkg/time/sleep.go:122 +0x56\n  timerproc()\n     src/pkg/runtime/ztime_linux_amd64.c:181 +0x189\n==================\n    The race detector shows the problem: an unsynchronized read and write of the\n    variable t from different goroutines. If the initial timer duration is very\n    small, the timer function may fire before the main goroutine has assigned a\n    value to t and so the call to t.Reset is made with a nil t.\n    To fix the race condition we change the code to read and write the variable\n    t only from the main goroutine:\n	\n// +build OMIT\npackage main\nimport (\n	\"fmt\"\n	\"math/rand\"\n	\"time\"\n)\nfunc main() {\n    start := time.Now()\n    reset := make(chan bool)\n    var t *time.Timer\n    t = time.AfterFunc(randomDuration(), func() {\n        fmt.Println(time.Now().Sub(start))\n        reset &lt;- true\n    })\n    for time.Since(start) &lt; 5*time.Second {\n        &lt;-reset\n        t.Reset(randomDuration())\n    }\n}\nfunc randomDuration() time.Duration {\n	return time.Duration(rand.Int63n(1e9))\n}\n    Here the main goroutine is wholly responsible for setting and resetting the\n    Timer t and a new reset channel communicates the need to reset the timer in\n    a thread-safe way.\n    A simpler but less efficient approach is to\n    avoid reusing timers.\n  Example 2: ioutil.Discard\n    The second example is more subtle.\n    The ioutil package\'s\n    Discard object implements \n    io.Writer,\n    but discards all the data written to it.\n    Think of it like /dev/null: a place to send data that you need to read but\n    don\'t want to store.\n    It is commonly used with io.Copy\n    to drain a reader, like this:\n  io.Copy(ioutil.Discard, reader)\n    Back in July 2011 the Go team noticed that using Discard in this way was\n    inefficient: the Copy function allocates an internal 32 kB buffer each time it\n    is called, but when used with Discard the buffer is unnecessary since we\'re\n    just throwing the read data away.\n    We thought that this idiomatic use of Copy and Discard should not be so costly.\n    The fix was simple.\n    If the given Writer implements a ReadFrom method, a Copy call like this:\n  io.Copy(writer, reader)\n    is delegated to this potentially more efficient call:\n  writer.ReadFrom(reader)\n    We\n    added a ReadFrom method\n    to Discard\'s underlying type, which has an internal buffer that is shared\n    between all its users.\n    We knew this was theoretically a race condition, but since all writes to the\n    buffer should be thrown away we didn\'t think it was important.\n    When the race detector was implemented it immediately\n    flagged this code as racy.\n    Again, we considered that the code might be problematic, but decided that the\n    race condition wasn\'t \"real\".\n    To avoid the \"false positive\" in our build we implemented\n    a non-racy version\n    that is enabled only when the race detector is running.\n    But a few months later Brad encountered a\n    frustrating and strange bug.\n    After a few days of debugging, he narrowed it down to a real race condition\n    caused by ioutil.Discard.\n    Here is the known-racy code in io/ioutil, where Discard is a\n    devNull that shares a single buffer between all of its users.\n	\nvar blackHole [4096]byte // shared buffer\nfunc (devNull) ReadFrom(r io.Reader) (n int64, err error) {\n    readSize := 0\n    for {\n        readSize, err = r.Read(blackHole[:])\n        n += int64(readSize)\n        if err != nil {\n            if err == io.EOF {\n                return n, nil\n            }\n            return\n        }\n    }\n}\n    Brad\'s program includes a trackDigestReader type, which wraps an io.Reader\n    and records the hash digest of what it reads.\n  type trackDigestReader struct {\n    r io.Reader\n    h hash.Hash\n}\nfunc (t trackDigestReader) Read(p []byte) (n int, err error) {\n    n, err = t.r.Read(p)\n    t.h.Write(p[:n])\n    return\n}\n    For example, it could be used to compute the SHA-1 hash of a file while reading it:\n  tdr := trackDigestReader{r: file, h: sha1.New()}\nio.Copy(writer, tdr)\nfmt.Printf(\"File hash: %x\", tdr.h.Sum(nil))\n    In some cases there would be nowhere to write the data—but still a need to hash\n    the file—and so Discard would be used:\n  io.Copy(ioutil.Discard, tdr)\n    But in this case the blackHole buffer isn\'t just a black hole; it is a\n    legitimate place to store the data between reading it from the source\n    io.Reader and writing it to the hash.Hash.\n    With multiple goroutines hashing files simultaneously, each sharing the same\n    blackHole buffer, the race condition manifested itself by corrupting the data\n    between reading and hashing.\n    No errors or panics occurred, but the hashes were wrong. Nasty!\n  func (t trackDigestReader) Read(p []byte) (n int, err error) {\n    // the buffer p is blackHole\n    n, err = t.r.Read(p)\n    // p may be corrupted by another goroutine here,\n    // between the Read above and the Write below\n    t.h.Write(p[:n])\n    return\n}\n    The bug was finally\n    fixed\n    by giving a unique buffer to each use of ioutil.Discard, eliminating the race\n    condition on the shared buffer.\n  Conclusions\n    The race detector is a powerful tool for checking the correctness of concurrent\n    programs. It will not issue false positives, so take its warnings seriously.\n    But it is only as good as your tests; you must make sure they thoroughly\n    exercise the concurrent properties of your code so that the race detector can\n    do its job.\n    What are you waiting for? Run \"go test -race\" on your code today!\n		\n			By Dmitry Vyukov and Andrew Gerrand\n		\n	','https://blog.golang.org/race-detector','2013-06-26','2017-05-07 13:50:02.862745'),(545,'Defer, Panic, and Recover','Andrew Gerrand','		Defer, Panic, and Recover\n		4 August 2010\n		\n    Go has the usual mechanisms for control flow: if, for, switch, goto.  It also has the go statement to run code in a separate goroutine.  Here I\'d like to discuss some of the less common ones: defer, panic, and recover.\n    A defer statement pushes a function call onto a list. The list of saved calls is executed after the surrounding function returns. Defer is commonly used to simplify functions that perform various clean-up actions.\n    For example, let\'s look at a function that opens two files and copies the contents of one file to the other:\n  func CopyFile(dstName, srcName string) (written int64, err error) {\n    src, err := os.Open(srcName)\n    if err != nil {\n        return\n    }\n    dst, err := os.Create(dstName)\n    if err != nil {\n        return\n    }\n    written, err = io.Copy(dst, src)\n    dst.Close()\n    src.Close()\n    return\n}\n    This works, but there is a bug. If the call to os.Create fails, the function will return without closing the source file. This can be easily remedied by putting a call to src.Close before the second return statement, but if the function were more complex the problem might not be so easily noticed and resolved. By introducing defer statements we can ensure that the files are always closed:\n  func CopyFile(dstName, srcName string) (written int64, err error) {\n    src, err := os.Open(srcName)\n    if err != nil {\n        return\n    }\n    defer src.Close()\n    dst, err := os.Create(dstName)\n    if err != nil {\n        return\n    }\n    defer dst.Close()\n    return io.Copy(dst, src)\n}\n    Defer statements allow us to think about closing each file right after opening it, guaranteeing that, regardless of the number of return statements in the function, the files will be closed.\n    The behavior of defer statements is straightforward and predictable. There are three simple rules:\n    1. A deferred function\'s arguments are evaluated when the defer statement is evaluated.\n    In this example, the expression \"i\" is evaluated when the Println call is deferred. The deferred call will print \"0\" after the function returns.\n  func a() {\n    i := 0\n    defer fmt.Println(i)\n    i++\n    return\n}\n    2. Deferred function calls are executed in Last In First Out order after the surrounding function returns.\n    This function prints \"3210\":\n  func b() {\n    for i := 0; i &lt; 4; i++ {\n        defer fmt.Print(i)\n    }\n}\n    3. Deferred functions may read and assign to the returning function\'s named return values.\n    In this example, a deferred function increments the return value i after the surrounding function returns. Thus, this function returns 2:\n  func c() (i int) {\n    defer func() { i++ }()\n    return 1\n}\n    This is convenient for modifying the error return value of a function; we will see an example of this shortly.\n    Panic is a built-in function that stops the ordinary flow of control and begins panicking. When the function F calls panic, execution of F stops, any deferred functions in F are executed normally, and then F returns to its caller. To the caller, F then behaves like a call to panic. The process continues up the stack until all functions in the current goroutine have returned, at which point the program crashes. Panics can be initiated by invoking panic directly. They can also be caused by runtime errors, such as out-of-bounds array accesses.\n    Recover is a built-in function that regains control of a panicking goroutine. Recover is only useful inside deferred functions. During normal execution, a call to recover will return nil and have no other effect. If the current goroutine is panicking, a call to recover will capture the value given to panic and resume normal execution.\n    Here\'s an example program that demonstrates the mechanics of panic and defer:\n  package main\nimport \"fmt\"\nfunc main() {\n    f()\n    fmt.Println(\"Returned normally from f.\")\n}\nfunc f() {\n    defer func() {\n        if r := recover(); r != nil {\n            fmt.Println(\"Recovered in f\", r)\n        }\n    }()\n    fmt.Println(\"Calling g.\")\n    g(0)\n    fmt.Println(\"Returned normally from g.\")\n}\nfunc g(i int) {\n    if i &gt; 3 {\n        fmt.Println(\"Panicking!\")\n        panic(fmt.Sprintf(\"%v\", i))\n    }\n    defer fmt.Println(\"Defer in g\", i)\n    fmt.Println(\"Printing in g\", i)\n    g(i + 1)\n}\n    The function g takes the int i, and panics if i is greater than 3, or else it calls itself with the argument i+1. The function f defers a function that calls recover and prints the recovered value (if it is non-nil). Try to picture what the output of this program might be before reading on.\n    The program will output:\n  Calling g.\nPrinting in g 0\nPrinting in g 1\nPrinting in g 2\nPrinting in g 3\nPanicking!\nDefer in g 3\nDefer in g 2\nDefer in g 1\nDefer in g 0\nRecovered in f 4\nReturned normally from f.\n    If we remove the deferred function from f the panic is not recovered and reaches the top of the goroutine\'s call stack, terminating the program. This modified program will output:\n  Calling g.\nPrinting in g 0\nPrinting in g 1\nPrinting in g 2\nPrinting in g 3\nPanicking!\nDefer in g 3\nDefer in g 2\nDefer in g 1\nDefer in g 0\npanic: 4\npanic PC=0x2a9cd8\n[stack trace omitted]\n    For a real-world example of panic and recover, see the json package from the Go standard library. It decodes JSON-encoded data with a set of recursive functions. When malformed JSON is encountered, the parser calls panic to unwind the stack to the top-level function call, which recovers from the panic and returns an appropriate error value (see the \'error\' and \'unmarshal\' methods of the decodeState type in decode.go).\n    The convention in the Go libraries is that even when a package uses panic internally, its external API still presents explicit error return values.\n    Other uses of defer (beyond the file.Close example given earlier) include releasing a mutex:\n  mu.Lock()\ndefer mu.Unlock()\n    printing a footer:\n  printHeader()\ndefer printFooter()\n    and more.\n    In summary, the defer statement (with or without panic and recover) provides an unusual and powerful mechanism for control flow.  It can be used to model a number of features implemented by special-purpose structures in other programming languages. Try it out.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/defer-panic-and-recover','2010-08-04','2017-05-07 13:50:03.000008'),(546,'C? Go? Cgo!','Andrew Gerrand','		C? Go? Cgo!\n		17 March 2011\n		\n  Introduction\n    Cgo lets Go packages call C code. Given a Go source file written with some special features, cgo outputs Go and C files that can be combined into a single Go package.\n    To lead with an example, here\'s a Go package that provides two functions - Random and Seed - that wrap C\'s random and srandom functions.\n  package rand\n/*\n#include &lt;stdlib.h&gt;\n*/\nimport \"C\"\nfunc Random() int {\n    return int(C.random())\n}\nfunc Seed(i int) {\n    C.srandom(C.uint(i))\n}\n    Let\'s look at what\'s happening here, starting with the import statement.\n    The rand package imports \"C\", but you\'ll find there\'s no such package in the standard Go library. That\'s because C is a \"pseudo-package\", a special name interpreted by cgo as a reference to C\'s name space.\n    The rand package contains four references to the C package: the calls to C.random and C.srandom, the conversion C.uint(i), and the import statement.\n    The Random function calls the standard C library\'s random function and returns the result.  In C, random returns a value of the C type long, which cgo represents as the type C.long. It must be converted to a Go type before it can be used by Go code outside this package, using an ordinary Go type conversion:\n  func Random() int {\n    return int(C.random())\n}\n    Here\'s an equivalent function that uses a temporary variable to illustrate the type conversion more explicitly:\n  func Random() int {\n    var r C.long = C.random()\n    return int(r)\n}\n    The Seed function does the reverse, in a way. It takes a regular Go int, converts it to the C unsigned int type, and passes it to the C function srandom.\n  func Seed(i int) {\n    C.srandom(C.uint(i))\n}\n    Note that cgo knows the unsigned int type as C.uint; see the cgo documentation for a complete list of these numeric type names.\n    The one detail of this example we haven\'t examined yet is the comment above the import statement.\n  /*\n#include &lt;stdlib.h&gt;\n*/\nimport \"C\"\n    Cgo recognizes this comment.  Any lines starting with #cgo followed by a space character are removed; these become directives for cgo. The remaining lines are used as a header when compiling the C parts of the package.  In this case those lines are just a single #include statement, but they can be almost any C code.  The #cgo directives are used to provide flags for the compiler and linker when building the C parts of the package.\n    There is a limitation: if your program uses any //export directives, then the C code in the comment may only include declarations (extern int f();), not definitions (int f() { return 1; }).  You can use //export directives to make Go functions accessible to C code.\n    The #cgo and //export directives are documented in the cgo documentation.\n  Strings and things\n    Unlike Go, C doesn\'t have an explicit string type. Strings in C are represented by a zero-terminated array of chars.\n    Conversion between Go and C strings is done with the C.CString, C.GoString, and C.GoStringN functions. These conversions make a copy of the string data.\n    This next example implements a Print function that writes a string to standard output using C\'s fputs function from the stdio library:\n  package print\n// #include &lt;stdio.h&gt;\n// #include &lt;stdlib.h&gt;\nimport \"C\"\nimport \"unsafe\"\nfunc Print(s string) {\n    cs := C.CString(s)\n    C.fputs(cs, (*C.FILE)(C.stdout))\n    C.free(unsafe.Pointer(cs))\n}\n    Memory allocations made by C code are not known to Go\'s memory manager. When you create a C string with C.CString (or any C memory allocation) you must remember to free the memory when you\'re done with it by calling C.free.\n    The call to C.CString returns a pointer to the start of the char array, so before the function exits we convert it to an unsafe.Pointer and release the memory allocation with C.free. A common idiom in cgo programs is to defer the free immediately after allocating (especially when the code that follows is more complex than a single function call), as in this rewrite of Print:\n  func Print(s string) {\n    cs := C.CString(s)\n    defer C.free(unsafe.Pointer(cs))\n    C.fputs(cs, (*C.FILE)(C.stdout))\n}\n  Building cgo packages\n    To build cgo packages, just use go build or go install as usual. The go tool recognizes the special \"C\" import and automatically uses cgo for those files.\n  More cgo resources\n    The cgo command documentation has more detail about the C pseudo-package and the build process. The cgo examples in the Go tree demonstrate more advanced concepts.\n    Finally, if you\'re curious as to how all this works internally, take a look at the introductory comment of the runtime package\'s cgocall.go.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/c-go-cgo','2011-03-17','2017-05-07 13:50:03.098692'),(547,'Go at Heroku','Keith Rarick and Blake Mizerany','		Go at Heroku\n		21 April 2011\n		\n    This week’s blog post is written by Keith Rarick and Blake Mizerany, systems engineers at Heroku. In their own words, they \"eat, drink, and sleep distributed systems.\" Here they discuss their experiences using Go.\n    A big problem that comes with building distributed systems is the coordination of physical servers. Each server needs to know various facts about the system as a whole. This critical data includes locks, configuration data, and so on, and it must be consistent and available even during data store failures, so we need a data store with solid consistency guarantees. Our solution to this problem is Doozer, a new, consistent, highly-available data store written in Go.\n    At Doozer\'s core is Paxos, a family of protocols for solving consensus in an unreliable network of unreliable nodes. While Paxos is essential to running a fault-tolerant system, it is notorious for being difficult to implement. Even example implementations that can be found online are complex and hard to follow, despite being simplified for educational purposes. Existing production systems have a reputation for being worse.\n    Fortunately, Go\'s concurrency primitives made the task much easier. Paxos is defined in terms of independent, concurrent processes that communicate via passing messages. In Doozer, these processes are implemented as goroutines, and their communications as channel operations. In the same way that garbage collectors improve upon malloc and free, we found that goroutines and channels improve upon the lock-based approach to concurrency. These tools let us avoid complex bookkeeping and stay focused on the problem at hand. We are still amazed at how few lines of code it took to achieve something renowned for being difficult.\n    The standard packages in Go were another big win for Doozer. The Go team is very pragmatic about what goes into them. For instance, a package we quickly found useful was websocket. Once we had a working data store, we needed an easy way to introspect it and visualize activity. Using the websocket package, Keith was able to add the web viewer on his train ride home and without requiring external dependencies. This is a real testament to how well Go mixes systems and application programming.\n    One of our favorite productivity gains was provided by Go\'s source formatter: gofmt. We never argued over where to put a curly-brace, tabs vs. spaces, or if we should align assignments. We simply agreed that the buck stopped at the default output from gofmt.\n    Deploying Doozer was satisfyingly simple. Go builds statically linked binaries which means Doozer has no external dependencies; it\'s a single file that can be copied to any machine and immediately launched to join a cluster of running Doozers.\n    Finally, Go\'s maniacal focus on simplicity and orthogonality aligns with our view of software engineering. Like the Go team, we are pragmatic about what features go into Doozer. We sweat the details, preferring to change an existing feature instead of introducing a new one. In this sense, Go is a perfect match for Doozer.\n    We already have future projects in mind for Go. Doozer is just the start of much bigger system.\n		\n			By Keith Rarick and Blake Mizerany\n		\n	','https://blog.golang.org/go-at-heroku','2011-04-21','2017-05-07 13:50:03.143051'),(548,'Go Programming session video from Google I/O','Andrew Gerrand','		Go Programming session video from Google I/O\n		6 June 2010\n		\n    Below is the video of the talk given by Rob Pike and Russ Cox at Google I/O 2010.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go-programming-session-video-from','2010-06-06','2017-05-07 13:50:03.168301'),(549,'Go at I/O: Frequently Asked Questions','Andrew Gerrand','		Go at I/O: Frequently Asked Questions\n		27 May 2010\n		\n    Among the high-profile product launches at Google I/O last week, our small team gave presentations to packed rooms and met many present and future Go programmers. It was especially gratifying to meet with so many people who, after learning a bit about Go, were excited by the potential benefits (both immediate and long-term) they could gain from using it.\n    We were asked a lot of good questions during I/O, and in this post I\'d like to recap and expand upon some of them.\n    How suitable is Go for production systems?\n    Go is ready and stable now. We are pleased to report that Google is using Go for some production systems, and they are performing well. Of course there is still room for improvement - that\'s why we\'re continuing to work on the language, libraries, tools, and runtime.\n    Do you have plans to implement generics?\n    Many proposals for generics-like features have been mooted both publicly and internally, but as yet we haven\'t found a proposal that is consistent with the rest of the language. We think that one of Go\'s key strengths is its simplicity, so we are wary of introducing new features that might make the language more difficult to understand. Additionally, the more Go code we write (and thus the better we learn how to write Go code ourselves), the less we feel the need for such a language feature.\n    Do you have any plans to support GPU programming? \n    We don\'t have any immediate plans to do this, but as Go is architecture-agnostic it\'s quite possible. The ability to launch a goroutine that runs on a different processor architecture, and to use channels to communicate between goroutines running on separate architectures, seem like good ideas.\n    Are there plans to support Go under App Engine?\n    Both the Go and App Engine teams would like to see this happen. As always, it is a question of resources and priorities as to if and when it will become a reality.\n    Are there plans to support Go under Android?\n    Both Go compilers support ARM code generation, so it is possible. While we think Go would be a great language for writing mobile applications, Android support is not something that\'s being actively worked on.\n    What can I use Go for?\n    Go was designed with systems programming in mind. Servers, clients, databases, caches, balancers, distributors - these are applications Go is obviously useful for, and  this is how we have begun to use it within Google. However, since Go\'s open-source release, the community has found a diverse range of applications for the language. From web apps to games to graphics tools, Go promises to shine as a general-purpose programming language. The potential is only limited by library support, which is improving at a tremendous rate. Additionally, educators have expressed interest in using Go to teach programming, citing its succinct syntax and consistency as well-suited to the task.\n    Thanks to everyone who attended our presentations, or came to talk with us at Office Hours. We hope to see you again at future events.\n    The video of Rob and Russ\' talk will be uploaded to YouTube within the next week, and will then be posted on this blog.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go-at-io-frequently-asked-questions','2010-05-27','2017-05-07 13:50:03.187021'),(550,'From zero to Go: launching on the Google homepage in 24 hours','Reinaldo Aguiar','		From zero to Go: launching on the Google homepage in 24 hours\n		13 December 2011\n		\n  Introduction\n    This article was written by Reinaldo Aguiar, a software engineer from the Search team at Google. He shares his experience developing his first Go program and launching it to an audience of millions - all in one day!\n    I was recently given the opportunity to collaborate on a small but highly visible \"20% project\": the Thanksgiving 2011 Google Doodle. The doodle features a turkey produced by randomly combining different styles of head, wings, feathers and legs. The user can customize it by clicking on the different parts of the turkey. This interactivity is implemented in the browser by a combination of JavaScript, CSS and of course HTML, creating turkeys on the fly.\n    Once the user has created a personalized turkey it can be shared with friends and family by posting to Google+. Clicking a \"Share\" button (not pictured here) creates in the user\'s Google+ stream a post containing a snapshot of the turkey. The snapshot is a single image that matches the turkey the user created.\n    With 13 alternatives for each of 8 parts of the turkey (heads, pairs of legs, distinct feathers, etc.) there are more than than 800 million possible snapshot images that could be generated. To pre-compute them all is clearly infeasible. Instead, we must generate the snapshots on the fly. Combining that problem with a need for immediate scalability and high availability, the choice of platform is obvious: Google App Engine!\n    The next thing we needed to decide was which App Engine runtime to use. Image manipulation tasks are CPU-bound, so performance is the deciding factor in this case.\n    To make an informed decision we ran a test. We quickly prepared a couple of equivalent demo apps for the new Python 2.7 runtime (which provides PIL, a C-based imaging library) and the Go runtime. Each app generates an image composed of several small images, encodes the image as a JPEG, and sends the JPEG data as the HTTP response. The Python 2.7 app served requests with a median latency of 65 milliseconds, while the Go app ran with a median latency of just 32 milliseconds.\n    This problem therefore seemed the perfect opportunity to try the experimental Go runtime.\n    I had no previous experience with Go and the timeline was tight: two days to be production ready. This was intimidating, but I saw it as an opportunity to test Go from a different, often overlooked angle: development velocity. How fast can a person with no Go experience pick it up and build something that performs and scales?\n  Design\n    The approach was to encode the state of the turkey in the URL, drawing and encoding the snapshot on the fly.\n    The base for every doodle is the background:\n    A valid request URL might look like this: http://google-turkey.appspot.com/thumb/20332620][http://google-turkey.appspot.com/thumb/20332620\n    The alphanumeric string that follows \"/thumb/\" indicates (in hexadecimal) which choice to draw for each layout element, as illustrated by this image:\n    The program\'s request handler parses the URL to determine which element is selected for each component, draws the appropriate images on top of the background image, and serves the result as a JPEG.\n    If an error occurs, a default image is served. There\'s no point serving an error page because the user will never see it - the browser is almost certainly loading this URL into an image tag.\n  Implementation\n    In the package scope we declare some data structures to describe the elements of the turkey, the location of the corresponding images, and where they should be drawn on the background image.\n  var (\n    // dirs maps each layout element to its location on disk.\n    dirs = map[string]string{\n        \"h\": \"img/heads\",\n        \"b\": \"img/eyes_beak\",\n        \"i\": \"img/index_feathers\",\n        \"m\": \"img/middle_feathers\",\n        \"r\": \"img/ring_feathers\",\n        \"p\": \"img/pinky_feathers\",\n        \"f\": \"img/feet\",\n        \"w\": \"img/wing\",\n    }\n    // urlMap maps each URL character position to\n    // its corresponding layout element.\n    urlMap = [...]string{\"b\", \"h\", \"i\", \"m\", \"r\", \"p\", \"f\", \"w\"}\n    // layoutMap maps each layout element to its position\n    // on the background image.\n    layoutMap = map[string]image.Rectangle{\n        \"h\": {image.Pt(109, 50), image.Pt(166, 152)},\n        \"i\": {image.Pt(136, 21), image.Pt(180, 131)},\n        \"m\": {image.Pt(159, 7), image.Pt(201, 126)},\n        \"r\": {image.Pt(188, 20), image.Pt(230, 125)},\n        \"p\": {image.Pt(216, 48), image.Pt(258, 134)},\n        \"f\": {image.Pt(155, 176), image.Pt(243, 213)},\n        \"w\": {image.Pt(169, 118), image.Pt(250, 197)},\n        \"b\": {image.Pt(105, 104), image.Pt(145, 148)},\n    }\n)\n    The geometry of the points above was calculated by measuring the actual location and size of each layout element within the image.\n    Loading the images from disk on each request would be wasteful repetition, so we load all 106 images (13 * 8 elements + 1 background + 1 default) into global variables upon receipt of the first request.\n  var (\n    // elements maps each layout element to its images.\n    elements = make(map[string][]*image.RGBA)\n    // backgroundImage contains the background image data.\n    backgroundImage *image.RGBA\n    // defaultImage is the image that is served if an error occurs. \n    defaultImage *image.RGBA\n    // loadOnce is used to call the load function only on the first request.\n    loadOnce sync.Once\n)\n// load reads the various PNG images from disk and stores them in their\n// corresponding global variables.\nfunc load() {\n    defaultImage = loadPNG(defaultImageFile)\n    backgroundImage = loadPNG(backgroundImageFile)\n    for dirKey, dir := range dirs {\n        paths, err := filepath.Glob(dir + \"/*.png\")\n        if err != nil {\n            panic(err)\n        }\n        for _, p := range paths {\n            elements[dirKey] = append(elements[dirKey], loadPNG(p))\n        }\n    }\n}\n    Requests are handled in a straightforward sequence:\n    Parse the request URL, decoding the decimal value of each character in the path.\n    Make a copy of the background image as the base for the final image.\n    Draw each image element onto the background image using the layoutMap to determine where they should be drawn.\n    Encode the image as a JPEG\n    Return the image to user by writing the JPEG directly to the HTTP response writer.\n    Should any error occur, we serve the defaultImage to the user and log the error to the App Engine dashboard for later analysis.\n    Here\'s the code for the request handler with explanatory comments:\n  func handler(w http.ResponseWriter, r *http.Request) {\n    // [[http://blog.golang.org/2010/08/defer-panic-and-recover.html][Defer]] a function to recover from any panics.\n    // When recovering from a panic, log the error condition to\n    // the App Engine dashboard and send the default image to the user.\n    defer func() {\n        if err := recover(); err != nil {\n            c := appengine.NewContext(r)\n            c.Errorf(\"%s\", err)\n            c.Errorf(\"%s\", \"Traceback: %s\", r.RawURL)\n            if defaultImage != nil {\n                w.Header().Set(\"Content-type\", \"image/jpeg\")\n                jpeg.Encode(w, defaultImage, &amp;imageQuality)\n            }\n        }\n    }()\n    // Load images from disk on the first request.\n    loadOnce.Do(load)\n    // Make a copy of the background to draw into.\n    bgRect := backgroundImage.Bounds()\n    m := image.NewRGBA(bgRect.Dx(), bgRect.Dy())\n    draw.Draw(m, m.Bounds(), backgroundImage, image.ZP, draw.Over)\n    // Process each character of the request string.\n    code := strings.ToLower(r.URL.Path[len(prefix):])\n    for i, p := range code {\n        // Decode hex character p in place.\n        if p &lt; \'a\' {\n            // it\'s a digit\n            p = p - \'0\'\n        } else {\n            // it\'s a letter\n            p = p - \'a\' + 10\n        }\n        t := urlMap[i]    // element type by index\n        em := elements[t] // element images by type\n        if p &gt;= len(em) {\n            panic(fmt.Sprintf(\"element index out of range %s: \"+\n                \"%d &gt;= %d\", t, p, len(em)))\n        }\n        // Draw the element to m,\n        // using the layoutMap to specify its position.\n        draw.Draw(m, layoutMap[t], em[p], image.ZP, draw.Over)\n    }\n    // Encode JPEG image and write it as the response.\n    w.Header().Set(\"Content-type\", \"image/jpeg\")\n    w.Header().Set(\"Cache-control\", \"public, max-age=259200\")\n    jpeg.Encode(w, m, &amp;imageQuality)\n}\n    For brevity, I\'ve omitted several helper functions from these code listings. See the source code for the full scoop.\n  Performance\n    This chart - taken directly from the App Engine dashboard - shows average request latency during launch. As you can see, even under load it never exceeds 60 ms, with a median latency of 32 milliseconds. This is wicked fast, considering that our request handler is doing image manipulation and encoding on the fly.\n  Conclusions\n    I found Go\'s syntax to be intuitive, simple and clean. I have worked a lot with interpreted languages in the past, and although Go is instead a statically typed and compiled language, writing this app felt more like working with a dynamic, interpreted language.\n    The development server provided with the SDK quickly recompiles the program after any change, so I could iterate as fast as I would with an interpreted language. It\'s dead simple, too - it took less than a minute to set up my development environment.\n    Go\'s great documentation also helped me put this together fast. The docs are generated from the source code, so each function\'s documentation links directly to the associated source code. This not only allows the developer to understand very quickly what a particular function does but also encourages the developer to dig into the package implementation, making it easier to learn good style and conventions.\n    In writing this application I used just three resources: App Engine\'s Hello World Go example, the Go packages documentation, and a blog post showcasing the Draw package. Thanks to the rapid iteration made possible by the development server and the language itself, I was able to pick up the language and build a super fast, production ready, doodle generator in less than 24 hours.\n    Download the full app source code (including images) at the Google Code project.\n    Special thanks go to Guillermo Real and Ryan Germick who designed the doodle.\n		\n			By Reinaldo Aguiar\n		\n	','https://blog.golang.org/from-zero-to-go-launching-on-google','2011-12-13','2017-05-07 13:50:03.216598'),(551,'Go videos from Google I/O 2012','Andrew Gerrand','		Go videos from Google I/O 2012\n		2 July 2012\n		\n  Introduction\n    Phew! Google I/O is over for another year, and what an event it was. Thanks to our guest speakers and everyone who attended the four Go sessions. It was a lot of fun.\n    Here are the session videos:\n    Go concurrency patterns by Rob Pike\n    Concurrency is the key to designing high performance network services. Go\'s concurrency primitives (goroutines and channels) provide a simple and efficient means of expressing concurrent execution. In this talk we see how tricky concurrency problems can be solved gracefully with simple Go code.\n  Go in production\n    Since Go\'s release in 2009 many companies (besides Google, of course) have used the language to build cool stuff. In this session Gustavo Niemeyer (Canonical), Keith Rarick (Heroku), Evan Shaw (Iron.io), and Patrick Crosby (StatHat) share their first-hand experience using Go in production environments.\n  Meet the Go team\n    A panel discussion with David Symonds, Robert Griesemer, Rob Pike, Ken Thompson, Andrew Gerrand, and Brad Fitzpatrick.\n    Computing Map Tiles with Go on App Engine by Chris Broadfoot and Andrew Gerrand\n    In this talk we use the Maps API and Go on App Engine to build an app to build custom tile sets for Google Maps. The app demonstrates using Go\'s suitability for computation in the cloud and App Engine\'s key scalability features, such as Task Queues and Backends.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go-videos-from-google-io-2012','2012-07-02','2017-05-07 13:50:03.253098'),(552,'Go becomes more stable','Andrew Gerrand','		Go becomes more stable\n		16 March 2011\n		\n    The Go project is moving fast. As we learn more about Go we are compelled to change our tools, libraries, and occasionally even the language itself. We permit backward-incompatible changes so that we can learn from, rather than immortalize, our mistakes. We believe flexibility at this stage of Go’s development is essential to the project’s development and, ultimately, its longevity.\n    Since Go\'s launch we have been making releases approximately once a week. Accompanying each release are notes describing what changed, with any backwards-incompatible changes signposted. Questions I hear often are \"Is Go stable? How can I be sure that I won’t have to update my Go code every week?\" The answer to those questions are now \"Yes,\" and \"You won\'t.\"\n    With this week’s release we’re introducing a new release tagging scheme. We intend to continue with our weekly releases, but have renamed the existing tags from release to weekly. The release tag will now be applied to one hand-picked stable release each month or two. This more relaxed release schedule should make life easier for the average Go programmer.\n    Users will still need to update their code periodically (this is the cost of using a young language) but with less frequency. An additional benefit is that by tagging stable releases less often we can put more effort into automating updates. To this end we have introduced gofix, a tool that will help you update your code.\n    The revision formerly tagged release.2011-03-07.1 (now weekly.2011-03-07.1) has been nominated our first stable release, and has been given the tag release.r56. As we tag each stable release we will post an announcement to the new golang-announce mailing list. (Why not subscribe now?)\n    What’s the upshot of all this? You can continue to keep your Go installation updated using hg update release, but now you should only need to update when we tag a new stable release. If you wish to stay at the leading edge, you should switch to the weekly tag with hg update weekly.\n    Happy coding!\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go-becomes-more-stable','2011-03-16','2017-05-07 13:50:03.294106'),(553,'Profiling Go Programs','Russ Cox, July 2011; updated by Shenghou Ma, May 2013','		Profiling Go Programs\n		24 June 2011\n		\n    At Scala Days 2011, Robert Hundt presented a paper titled\n    Loop Recognition in C++/Java/Go/Scala.\n    The paper implemented a specific loop finding algorithm, such as you might use\n    in a flow analysis pass of a compiler, in C++, Go, Java, Scala, and then used\n    those programs to draw conclusions about typical performance concerns in these\n    languages.\n    The Go program presented in that paper runs quite slowly, making it\n    an excellent opportunity to demonstrate how to use Go\'s profiling tools to take\n    a slow program and make it faster.\n    By using Go\'s profiling tools to identify and correct specific bottlenecks, we can make the Go loop finding program run an order of magnitude faster and use 6x less memory.\n    (Update: Due to recent optimizations of libstdc++ in gcc, the memory reduction is now 3.7x.)\n    Hundt\'s paper does not specify which versions of the C++, Go, Java, and Scala\n    tools he used.\n    In this blog post, we will be using the most recent weekly snapshot of the 6g\n    Go compiler and the version of g++ that ships with the Ubuntu Natty\n    distribution.\n    (We will not be using Java or Scala, because we are not skilled at writing efficient\n    programs in either of those languages, so the comparison would be unfair.\n    Since C++ was the fastest language in the paper, the comparisons here with C++ should\n    suffice.)\n    (Update: In this updated post, we will be using the most recent development snapshot\n    of the Go compiler on amd64 and the most recent version of g++ -- 4.8.0, which was\n    released in March 2013.)\n  $ go version\ngo version devel +08d20469cc20 Tue Mar 26 08:27:18 2013 +0100 linux/amd64\n$ g++ --version\ng++ (GCC) 4.8.0\nCopyright (C) 2013 Free Software Foundation, Inc.\n...\n$\n    The programs are run on a computer with a 3.4GHz Core i7-2600 CPU and 16 GB of\n    RAM running Gentoo Linux\'s 3.8.4-gentoo kernel.\n    The machine is running with CPU frequency scaling disabled via\n  $ sudo bash\n# for i in /sys/devices/system/cpu/cpu[0-7]\ndo\n    echo performance &gt; $i/cpufreq/scaling_governor\ndone\n#\n    We\'ve taken Hundt\'s benchmark programs\n    in C++ and Go, combined each into a single source file, and removed all but one\n    line of output.\n    We\'ll time the program using Linux\'s time utility with a format that shows user time,\n    system time, real time, and maximum memory usage:\n  $ cat xtime\n#!/bin/sh\n/usr/bin/time -f \'%Uu %Ss %er %MkB %C\' \"$@\"\n$\n$ make havlak1cc\ng++ -O3 -o havlak1cc havlak1.cc\n$ ./xtime ./havlak1cc\n# of loops: 76002 (total 3800100)\nloop-0, nest: 0, depth: 0\n17.70u 0.05s 17.80r 715472kB ./havlak1cc\n$\n$ make havlak1\ngo build havlak1.go\n$ ./xtime ./havlak1\n# of loops: 76000 (including 1 artificial root node)\n25.05u 0.11s 25.20r 1334032kB ./havlak1\n$\n    The C++ program runs in 17.80 seconds and uses 700 MB of memory.\n    The Go program runs in 25.20 seconds and uses 1302 MB of memory.\n    (These measurements are difficult to reconcile with the ones in the paper, but the\n    point of this post is to explore how to use `go tool pprof`, not to reproduce the\n    results from the paper.)\n    To start tuning the Go program, we have to enable profiling.\n    If the code used the Go testing package\'s\n    benchmarking support, we could use gotest\'s standard -cpuprofile and -memprofile\n    flags.\n    In a standalone program like this one, we have to import runtime/pprof and add a few\n    lines of code:\n  var cpuprofile = flag.String(\"cpuprofile\", \"\", \"write cpu profile to file\")\nfunc main() {\n    flag.Parse()\n    if *cpuprofile != \"\" {\n        f, err := os.Create(*cpuprofile)\n        if err != nil {\n            log.Fatal(err)\n        }\n        pprof.StartCPUProfile(f)\n        defer pprof.StopCPUProfile()\n    }\n    ...\n    The new code defines a flag named cpuprofile, calls the\n    Go flag library to parse the command line flags,\n    and then, if the cpuprofile flag has been set on the command line,\n    starts CPU profiling\n    redirected to that file.\n    The profiler requires a final call to\n    StopCPUProfile to\n    flush any pending writes to the file before the program exits; we use defer\n    to make sure this happens as main returns.\n    After adding that code, we can run the program with the new -cpuprofile flag\n    and then run `go tool pprof` to interpret the profile.\n  $ make havlak1.prof\n./havlak1 -cpuprofile=havlak1.prof\n# of loops: 76000 (including 1 artificial root node)\n$ go tool pprof havlak1 havlak1.prof\nWelcome to pprof!  For help, type \'help\'.\n(pprof)\n    The `go tool pprof` program is a slight variant of\n    Google\'s pprof C++ profiler.\n    The most important command is topN, which shows the top N samples in the profile:\n  (pprof) top10\nTotal: 2525 samples\n     298  11.8%  11.8%      345  13.7% runtime.mapaccess1_fast64\n     268  10.6%  22.4%     2124  84.1% main.FindLoops\n     251   9.9%  32.4%      451  17.9% scanblock\n     178   7.0%  39.4%      351  13.9% hash_insert\n     131   5.2%  44.6%      158   6.3% sweepspan\n     119   4.7%  49.3%      350  13.9% main.DFS\n      96   3.8%  53.1%       98   3.9% flushptrbuf\n      95   3.8%  56.9%       95   3.8% runtime.aeshash64\n      95   3.8%  60.6%      101   4.0% runtime.settype_flush\n      88   3.5%  64.1%      988  39.1% runtime.mallocgc\n    When CPU profiling is enabled, the Go program stops about 100 times per second\n    and records a sample consisting of the program counters on the currently executing\n    goroutine\'s stack.\n    The profile has 2525 samples, so it was running for a bit over 25 seconds.\n    In the `go tool pprof` output, there is a row for each function that appeared in\n    a sample.\n    The first two columns show the number of samples in which the function was running\n    (as opposed to waiting for a called function to return), as a raw count and as a\n    percentage of total samples.\n    The runtime.mapaccess1_fast64 function was running during 298 samples, or 11.8%.\n    The top10 output is sorted by this sample count.\n    The third column shows the running total during the listing:\n    the first three rows account for 32.4% of the samples.\n    The fourth and fifth columns show the number of samples in which the function appeared\n    (either running or waiting for a called function to return).\n    The main.FindLoops function was running in 10.6% of the samples, but it was on the\n    call stack (it or functions it called were running) in 84.1% of the samples.\n    To sort by the fourth and fifth columns, use the -cum (for cumulative) flag:\n  (pprof) top5 -cum\nTotal: 2525 samples\n       0   0.0%   0.0%     2144  84.9% gosched0\n       0   0.0%   0.0%     2144  84.9% main.main\n       0   0.0%   0.0%     2144  84.9% runtime.main\n       0   0.0%   0.0%     2124  84.1% main.FindHavlakLoops\n     268  10.6%  10.6%     2124  84.1% main.FindLoops\n(pprof) top5 -cum\n    In fact the total for main.FindLoops and main.main should have been 100%, but\n    each stack sample only includes the bottom 100 stack frames; during about a quarter\n    of the samples, the recursive main.DFS function was more than 100 frames deeper\n    than main.main so the complete trace was truncated.\n    The stack trace samples contain more interesting data about function call relationships\n    than the text listings can show.\n    The web command writes a graph of the profile data in SVG format and opens it in a web\n    browser.\n    (There is also a gv command that writes PostScript and opens it in Ghostview.\n    For either command, you need graphviz installed.)\n  (pprof) web\n    A small fragment of\n    the full graph looks like:\n    Each box in the graph corresponds to a single function, and the boxes are sized\n    according to the number of samples in which the function was running.\n    An edge from box X to box Y indicates that X calls Y; the number along the edge is\n    the number of times that call appears in a sample.\n    If a call appears multiple times in a single sample, such as during recursive function\n    calls, each appearance counts toward the edge weight.\n    That explains the 21342 on the self-edge from main.DFS to itself.\n    Just at a glance, we can see that the program spends much of its time in hash\n    operations, which correspond to use of Go\'s map values.\n    We can tell web to use only samples that include a specific function, such as\n    runtime.mapaccess1_fast64, which clears some of the noise from the graph:\n  (pprof) web mapaccess1\n    If we squint, we can see that the calls to runtime.mapaccess1_fast64 are being\n    made by main.FindLoops and main.DFS.\n    Now that we have a rough idea of the big picture, it\'s time to zoom in on a particular\n    function.\n    Let\'s look at main.DFS first, just because it is a shorter function:\n  (pprof) list DFS\nTotal: 2525 samples\nROUTINE ====================== main.DFS in /home/rsc/g/benchgraffiti/havlak/havlak1.go\n   119    697 Total samples (flat / cumulative)\n     3      3  240: func DFS(currentNode *BasicBlock, nodes []*UnionFindNode, number map[*BasicBlock]int, last []int, current int) int {\n     1      1  241:     nodes[current].Init(currentNode, current)\n     1     37  242:     number[currentNode] = current\n     .      .  243:\n     1      1  244:     lastid := current\n    89     89  245:     for _, target := range currentNode.OutEdges {\n     9    152  246:             if number[target] == unvisited {\n     7    354  247:                     lastid = DFS(target, nodes, number, last, lastid+1)\n     .      .  248:             }\n     .      .  249:     }\n     7     59  250:     last[number[currentNode]] = lastid\n     1      1  251:     return lastid\n(pprof)\n    The listing shows the source code for the DFS function (really, for every function\n    matching the regular expression DFS).\n    The first three columns are the number of samples taken while running that line, the\n    number of samples taken while running that line or in code called from that line, and\n    the line number in the file.\n    The related command disasm shows a disassembly of the function instead of a source\n    listing; when there are enough samples this can help you see which instructions are\n    expensive.\n    The weblist command mixes the two modes: it shows\n    a source listing in which clicking a line shows the disassembly.\n    Since we already know that the time is going into map lookups implemented by the\n    hash runtime functions, we care most about the second column.\n    A large fraction of time is spent in recursive calls to DFS (line 247), as would be\n    expected from a recursive traversal.\n    Excluding the recursion, it looks like the time is going into the accesses to the\n    number map on lines 242, 246, and 250.\n    For that particular lookup, a map is not the most efficient choice.\n    Just as they would be in a compiler, the basic block structures have unique sequence\n    numbers assigned to them.\n    Instead of using a map[*BasicBlock]int we can use a []int, a slice indexed by the\n    block number.\n    There\'s no reason to use a map when an array or slice will do.\n    Changing number from a map to a slice requires editing seven lines in the program\n    and cut its run time by nearly a factor of two:\n  $ make havlak2\ngo build havlak2.go\n$ ./xtime ./havlak2\n# of loops: 76000 (including 1 artificial root node)\n16.55u 0.11s 16.69r 1321008kB ./havlak2\n$\n    (See the diff between havlak1 and havlak2)\n    We can run the profiler again to confirm that main.DFS is no longer a significant\n    part of the run time:\n  $ make havlak2.prof\n./havlak2 -cpuprofile=havlak2.prof\n# of loops: 76000 (including 1 artificial root node)\n$ go tool pprof havlak2 havlak2.prof\nWelcome to pprof!  For help, type \'help\'.\n(pprof)\n(pprof) top5\nTotal: 1652 samples\n     197  11.9%  11.9%      382  23.1% scanblock\n     189  11.4%  23.4%     1549  93.8% main.FindLoops\n     130   7.9%  31.2%      152   9.2% sweepspan\n     104   6.3%  37.5%      896  54.2% runtime.mallocgc\n      98   5.9%  43.5%      100   6.1% flushptrbuf\n(pprof)\n    The entry main.DFS no longer appears in the profile, and the rest of the program\n    runtime has dropped too.\n    Now the program is spending most of its time allocating memory and garbage collecting\n    (runtime.mallocgc, which both allocates and runs periodic garbage collections,\n    accounts for 54.2% of the time).\n    To find out why the garbage collector is running so much, we have to find out what is\n    allocating memory.\n    One way is to add memory profiling to the program.\n    We\'ll arrange that if the -memprofile flag is supplied, the program stops after one\n    iteration of the loop finding, writes a memory profile, and exits:\n  var memprofile = flag.String(\"memprofile\", \"\", \"write memory profile to this file\")\n...\n    FindHavlakLoops(cfgraph, lsgraph)\n    if *memprofile != \"\" {\n        f, err := os.Create(*memprofile)\n        if err != nil {\n            log.Fatal(err)\n        }\n        pprof.WriteHeapProfile(f)\n        f.Close()\n        return\n    }\n    We invoke the program with -memprofile flag to write a profile:\n  $ make havlak3.mprof\ngo build havlak3.go\n./havlak3 -memprofile=havlak3.mprof\n$\n    (See the diff from havlak2)\n    We use `go tool pprof` exactly the same way. Now the samples we are examining are\n    memory allocations, not clock ticks.\n  $ go tool pprof havlak3 havlak3.mprof\nAdjusting heap profiles for 1-in-524288 sampling rate\nWelcome to pprof!  For help, type \'help\'.\n(pprof) top5\nTotal: 82.4 MB\n    56.3  68.4%  68.4%     56.3  68.4% main.FindLoops\n    17.6  21.3%  89.7%     17.6  21.3% main.(*CFG).CreateNode\n     8.0   9.7%  99.4%     25.6  31.0% main.NewBasicBlockEdge\n     0.5   0.6% 100.0%      0.5   0.6% itab\n     0.0   0.0% 100.0%      0.5   0.6% fmt.init\n(pprof)\n    The command `go tool pprof` reports that FindLoops has allocated approximately\n    56.3 of the 82.4 MB in use; CreateNode accounts for another 17.6 MB.\n    To reduce overhead, the memory profiler only records information for approximately\n    one block per half megabyte allocated (the “1-in-524288 sampling rate”), so these\n    are approximations to the actual counts.\n    To find the memory allocations, we can list those functions.\n  (pprof) list FindLoops\nTotal: 82.4 MB\nROUTINE ====================== main.FindLoops in /home/rsc/g/benchgraffiti/havlak/havlak3.go\n  56.3   56.3 Total MB (flat / cumulative)\n...\n   1.9    1.9  268:     nonBackPreds := make([]map[int]bool, size)\n   5.8    5.8  269:     backPreds := make([][]int, size)\n     .      .  270:\n   1.9    1.9  271:     number := make([]int, size)\n   1.9    1.9  272:     header := make([]int, size, size)\n   1.9    1.9  273:     types := make([]int, size, size)\n   1.9    1.9  274:     last := make([]int, size, size)\n   1.9    1.9  275:     nodes := make([]*UnionFindNode, size, size)\n     .      .  276:\n     .      .  277:     for i := 0; i &lt; size; i++ {\n   9.5    9.5  278:             nodes[i] = new(UnionFindNode)\n     .      .  279:     }\n...\n     .      .  286:     for i, bb := range cfgraph.Blocks {\n     .      .  287:             number[bb.Name] = unvisited\n  29.5   29.5  288:             nonBackPreds[i] = make(map[int]bool)\n     .      .  289:     }\n...\n    It looks like the current bottleneck is the same as the last one: using maps where\n    simpler data structures suffice.\n    FindLoops is allocating about 29.5 MB of maps.\n    As an aside, if we run `go tool pprof` with the --inuse_objects flag, it will\n    report allocation counts instead of sizes:\n  $ go tool pprof --inuse_objects havlak3 havlak3.mprof\nAdjusting heap profiles for 1-in-524288 sampling rate\nWelcome to pprof!  For help, type \'help\'.\n(pprof) list FindLoops\nTotal: 1763108 objects\nROUTINE ====================== main.FindLoops in /home/rsc/g/benchgraffiti/havlak/havlak3.go\n720903 720903 Total objects (flat / cumulative)\n...\n     .      .  277:     for i := 0; i &lt; size; i++ {\n311296 311296  278:             nodes[i] = new(UnionFindNode)\n     .      .  279:     }\n     .      .  280:\n     .      .  281:     // Step a:\n     .      .  282:     //   - initialize all nodes as unvisited.\n     .      .  283:     //   - depth-first traversal and numbering.\n     .      .  284:     //   - unreached BB\'s are marked as dead.\n     .      .  285:     //\n     .      .  286:     for i, bb := range cfgraph.Blocks {\n     .      .  287:             number[bb.Name] = unvisited\n409600 409600  288:             nonBackPreds[i] = make(map[int]bool)\n     .      .  289:     }\n...\n(pprof)\n    Since the ~200,000 maps account for 29.5 MB, it looks like the initial map allocation\n    takes about 150 bytes.\n    That\'s reasonable when a map is being used to hold key-value pairs, but not when a map\n    is being used as a stand-in for a simple set, as it is here.\n    Instead of using a map, we can use a simple slice to list the elements.\n    In all but one of the cases where maps are being used, it is impossible for the algorithm\n    to insert a duplicate element.\n    In the one remaining case, we can write a simple variant of the append built-in function:\n  func appendUnique(a []int, x int) []int {\n    for _, y := range a {\n        if x == y {\n            return a\n        }\n    }\n    return append(a, x)\n}\n    In addition to writing that function, changing the Go program to use slices instead\n    of maps requires changing just a few lines of code.\n  $ make havlak4\ngo build havlak4.go\n$ ./xtime ./havlak4\n# of loops: 76000 (including 1 artificial root node)\n11.84u 0.08s 11.94r 810416kB ./havlak4\n$\n    (See the diff from havlak3)\n    We\'re now at 2.11x faster than when we started. Let\'s look at a CPU profile again.\n  $ make havlak4.prof\n./havlak4 -cpuprofile=havlak4.prof\n# of loops: 76000 (including 1 artificial root node)\n$ go tool pprof havlak4 havlak4.prof\nWelcome to pprof!  For help, type \'help\'.\n(pprof) top10\nTotal: 1173 samples\n     205  17.5%  17.5%     1083  92.3% main.FindLoops\n     138  11.8%  29.2%      215  18.3% scanblock\n      88   7.5%  36.7%       96   8.2% sweepspan\n      76   6.5%  43.2%      597  50.9% runtime.mallocgc\n      75   6.4%  49.6%       78   6.6% runtime.settype_flush\n      74   6.3%  55.9%       75   6.4% flushptrbuf\n      64   5.5%  61.4%       64   5.5% runtime.memmove\n      63   5.4%  66.8%      524  44.7% runtime.growslice\n      51   4.3%  71.1%       51   4.3% main.DFS\n      50   4.3%  75.4%      146  12.4% runtime.MCache_Alloc\n(pprof)\n    Now memory allocation and the consequent garbage collection (runtime.mallocgc)\n    accounts for 50.9% of our run time.\n    Another way to look at why the system is garbage collecting is to look at the\n    allocations that are causing the collections, the ones that spend most of the time\n    in mallocgc:\n  (pprof) web mallocgc\n    It\'s hard to tell what\'s going on in that graph, because there are many nodes with\n    small sample numbers obscuring the big ones.\n    We can tell `go tool pprof` to ignore nodes that don\'t account for at least 10% of\n    the samples:\n  $ go tool pprof --nodefraction=0.1 havlak4 havlak4.prof\nWelcome to pprof!  For help, type \'help\'.\n(pprof) web mallocgc\n    We can follow the thick arrows easily now, to see that FindLoops is triggering\n    most of the garbage collection.\n    If we list FindLoops we can see that much of it is right at the beginning:\n  (pprof) list FindLoops\n...\n     .      .  270: func FindLoops(cfgraph *CFG, lsgraph *LSG) {\n     .      .  271:     if cfgraph.Start == nil {\n     .      .  272:             return\n     .      .  273:     }\n     .      .  274:\n     .      .  275:     size := cfgraph.NumNodes()\n     .      .  276:\n     .    145  277:     nonBackPreds := make([][]int, size)\n     .      9  278:     backPreds := make([][]int, size)\n     .      .  279:\n     .      1  280:     number := make([]int, size)\n     .     17  281:     header := make([]int, size, size)\n     .      .  282:     types := make([]int, size, size)\n     .      .  283:     last := make([]int, size, size)\n     .      .  284:     nodes := make([]*UnionFindNode, size, size)\n     .      .  285:\n     .      .  286:     for i := 0; i &lt; size; i++ {\n     2     79  287:             nodes[i] = new(UnionFindNode)\n     .      .  288:     }\n...\n(pprof)\n    Every time FindLoops is called, it allocates some sizable bookkeeping structures.\n    Since the benchmark calls FindLoops 50 times, these add up to a significant amount\n    of garbage, so a significant amount of work for the garbage collector.\n    Having a garbage-collected language doesn\'t mean you can ignore memory allocation\n    issues.\n    In this case, a simple solution is to introduce a cache so that each call to FindLoops\n    reuses the previous call\'s storage when possible.\n    (In fact, in Hundt\'s paper, he explains that the Java program needed just this change to\n    get anything like reasonable performance, but he did not make the same change in the\n    other garbage-collected implementations.)\n    We\'ll add a global cache structure:\n  var cache struct {\n    size int\n    nonBackPreds [][]int\n    backPreds [][]int\n    number []int\n    header []int\n    types []int\n    last []int\n    nodes []*UnionFindNode\n}\n    and then have FindLoops consult it as a replacement for allocation:\n  if cache.size &lt; size {\n    cache.size = size\n    cache.nonBackPreds = make([][]int, size)\n    cache.backPreds = make([][]int, size)\n    cache.number = make([]int, size)\n    cache.header = make([]int, size)\n    cache.types = make([]int, size)\n    cache.last = make([]int, size)\n    cache.nodes = make([]*UnionFindNode, size)\n    for i := range cache.nodes {\n        cache.nodes[i] = new(UnionFindNode)\n    }\n}\nnonBackPreds := cache.nonBackPreds[:size]\nfor i := range nonBackPreds {\n    nonBackPreds[i] = nonBackPreds[i][:0]\n}\nbackPreds := cache.backPreds[:size]\nfor i := range nonBackPreds {\n    backPreds[i] = backPreds[i][:0]\n}\nnumber := cache.number[:size]\nheader := cache.header[:size]\ntypes := cache.types[:size]\nlast := cache.last[:size]\nnodes := cache.nodes[:size]\n    Such a global variable is bad engineering practice, of course: it means that\n    concurrent calls to FindLoops are now unsafe.\n    For now, we are making the minimal possible changes in order to understand what\n    is important for the performance of our program; this change is simple and mirrors\n    the code in the Java implementation.\n    The final version of the Go program will use a separate LoopFinder instance to\n    track this memory, restoring the possibility of concurrent use.\n  $ make havlak5\ngo build havlak5.go\n$ ./xtime ./havlak5\n# of loops: 76000 (including 1 artificial root node)\n8.03u 0.06s 8.11r 770352kB ./havlak5\n$\n    (See the diff from havlak4)\n    There\'s more we can do to clean up the program and make it faster, but none of\n    it requires profiling techniques that we haven\'t already shown.\n    The work list used in the inner loop can be reused across iterations and across\n    calls to FindLoops, and it can be combined with the separate “node pool” generated\n    during that pass.\n    Similarly, the loop graph storage can be reused on each iteration instead of reallocated.\n    In addition to these performance changes, the\n    final version\n    is written using idiomatic Go style, using data structures and methods.\n    The stylistic changes have only a minor effect on the run time: the algorithm and\n    constraints are unchanged.\n    The final version runs in 2.29 seconds and uses 351 MB of memory:\n  $ make havlak6\ngo build havlak6.go\n$ ./xtime ./havlak6\n# of loops: 76000 (including 1 artificial root node)\n2.26u 0.02s 2.29r 360224kB ./havlak6\n$\n    That\'s 11 times faster than the program we started with.\n    Even if we disable reuse of the generated loop graph, so that the only cached memory\n    is the loop finding bookeeping, the program still runs 6.7x faster than the original\n    and uses 1.5x less memory.\n  $ ./xtime ./havlak6 -reuseloopgraph=false\n# of loops: 76000 (including 1 artificial root node)\n3.69u 0.06s 3.76r 797120kB ./havlak6 -reuseloopgraph=false\n$\n    Of course, it\'s no longer fair to compare this Go program to the original C++\n    program, which used inefficient data structures like `set`s where `vector`s would\n    be more appropriate.\n    As a sanity check, we translated the final Go program into\n    equivalent C++ code.\n    Its execution time is similar to the Go program\'s:\n  $ make havlak6cc\ng++ -O3 -o havlak6cc havlak6.cc\n$ ./xtime ./havlak6cc\n# of loops: 76000 (including 1 artificial root node)\n1.99u 0.19s 2.19r 387936kB ./havlak6cc\n    The Go program runs almost as fast as the C++ program.\n    As the C++ program is using automatic deletes and allocation instead of an explicit\n    cache, the C++ program a bit shorter and easier to write, but not dramatically so:\n  $ wc havlak6.cc; wc havlak6.go\n 401 1220 9040 havlak6.cc\n 461 1441 9467 havlak6.go\n$\n    (See havlak6.cc\n    and havlak6.go)\n    Benchmarks are only as good as the programs they measure.\n    We used `go tool pprof` to study an inefficient Go program and then to improve its\n    performance by an order of magnitude and to reduce its memory usage by a factor of 3.7.\n    A subsequent comparison with an equivalently optimized C++ program shows that Go can be\n    competitive with C++ when programmers are careful about how much garbage is generated\n    by inner loops.\n    The program sources, Linux x86-64 binaries, and profiles used to write this post\n    are available in the benchgraffiti project on GitHub.\n    As mentioned above, `go test` includes\n    these profiling flags already: define a\n    benchmark function and you\'re all set.\n    There is also a standard HTTP interface to profiling data. In an HTTP server, adding\n  import _ \"net/http/pprof\"\n    will install handlers for a few URLs under /debug/pprof/.\n    Then you can run `go tool pprof` with a single argument—the URL to your server\'s\n    profiling data and it will download and examine a live profile.\n  go tool pprof http://localhost:6060/debug/pprof/profile   # 30-second CPU profile\ngo tool pprof http://localhost:6060/debug/pprof/heap      # heap profile\ngo tool pprof http://localhost:6060/debug/pprof/block     # goroutine blocking profile\n    The goroutine blocking profile will be explained in a future post. Stay tuned.\n		\n			By Russ Cox, July 2011; updated by Shenghou Ma, May 2013\n		\n	','https://blog.golang.org/profiling-go-programs','2011-06-24','2017-05-07 13:50:03.325971'),(554,'A preview of Go version 1','Russ Cox','		A preview of Go version 1\n		5 October 2011\n		\n    We want to be able to provide a stable base for people using Go.  People should be able to write Go programs and expect that they will continue to compile and run without change, on a timescale of years.  Similarly, people should be able to write books about Go, be able to say which version of Go the book is describing, and have that version number still be meaningful much later.  None of these properties is true for Go today.\n    We propose to issue a Go release early next year that will be called “Go version 1”, Go 1 for short, that will be the first Go release to be stable in this way.  Code that compiles in Go version 1 should, with few exceptions, continue to compile throughout the lifetime of that version, as we issue updates and bug fixes such as Go version 1.1, 1.2, and so on. It will also be maintained with fixes for bugs and security flaws even as other versions may evolve. Also, production environments such as Google App Engine will support it for an extended time.\n    Go version 1 will be a stable language with stable libraries. Other than critical fixes, changes made to the library and packages for versions 1.1, 1.2 and so on may add functionality but will not break existing Go version 1 programs.\n    Our goal is for Go 1 to be a stable version of today’s Go, not a wholesale rethinking of the language.  In particular, we are explicitly resisting any efforts to design new language features “by committee.”\n    However, there are various changes to the Go language and packages that we have intended for some time and prototyped but have not deployed yet, primarily because they are significant and backwards-incompatible. If Go 1 is to be long-lasting, it is important that we plan, announce, implement, and test these changes as part of the preparation of Go 1, rather than delay them until after it is released and thereby introduce divergence that contradicts our goals.\n    Today, we are publishing our preliminary plan for Go 1 for feedback from the Go community.  If you have feedback, please reply to the thread on the golang-nuts mailing list.\n		\n			By Russ Cox\n		\n	','https://blog.golang.org/preview-of-go-version-1','2011-10-05','2017-05-07 13:50:03.412632'),(555,'Error handling and Go','Andrew Gerrand','		Error handling and Go\n		12 July 2011\n		\n  Introduction\n    If you have written any Go code you have probably encountered the built-in error type. Go code uses error values to indicate an abnormal state. For example, the os.Open function returns a non-nil error value when it fails to open a file.\n  func Open(name string) (file *File, err error)\n    The following code uses os.Open to open a file. If an error occurs it calls log.Fatal to print the error message and stop.\n  f, err := os.Open(\"filename.ext\")\nif err != nil {\n    log.Fatal(err)\n}\n// do something with the open *File f\n    You can get a lot done in Go knowing just this about the error type, but in this article we\'ll take a closer look at error and discuss some good practices for error handling in Go.\n  The error type\n    The error type is an interface type. An error variable represents any value that can describe itself as a string. Here is the interface\'s declaration:\n  type error interface {\n    Error() string\n}\n    The error type, as with all built in types, is predeclared in the universe block.\n    The most commonly-used error implementation is the errors package\'s unexported errorString type.\n  // errorString is a trivial implementation of error.\ntype errorString struct {\n    s string\n}\nfunc (e *errorString) Error() string {\n    return e.s\n}\n    You can construct one of these values with the errors.New function. It takes a string that it converts to an errors.errorString and returns as an error value.\n  // New returns an error that formats as the given text.\nfunc New(text string) error {\n    return &amp;errorString{text}\n}\n    Here\'s how you might use errors.New:\n  func Sqrt(f float64) (float64, error) {\n    if f &lt; 0 {\n        return 0, errors.New(\"math: square root of negative number\")\n    }\n    // implementation\n}\n    A caller passing a negative argument to Sqrt receives a non-nil error value (whose concrete representation is an errors.errorString value). The caller can access the error string (\"math: square root of...\") by calling the error\'s Error method, or by just printing it:\n  f, err := Sqrt(-1)\nif err != nil {\n    fmt.Println(err)\n}\n    The fmt package formats an error value by calling its Error() string method.\n    It is the error implementation\'s responsibility to summarize the context. The error returned by os.Open formats as \"open /etc/passwd: permission denied,\" not just \"permission denied.\"  The error returned by our Sqrt is missing information about the invalid argument.\n    To add that information, a useful function is the fmt package\'s Errorf. It formats a string according to Printf\'s rules and returns it as an error created by errors.New.\n  if f &lt; 0 {\n    return 0, fmt.Errorf(\"math: square root of negative number %g\", f)\n}\n    In many cases fmt.Errorf is good enough, but since error is an interface, you can use arbitrary data structures as error values, to allow callers to inspect the details of the error.\n    For instance, our hypothetical callers might want to recover the invalid argument passed to Sqrt. We can enable that by defining a new error implementation instead of using errors.errorString:\n  type NegativeSqrtError float64\nfunc (f NegativeSqrtError) Error() string {\n    return fmt.Sprintf(\"math: square root of negative number %g\", float64(f))\n}\n    A sophisticated caller can then use a type assertion to check for a NegativeSqrtError and handle it specially, while callers that just pass the error to fmt.Println or log.Fatal will see no change in behavior.\n    As another example, the json package specifies a SyntaxError type that the json.Decode function returns when it encounters a syntax error parsing a JSON blob.\n  type SyntaxError struct {\n    msg    string // description of error\n    Offset int64  // error occurred after reading Offset bytes\n}\nfunc (e *SyntaxError) Error() string { return e.msg }\n    The Offset field isn\'t even shown in the default formatting of the error, but callers can use it to add file and line information to their error messages:\n  if err := dec.Decode(&amp;val); err != nil {\n    if serr, ok := err.(*json.SyntaxError); ok {\n        line, col := findLine(f, serr.Offset)\n        return fmt.Errorf(\"%s:%d:%d: %v\", f.Name(), line, col, err)\n    }\n    return err\n}\n    (This is a slightly simplified version of some actual code from the Camlistore project.)\n    The error interface requires only a Error method; specific error implementations might have additional methods. For instance, the net package returns errors of type error, following the usual convention, but some of the error implementations have additional methods defined by the net.Error interface:\n  package net\ntype Error interface {\n    error\n    Timeout() bool   // Is the error a timeout?\n    Temporary() bool // Is the error temporary?\n}\n    Client code can test for a net.Error with a type assertion and then distinguish transient network errors from permanent ones. For instance, a web crawler might sleep and retry when it encounters a temporary error and give up otherwise.\n  if nerr, ok := err.(net.Error); ok &amp;&amp; nerr.Temporary() {\n    time.Sleep(1e9)\n    continue\n}\nif err != nil {\n    log.Fatal(err)\n}\n  Simplifying repetitive error handling\n    In Go, error handling is important. The language\'s design and conventions encourage you to explicitly check for errors where they occur (as distinct from the convention in other languages of throwing exceptions and sometimes catching them). In some cases this makes Go code verbose, but fortunately there are some techniques you can use to minimize repetitive error handling.\n    Consider an App Engine application with an HTTP handler that retrieves a record from the datastore and formats it with a template.\n  func init() {\n    http.HandleFunc(\"/view\", viewRecord)\n}\nfunc viewRecord(w http.ResponseWriter, r *http.Request) {\n    c := appengine.NewContext(r)\n    key := datastore.NewKey(c, \"Record\", r.FormValue(\"id\"), 0, nil)\n    record := new(Record)\n    if err := datastore.Get(c, key, record); err != nil {\n        http.Error(w, err.Error(), 500)\n        return\n    }\n    if err := viewTemplate.Execute(w, record); err != nil {\n        http.Error(w, err.Error(), 500)\n    }\n}\n    This function handles errors returned by the datastore.Get function and viewTemplate\'s Execute method. In both cases, it presents a simple error message to the user with the HTTP status code 500 (\"Internal Server Error\"). This looks like a manageable amount of code, but add some more HTTP handlers and you quickly end up with many copies of identical error handling code.\n    To reduce the repetition we can define our own HTTP appHandler type that includes an error return value:\n  type appHandler func(http.ResponseWriter, *http.Request) error\n    Then we can change our viewRecord function to return errors:\n  func viewRecord(w http.ResponseWriter, r *http.Request) error {\n    c := appengine.NewContext(r)\n    key := datastore.NewKey(c, \"Record\", r.FormValue(\"id\"), 0, nil)\n    record := new(Record)\n    if err := datastore.Get(c, key, record); err != nil {\n        return err\n    }\n    return viewTemplate.Execute(w, record)\n}\n    This is simpler than the original version, but the http package doesn\'t understand functions that return error. To fix this we can implement the http.Handler interface\'s ServeHTTP method on appHandler:\n  func (fn appHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n    if err := fn(w, r); err != nil {\n        http.Error(w, err.Error(), 500)\n    }\n}\n    The ServeHTTP method calls the appHandler function and displays the returned error (if any) to the user.  Notice that the method\'s receiver, fn, is a function. (Go can do that!) The method invokes the function by calling the receiver in the expression fn(w, r).\n    Now when registering viewRecord with the http package we use the Handle function (instead of HandleFunc) as appHandler is an http.Handler (not an http.HandlerFunc).\n  func init() {\n    http.Handle(\"/view\", appHandler(viewRecord))\n}\n    With this basic error handling infrastructure in place, we can make it more user friendly. Rather than just displaying the error string, it would be better to give the user a simple error message with an appropriate HTTP status code, while logging the full error to the App Engine developer console for debugging purposes.\n    To do this we create an appError struct containing an error and some other fields:\n  type appError struct {\n    Error   error\n    Message string\n    Code    int\n}\n    Next we modify the appHandler type to return *appError values:\n  type appHandler func(http.ResponseWriter, *http.Request) *appError\n    (It\'s usually a mistake to pass back the concrete type of an error rather than error, for reasons discussed in the Go FAQ, but it\'s the right thing to do here because ServeHTTP is the only place that sees the value and uses its contents.)\n    And make appHandler\'s ServeHTTP method display the appError\'s Message to the user with the correct HTTP status Code and log the full Error to the developer console:\n  func (fn appHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n    if e := fn(w, r); e != nil { // e is *appError, not os.Error.\n        c := appengine.NewContext(r)\n        c.Errorf(\"%v\", e.Error)\n        http.Error(w, e.Message, e.Code)\n    }\n}\n    Finally, we update viewRecord to the new function signature and have it return more context when it encounters an error:\n  func viewRecord(w http.ResponseWriter, r *http.Request) *appError {\n    c := appengine.NewContext(r)\n    key := datastore.NewKey(c, \"Record\", r.FormValue(\"id\"), 0, nil)\n    record := new(Record)\n    if err := datastore.Get(c, key, record); err != nil {\n        return &amp;appError{err, \"Record not found\", 404}\n    }\n    if err := viewTemplate.Execute(w, record); err != nil {\n        return &amp;appError{err, \"Can\'t display record\", 500}\n    }\n    return nil\n}\n    This version of viewRecord is the same length as the original, but now each of those lines has specific meaning and we are providing a friendlier user experience.\n    It doesn\'t end there; we can further improve the error handling in our application. Some ideas:\n    give the error handler a pretty HTML template,\n    make debugging easier by writing the stack trace to the HTTP response when the user is an administrator,\n    write a constructor function for appError that stores the stack trace for easier debugging,\n    recover from panics inside the appHandler, logging the error to the console as \"Critical,\" while telling the user \"a serious error has occurred.\" This is a nice touch to avoid exposing the user to inscrutable error messages caused by programming errors. See the Defer, Panic, and Recover article for more details.\n  Conclusion\n    Proper error handling is an essential requirement of good software. By employing the techniques described in this post you should be able to write more reliable and succinct Go code.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/error-handling-and-go','2011-07-12','2017-05-07 13:50:03.430473'),(556,'The Laws of Reflection','Rob Pike','		The Laws of Reflection\n		6 September 2011\n		\n  Introduction\n    Reflection in computing is the ability of a program to examine its own structure, particularly through types; it\'s a form of metaprogramming. It\'s also a great source of confusion.\n    In this article we attempt to clarify things by explaining how reflection works in Go. Each language\'s reflection model is different (and many languages don\'t support it at all), but this article is about Go, so for the rest of this article the word \"reflection\" should be taken to mean \"reflection in Go\".\n  Types and interfaces\n    Because reflection builds on the type system, let\'s start with a refresher about types in Go.\n    Go is statically typed. Every variable has a static type, that is, exactly one type known and fixed at compile time: int, float32, *MyType, []byte, and so on. If we declare\n  type MyInt int\nvar i int\nvar j MyInt\n    then i has type int and j has type MyInt. The variables i and j have distinct static types and, although they have the same underlying type, they cannot be assigned to one another without a conversion.\n    One important category of type is interface types, which represent fixed sets of methods. An interface variable can store any concrete (non-interface) value as long as that value implements the interface\'s methods. A well-known pair of examples is io.Reader and io.Writer, the types Reader and Writer from the io package:\n  // Reader is the interface that wraps the basic Read method.\ntype Reader interface {\n    Read(p []byte) (n int, err error)\n}\n// Writer is the interface that wraps the basic Write method.\ntype Writer interface {\n    Write(p []byte) (n int, err error)\n}\n    Any type that implements a Read (or Write) method with this signature is said to implement io.Reader (or io.Writer). For the purposes of this discussion, that means that a variable of type io.Reader can hold any value whose type has a Read method:\n  var r io.Reader\nr = os.Stdin\nr = bufio.NewReader(r)\nr = new(bytes.Buffer)\n// and so on\n    It\'s important to be clear that whatever concrete value r may hold, r\'s type is always io.Reader: Go is statically typed and the static type of r is io.Reader.\n    An extremely important example of an interface type is the empty interface:\n  interface{}\n    It represents the empty set of methods and is satisfied by any value at all, since any value has zero or more methods.\n    Some people say that Go\'s interfaces are dynamically typed, but that is misleading. They are statically typed: a variable of interface type always has the same static type, and even though at run time the value stored in the interface variable may change type, that value will always satisfy the interface.\n    We need to be precise about all this because reflection and interfaces are closely related.\n  The representation of an interface\n    Russ Cox has written a  detailed blog post about the representation of interface values in Go. It\'s not necessary to repeat the full story here, but a simplified summary is in order.\n    A variable of interface type stores a pair: the concrete value assigned to the variable, and that value\'s type descriptor. To be more precise, the value is the underlying concrete data item that implements the interface and the type describes the full type of that item. For instance, after\n  var r io.Reader\ntty, err := os.OpenFile(\"/dev/tty\", os.O_RDWR, 0)\nif err != nil {\n    return nil, err\n}\nr = tty\n    r contains, schematically, the (value, type) pair, (tty, *os.File). Notice that the type *os.File implements methods other than Read; even though the interface value provides access only to the Read method, the value inside carries all the type information about that value. That\'s why we can do things like this:\n  var w io.Writer\nw = r.(io.Writer)\n    The expression in this assignment is a type assertion; what it asserts is that the item inside r also implements io.Writer, and so we can assign it to w. After the assignment, w will contain the pair (tty, *os.File). That\'s the same pair as was held in r. The static type of the interface determines what methods may be invoked with an interface variable, even though the concrete value inside may have a larger set of methods.\n    Continuing, we can do this:\n  var empty interface{}\nempty = w\n    and our empty interface value empty will again contain that same pair, (tty, *os.File). That\'s handy: an empty interface can hold any value and contains all the information we could ever need about that value.\n    (We don\'t need a type assertion here because it\'s known statically that w satisfies the empty interface. In the example where we moved a value from a Reader to a Writer, we needed to be explicit and use a type assertion because Writer\'s methods are not a subset of Reader\'s.)\n    One important detail is that the pair inside an interface always has the form (value, concrete type) and cannot have the form (value, interface type). Interfaces do not hold interface values.\n    Now we\'re ready to reflect.\n  The first law of reflection\n  1. Reflection goes from interface value to reflection object.\n    At the basic level, reflection is just a mechanism to examine the type and value pair stored inside an interface variable. To get started, there are two types we need to know about in package reflect: Type and Value. Those two types give access to the contents of an interface variable, and two simple functions, called reflect.TypeOf and reflect.ValueOf, retrieve reflect.Type and reflect.Value pieces out of an interface value. (Also, from the reflect.Value it\'s easy to get to the reflect.Type, but let\'s keep the Value and Type concepts separate for now.)\n    Let\'s start with TypeOf:\n  package main\nimport (\n    \"fmt\"\n    \"reflect\"\n)\nfunc main() {\n    var x float64 = 3.4\n    fmt.Println(\"type:\", reflect.TypeOf(x))\n}\n    This program prints\n  type: float64\n    You might be wondering where the interface is here, since the program looks like it\'s passing the float64 variable x, not an interface value, to reflect.TypeOf. But it\'s there; as godoc reports, the signature of reflect.TypeOf includes an empty interface:\n  // TypeOf returns the reflection Type of the value in the interface{}.\nfunc TypeOf(i interface{}) Type\n    When we call reflect.TypeOf(x), x is first stored in an empty interface, which is then passed as the argument; reflect.TypeOf unpacks that empty interface to recover the type information.\n    The reflect.ValueOf function, of course, recovers the value (from here on we\'ll elide the boilerplate and focus just on the executable code):\n  var x float64 = 3.4\nfmt.Println(\"value:\", reflect.ValueOf(x).String())\n    prints\n  value: &lt;float64 Value&gt;\n    (We call the String method explicitly because by default the fmt package digs into a reflect.Value to show the concrete value inside.\n    The String method does not.)\n    Both reflect.Type and reflect.Value have lots of methods to let us examine and manipulate them. One important example is that Value has a Type method that returns the Type of a reflect.Value. Another is that both Type and Value have a Kind method that returns a constant indicating what sort of item is stored: Uint, Float64, Slice, and so on. Also methods on Value with names like Int and Float let us grab values (as int64 and float64) stored inside:\n  var x float64 = 3.4\nv := reflect.ValueOf(x)\nfmt.Println(\"type:\", v.Type())\nfmt.Println(\"kind is float64:\", v.Kind() == reflect.Float64)\nfmt.Println(\"value:\", v.Float())\n    prints\n  type: float64\nkind is float64: true\nvalue: 3.4\n    There are also methods like SetInt and SetFloat but to use them we need to understand settability, the subject of the third law of reflection, discussed below.\n    The reflection library has a couple of properties worth singling out. First, to keep the API simple, the \"getter\" and \"setter\" methods of Value operate on the largest type that can hold the value: int64 for all the signed integers, for instance. That is, the Int method of Value returns an int64 and the SetInt value takes an int64; it may be necessary to convert to the actual type involved:\n  var x uint8 = \'x\'\nv := reflect.ValueOf(x)\nfmt.Println(\"type:\", v.Type())                            // uint8.\nfmt.Println(\"kind is uint8: \", v.Kind() == reflect.Uint8) // true.\nx = uint8(v.Uint())                                       // v.Uint returns a uint64.\n    The second property is that the Kind of a reflection object describes the underlying type, not the static type. If a reflection object contains a value of a user-defined integer type, as in\n  type MyInt int\nvar x MyInt = 7\nv := reflect.ValueOf(x)\n    the Kind of v is still reflect.Int, even though the static type of x is MyInt, not int. In other words, the Kind cannot discriminate an int from a MyInt even though the Type can.\n  The second law of reflection\n  2. Reflection goes from reflection object to interface value.\n    Like physical reflection, reflection in Go generates its own inverse.\n    Given a reflect.Value we can recover an interface value using the Interface method; in effect the method packs the type and value information back into an interface representation and returns the result:\n  // Interface returns v\'s value as an interface{}.\nfunc (v Value) Interface() interface{}\n    As a consequence we can say\n  y := v.Interface().(float64) // y will have type float64.\nfmt.Println(y)\n    to print the float64 value represented by the reflection object v.\n    We can do even better, though. The arguments to fmt.Println, fmt.Printf and so on are all passed as empty interface values, which are then unpacked by the fmt package internally just as we have been doing in the previous examples. Therefore all it takes to print the contents of a reflect.Value correctly is to pass the result of the Interface method to the formatted print routine:\n  fmt.Println(v.Interface())\n    (Why not fmt.Println(v)? Because v is a reflect.Value; we want the concrete value it holds.) Since our value is a float64, we can even use a floating-point format if we want:\n  fmt.Printf(\"value is %7.1e\\n\", v.Interface())\n    and get in this case\n  3.4e+00\n    Again, there\'s no need to type-assert the result of v.Interface() to float64; the empty interface value has the concrete value\'s type information inside and Printf will recover it.\n    In short, the Interface method is the inverse of the ValueOf function, except that its result is always of static type interface{}.\n    Reiterating: Reflection goes from interface values to reflection objects and back again.\n  The third law of reflection\n  3. To modify a reflection object, the value must be settable.\n    The third law is the most subtle and confusing, but it\'s easy enough to understand if we start from first principles.\n    Here is some code that does not work, but is worth studying.\n  var x float64 = 3.4\nv := reflect.ValueOf(x)\nv.SetFloat(7.1) // Error: will panic.\n    If you run this code, it will panic with the cryptic message\n  panic: reflect.Value.SetFloat using unaddressable value\n    The problem is not that the value 7.1 is not addressable; it\'s that v is not settable. Settability is a property of a reflection Value, and not all reflection Values have it.\n    The CanSet method of Value reports the settability of a Value; in our case,\n  var x float64 = 3.4\nv := reflect.ValueOf(x)\nfmt.Println(\"settability of v:\", v.CanSet())\n    prints\n  settability of v: false\n    It is an error to call a Set method on an non-settable Value. But what is settability?\n    Settability is a bit like addressability, but stricter. It\'s the property that a reflection object can modify the actual storage that was used to create the reflection object. Settability is determined by whether the reflection object holds the original item. When we say\n  var x float64 = 3.4\nv := reflect.ValueOf(x)\n    we pass a copy of x to reflect.ValueOf, so the interface value created as the argument to reflect.ValueOf is a copy of x, not x itself. Thus, if the statement\n  v.SetFloat(7.1)\n    were allowed to succeed, it would not update x, even though v looks like it was created from x. Instead, it would update the copy of x stored inside the reflection value and x itself would be unaffected. That would be confusing and useless, so it is illegal, and settability is the property used to avoid this issue.\n    If this seems bizarre, it\'s not. It\'s actually a familiar situation in unusual garb. Think of passing x to a function:\n  f(x)\n    We would not expect f to be able to modify x because we passed a copy of x\'s value, not x itself. If we want f to modify x directly we must pass our function the address of x (that is, a pointer to x):\n  f(&amp;x)\n    This is straightforward and familiar, and reflection works the same way. If we want to modify x by reflection, we must give the reflection library a pointer to the value we want to modify.\n    Let\'s do that. First we initialize x as usual and then create a reflection value that points to it, called p.\n  var x float64 = 3.4\np := reflect.ValueOf(&amp;x) // Note: take the address of x.\nfmt.Println(\"type of p:\", p.Type())\nfmt.Println(\"settability of p:\", p.CanSet())\n    The output so far is\n  type of p: *float64\nsettability of p: false\n    The reflection object p isn\'t settable, but it\'s not p we want to set, it\'s (in effect) *p. To get to what p points to, we call the Elem method of Value, which indirects through the pointer, and save the result in a reflection Value called v:\n  v := p.Elem()\nfmt.Println(\"settability of v:\", v.CanSet())\n    Now v is a settable reflection object, as the output demonstrates,\n  settability of v: true\n    and since it represents x, we are finally able to use v.SetFloat to modify the value of x:\n  v.SetFloat(7.1)\nfmt.Println(v.Interface())\nfmt.Println(x)\n    The output, as expected, is\n  7.1\n7.1\n    Reflection can be hard to understand but it\'s doing exactly what the language does, albeit through reflection Types and Values that can disguise what\'s going on. Just keep in mind that reflection Values need the address of something in order to modify what they represent.\n  Structs\n    In our previous example v wasn\'t a pointer itself, it was just derived from one. A common way for this situation to arise is when using reflection to modify the fields of a structure. As long as we have the address of the structure, we can modify its fields.\n    Here\'s a simple example that analyzes a struct value, t. We create the reflection object with the address of the struct because we\'ll want to modify it later. Then we set typeOfT to its type and iterate over the fields using straightforward method calls (see package reflect for details). Note that we extract the names of the fields from the struct type, but the fields themselves are regular reflect.Value objects.\n  type T struct {\n    A int\n    B string\n}\nt := T{23, \"skidoo\"}\ns := reflect.ValueOf(&amp;t).Elem()\ntypeOfT := s.Type()\nfor i := 0; i &lt; s.NumField(); i++ {\n    f := s.Field(i)\n    fmt.Printf(\"%d: %s %s = %v\\n\", i,\n        typeOfT.Field(i).Name, f.Type(), f.Interface())\n}\n    The output of this program is\n  0: A int = 23\n1: B string = skidoo\n    There\'s one more point about settability introduced in passing here: the field names of T are upper case (exported) because only exported fields of a struct are settable.\n    Because s contains a settable reflection object, we can modify the fields of the structure.\n  s.Field(0).SetInt(77)\ns.Field(1).SetString(\"Sunset Strip\")\nfmt.Println(\"t is now\", t)\n    And here\'s the result:\n  t is now {77 Sunset Strip}\n    If we modified the program so that s was created from t, not &amp;t, the calls to SetInt and SetString would fail as the fields of t would not be settable.\n  Conclusion\n    Here again are the laws of reflection:\n    Reflection goes from interface value to reflection object.\n    Reflection goes from reflection object to interface value.\n    To modify a reflection object, the value must be settable.\n    Once you understand these laws reflection in Go becomes much easier to use, although it remains subtle. It\'s a powerful tool that should be used with care and avoided unless strictly necessary.\n    There\'s plenty more to reflection that we haven\'t covered — sending and receiving on channels, allocating memory, using slices and maps, calling methods and functions — but this post is long enough. We\'ll cover some of those topics in a later article.\n		\n			By Rob Pike\n		\n	','https://blog.golang.org/laws-of-reflection','2011-09-06','2017-05-07 13:50:03.469381'),(557,'The Go image package','Nigel Tao','		The Go image package\n		21 September 2011\n		\n  Introduction\n    The image and image/color packages define a number of types: color.Color and color.Model describe colors, image.Point and image.Rectangle describe basic 2-D geometry, and image.Image brings the two concepts together to represent a rectangular grid of colors. A separate article covers image composition with the image/draw package.\n  Colors and Color Models\n    Color is an interface that defines the minimal method set of any type that can be considered a color: one that can be converted to red, green, blue and alpha values. The conversion may be lossy, such as converting from CMYK or YCbCr color spaces.\n  type Color interface {\n    // RGBA returns the alpha-premultiplied red, green, blue and alpha values\n    // for the color. Each value ranges within [0, 0xFFFF], but is represented\n    // by a uint32 so that multiplying by a blend factor up to 0xFFFF will not\n    // overflow.\n    RGBA() (r, g, b, a uint32)\n}\n    There are three important subtleties about the return values. First, the red, green and blue are alpha-premultiplied: a fully saturated red that is also 25% transparent is represented by RGBA returning a 75% r. Second, the channels have a 16-bit effective range: 100% red is represented by RGBA returning an r of 65535, not 255, so that converting from CMYK or YCbCr is not as lossy. Third, the type returned is uint32, even though the maximum value is 65535, to guarantee that multiplying two values together won\'t overflow. Such multiplications occur when blending two colors according to an alpha mask from a third color, in the style of Porter and Duff\'s classic algebra:\n  dstr, dstg, dstb, dsta := dst.RGBA()\nsrcr, srcg, srcb, srca := src.RGBA()\n_, _, _, m := mask.RGBA()\nconst M = 1&lt;&lt;16 - 1\n// The resultant red value is a blend of dstr and srcr, and ranges in [0, M].\n// The calculation for green, blue and alpha is similar.\ndstr = (dstr*(M-m) + srcr*m) / M\n    The last line of that code snippet would have been more complicated if we worked with non-alpha-premultiplied colors, which is why Color uses alpha-premultiplied values.\n    The image/color package also defines a number of concrete types that implement the Color interface. For example, RGBA is a struct that represents the classic \"8 bits per channel\" color.\n  type RGBA struct {\n    R, G, B, A uint8\n}\n    Note that the R field of an RGBA is an 8-bit alpha-premultiplied color in the range [0, 255]. RGBA satisfies the Color interface by multiplying that value by 0x101 to generate a 16-bit alpha-premultiplied color in the range [0, 65535]. Similarly, the NRGBA struct type represents an 8-bit non-alpha-premultiplied color, as used by the PNG image format. When manipulating an NRGBA\'s fields directly, the values are non-alpha-premultiplied, but when calling the RGBA method, the return values are alpha-premultiplied.\n    A Model is simply something that can convert `Color`s to other `Color`s, possibly lossily. For example, the GrayModel can convert any Color to a desaturated Gray. A Palette can convert any Color to one from a limited palette.\n  type Model interface {\n    Convert(c Color) Color\n}\ntype Palette []Color\n  Points and Rectangles\n    A Point is an (x, y) co-ordinate on the integer grid, with axes increasing right and down. It is neither a pixel nor a grid square. A Point has no intrinsic width, height or color, but the visualizations below use a small colored square.\n  type Point struct {\n    X, Y int\n}\n  p := image.Point{2, 1}\n    A Rectangle is an axis-aligned rectangle on the integer grid, defined by its top-left and bottom-right Point.  A Rectangle also has no intrinsic color, but the visualizations below outline rectangles with a thin colored line, and call out their Min and Max `Point`s.\n  type Rectangle struct {\n    Min, Max Point\n}\n    For convenience, image.Rect(x0, y0, x1, y1) is equivalent to image.Rectangle{image.Point{x0, y0}, image.Point{x1, y1}}, but is much easier to type.\n    A Rectangle is inclusive at the top-left and exclusive at the bottom-right. For a Point p and a Rectangle r, p.In(r) if and only if r.Min.X &lt;= p.X &amp;&amp; p.X &lt; r.Max.X, and similarly for Y. This is analogous to how a slice s[i0:i1] is inclusive at the low end and exclusive at the high end. (Unlike arrays and slices, a Rectangle often has a non-zero origin.)\n  r := image.Rect(2, 1, 5, 5)\n// Dx and Dy return a rectangle\'s width and height.\nfmt.Println(r.Dx(), r.Dy(), image.Pt(0, 0).In(r)) // prints 3 4 false\n    Adding a Point to a Rectangle translates the Rectangle. Points and Rectangles are not restricted to be in the bottom-right quadrant.\n  r := image.Rect(2, 1, 5, 5).Add(image.Pt(-4, -2))\nfmt.Println(r.Dx(), r.Dy(), image.Pt(0, 0).In(r)) // prints 3 4 true\n    Intersecting two Rectangles yields another Rectangle, which may be empty.\n  r := image.Rect(0, 0, 4, 3).Intersect(image.Rect(2, 2, 5, 5))\n// Size returns a rectangle\'s width and height, as a Point.\nfmt.Printf(\"%#v\\n\", r.Size()) // prints image.Point{X:2, Y:1}\n    Points and Rectangles are passed and returned by value. A function that takes a Rectangle argument will be as efficient as a function that takes two Point arguments, or four int arguments.\n  Images\n    An Image maps every grid square in a Rectangle to a Color from a Model. \"The pixel at (x, y)\" refers to the color of the grid square defined by the points (x, y), (x+1, y), (x+1, y+1) and (x, y+1).\n  type Image interface {\n    // ColorModel returns the Image\'s color model.\n    ColorModel() color.Model\n    // Bounds returns the domain for which At can return non-zero color.\n    // The bounds do not necessarily contain the point (0, 0).\n    Bounds() Rectangle\n    // At returns the color of the pixel at (x, y).\n    // At(Bounds().Min.X, Bounds().Min.Y) returns the upper-left pixel of the grid.\n    // At(Bounds().Max.X-1, Bounds().Max.Y-1) returns the lower-right one.\n    At(x, y int) color.Color\n}\n    A common mistake is assuming that an Image\'s bounds start at (0, 0). For example, an animated GIF contains a sequence of Images, and each Image after the first typically only holds pixel data for the area that changed, and that area doesn\'t necessarily start at (0, 0). The correct way to iterate over an Image m\'s pixels looks like:\n  b := m.Bounds()\nfor y := b.Min.Y; y &lt; b.Max.Y; y++ {\n for x := b.Min.X; x &lt; b.Max.X; x++ {\n  doStuffWith(m.At(x, y))\n }\n}\n    Image implementations do not have to be based on an in-memory slice of pixel data. For example, a Uniform is an Image of enormous bounds and uniform color, whose in-memory representation is simply that color.\n  type Uniform struct {\n    C color.Color\n}\n    Typically, though, programs will want an image based on a slice. Struct types like RGBA and Gray (which other packages refer to as image.RGBA and image.Gray) hold slices of pixel data and implement the Image interface.\n  type RGBA struct {\n    // Pix holds the image\'s pixels, in R, G, B, A order. The pixel at\n    // (x, y) starts at Pix[(y-Rect.Min.Y)*Stride + (x-Rect.Min.X)*4].\n    Pix []uint8\n    // Stride is the Pix stride (in bytes) between vertically adjacent pixels.\n    Stride int\n    // Rect is the image\'s bounds.\n    Rect Rectangle\n}\n    These types also provide a Set(x, y int, c color.Color) method that allows modifying the image one pixel at a time.\n  m := image.NewRGBA(image.Rect(0, 0, 640, 480))\nm.Set(5, 5, color.RGBA{255, 0, 0, 255})\n    If you\'re reading or writing a lot of pixel data, it can be more efficient, but more complicated, to access these struct type\'s Pix field directly.\n    The slice-based Image implementations also provide a SubImage method, which returns an Image backed by the same array. Modifying the pixels of a sub-image will affect the pixels of the original image, analogous to how modifying the contents of a sub-slice s[i0:i1] will affect the contents of the original slice s.\n  m0 := image.NewRGBA(image.Rect(0, 0, 8, 5))\nm1 := m0.SubImage(image.Rect(1, 2, 5, 5)).(*image.RGBA)\nfmt.Println(m0.Bounds().Dx(), m1.Bounds().Dx()) // prints 8, 4\nfmt.Println(m0.Stride == m1.Stride)             // prints true\n    For low-level code that works on an image\'s Pix field, be aware that ranging over Pix can affect pixels outside an image\'s bounds. In the example above, the pixels covered by m1.Pix are shaded in blue. Higher-level code, such as the At and Set methods or the image/draw package, will clip their operations to the image\'s bounds.\n  Image Formats\n    The standard package library supports a number of common image formats, such as GIF, JPEG and PNG. If you know the format of a source image file, you can decode from an io.Reader directly.\n  import (\n \"image/jpeg\"\n \"image/png\"\n \"io\"\n)\n// convertJPEGToPNG converts from JPEG to PNG.\nfunc convertJPEGToPNG(w io.Writer, r io.Reader) error {\n img, err := jpeg.Decode(r)\n if err != nil {\n  return err\n }\n return png.Encode(w, img)\n}\n    If you have image data of unknown format, the image.Decode function can detect the format. The set of recognized formats is constructed at run time and is not limited to those in the standard package library. An image format package typically registers its format in an init function, and the main package will \"underscore import\" such a package solely for the side effect of format registration.\n  import (\n \"image\"\n \"image/png\"\n \"io\"\n _ \"code.google.com/p/vp8-go/webp\"\n _ \"image/jpeg\"\n)\n// convertToPNG converts from any recognized format to PNG.\nfunc convertToPNG(w io.Writer, r io.Reader) error {\n img, _, err := image.Decode(r)\n if err != nil {\n  return err\n }\n return png.Encode(w, img)\n}\n		\n			By Nigel Tao\n		\n	','https://blog.golang.org/go-image-package','2011-09-21','2017-05-07 13:50:03.503217'),(558,'The Go image/draw package','Nigel Tao','		The Go image/draw package\n		29 September 2011\n		\n  Introduction\n    Package image/draw defines only one operation: drawing a source image onto a destination image, through an optional mask image. This one operation is surprisingly versatile and can perform a number of common image manipulation tasks elegantly and efficiently.\n    Composition is performed pixel by pixel in the style of the Plan 9 graphics library and the X Render extension. The model is based on the classic \"Compositing Digital Images\" paper by Porter and Duff, with an additional mask parameter: dst = (src IN mask) OP dst. For a fully opaque mask, this reduces to the original Porter-Duff formula: dst = src OP dst. In Go, a nil mask image is equivalent to an infinitely sized, fully opaque mask image.\n    The Porter-Duff paper presented 12 different composition operators, but with an explicit mask, only 2 of these are needed in practice: source-over-destination and source. In Go, these operators are represented by the Over and Src constants. The Over operator performs the natural layering of a source image over a destination image: the change to the destination image is smaller where the source (after masking) is more transparent (that is, has lower alpha). The Src operator merely copies the source (after masking) with no regard for the destination image\'s original content. For fully opaque source and mask images, the two operators produce the same output, but the Src operator is usually faster.\n  Geometric Alignment\n    Composition requires associating destination pixels with source and mask pixels. Obviously, this requires destination, source and mask images, and a composition operator, but it also requires specifying what rectangle of each image to use. Not every drawing should write to the entire destination: when updating an animating image, it is more efficient to only draw the parts of the image that have changed. Not every drawing should read from the entire source: when using a sprite that combines many small images into one large one, only a part of the image is needed. Not every drawing should read from the entire mask: a mask image that collects a font\'s glyphs is similar to a sprite. Thus, drawing also needs to know three rectangles, one for each image. Since each rectangle has the same width and height, it suffices to pass a destination rectangle r and two points sp and mp: the source rectangle is equal to r translated so that r.Min in the destination image aligns with sp in the source image, and similarly for mp. The effective rectangle is also clipped to each image\'s bounds in their respective co-ordinate space.\n    The DrawMask function takes seven arguments, but an explicit mask and mask-point are usually unnecessary, so the Draw function takes five:\n  // Draw calls DrawMask with a nil mask.\nfunc Draw(dst Image, r image.Rectangle, src image.Image, sp image.Point, op Op)\nfunc DrawMask(dst Image, r image.Rectangle, src image.Image, sp image.Point,\n mask image.Image, mp image.Point, op Op)\n    The destination image must be mutable, so the image/draw package defines a draw.Image interface which has a Set method.\n  type Image interface {\n    image.Image\n    Set(x, y int, c color.Color)\n}\n  Filling a Rectangle\n    To fill a rectangle with a solid color, use an image.Uniform source. The ColorImage type re-interprets a Color as a practically infinite-sized Image of that color. For those familiar with the design of Plan 9\'s draw library, there is no need for an explicit \"repeat bit\" in Go\'s slice-based image types; the concept is subsumed by Uniform.\n  // image.ZP is the zero point -- the origin.\ndraw.Draw(dst, r, &amp;image.Uniform{c}, image.ZP, draw.Src)\n    To initialize a new image to all-blue:\n  m := image.NewRGBA(image.Rect(0, 0, 640, 480))\nblue := color.RGBA{0, 0, 255, 255}\ndraw.Draw(m, m.Bounds(), &amp;image.Uniform{blue}, image.ZP, draw.Src)\n    To reset an image to transparent (or black, if the destination image\'s color model cannot represent transparency), use image.Transparent, which is an image.Uniform:\n  draw.Draw(m, m.Bounds(), image.Transparent, image.ZP, draw.Src)\n  Copying an Image\n    To copy from a rectangle sr in the source image to a rectangle starting at a point dp in the destination, convert the source rectangle into the destination image\'s co-ordinate space:\n  r := image.Rectangle{dp, dp.Add(sr.Size())}\ndraw.Draw(dst, r, src, sr.Min, draw.Src)\n    Alternatively:\n  r := sr.Sub(sr.Min).Add(dp)\ndraw.Draw(dst, r, src, sr.Min, draw.Src)\n    To copy the entire source image, use sr = src.Bounds().\n  Scrolling an Image\n    Scrolling an image is just copying an image to itself, with different destination and source rectangles. Overlapping destination and source images are perfectly valid, just as Go\'s built-in copy function can handle overlapping destination and source slices. To scroll an image m by 20 pixels:\n  b := m.Bounds()\np := image.Pt(0, 20)\n// Note that even though the second argument is b,\n// the effective rectangle is smaller due to clipping.\ndraw.Draw(m, b, m, b.Min.Add(p), draw.Src)\ndirtyRect := b.Intersect(image.Rect(b.Min.X, b.Max.Y-20, b.Max.X, b.Max.Y))\n  Converting an Image to RGBA\n    The result of decoding an image format might not be an image.RGBA: decoding a GIF results in an image.Paletted, decoding a JPEG results in a ycbcr.YCbCr, and the result of decoding a PNG depends on the image data. To convert any image to an image.RGBA:\n  b := src.Bounds()\nm := image.NewRGBA(image.Rect(0, 0, b.Dx(), b.Dy()))\ndraw.Draw(m, m.Bounds(), src, b.Min, draw.Src)\n  Drawing Through a Mask\n    To draw an image through a circular mask with center p and radius r:\n  type circle struct {\n    p image.Point\n    r int\n}\nfunc (c *circle) ColorModel() color.Model {\n    return color.AlphaModel\n}\nfunc (c *circle) Bounds() image.Rectangle {\n    return image.Rect(c.p.X-c.r, c.p.Y-c.r, c.p.X+c.r, c.p.Y+c.r)\n}\nfunc (c *circle) At(x, y int) color.Color {\n    xx, yy, rr := float64(x-c.p.X)+0.5, float64(y-c.p.Y)+0.5, float64(c.r)\n    if xx*xx+yy*yy &lt; rr*rr {\n        return color.Alpha{255}\n    }\n    return color.Alpha{0}\n}\n    draw.DrawMask(dst, dst.Bounds(), src, image.ZP, &amp;circle{p, r}, image.ZP, draw.Over)\n  Drawing Font Glyphs\n    To draw a font glyph in blue starting from a point p, draw with an image.ColorImage source and an image.Alpha mask. For simplicity, we aren\'t performing any sub-pixel positioning or rendering, or correcting for a font\'s height above a baseline.\n  src := &amp;image.Uniform{color.RGBA{0, 0, 255, 255}}\nmask := theGlyphImageForAFont()\nmr := theBoundsFor(glyphIndex)\ndraw.DrawMask(dst, mr.Sub(mr.Min).Add(p), src, image.ZP, mask, mr.Min, draw.Over)\n  Performance\n    The image/draw package implementation demonstrates how to provide an image manipulation function that is both general purpose, yet efficient for common cases. The DrawMask function takes arguments of interface types, but immediately makes type assertions that its arguments are of specific struct types, corresponding to common operations like drawing one image.RGBA image onto another, or drawing an image.Alpha mask (such as a font glyph) onto an image.RGBA image. If a type assertion succeeds, that type information is used to run a specialized implementation of the general algorithm. If the assertions fail, the fallback code path uses the generic At and Set methods. The fast-paths are purely a performance optimization; the resultant destination image is the same either way. In practice, only a small number of special cases are necessary to support typical applications.\n		\n			By Nigel Tao\n		\n	','https://blog.golang.org/go-imagedraw-package','2011-09-29','2017-05-07 13:50:03.537282'),(559,'Godoc: documenting Go code','Andrew Gerrand','		Godoc: documenting Go code\n		31 March 2011\n		\n    The Go project takes documentation seriously. Documentation is a huge part of making software accessible and maintainable. Of course it must be well-written and accurate, but it also must be easy to write and to maintain. Ideally, it should be coupled to the code itself so the documentation evolves along with the code. The easier it is for programmers to produce good documentation, the better for everyone.\n    To that end, we have developed the godoc documentation tool. This article describes godoc\'s approach to documentation, and explains how you can use our conventions and tools to write good documentation for your own projects.\n    Godoc parses Go source code - including comments - and produces documentation as HTML or plain text. The end result is documentation tightly coupled with the code it documents. For example, through godoc\'s web interface you can navigate from a function\'s documentation to its implementation with one click.\n    Godoc is conceptually related to Python\'s Docstring and Java\'s Javadoc, but its design is simpler. The comments read by godoc are not language constructs (as with Docstring) nor must they have their own machine-readable syntax (as with Javadoc). Godoc comments are just good comments, the sort you would want to read even if godoc didn\'t exist.\n    The convention is simple: to document a type, variable, constant, function, or even a package, write a regular comment directly preceding its declaration, with no intervening blank line. Godoc will then present that comment as text alongside the item it documents. For example, this is the documentation for the fmt package\'s Fprint function:\n  // Fprint formats using the default formats for its operands and writes to w.\n// Spaces are added between operands when neither is a string.\n// It returns the number of bytes written and any write error encountered.\nfunc Fprint(w io.Writer, a ...interface{}) (n int, err error) {\n    Notice this comment is a complete sentence that begins with the name of the element it describes. This important convention allows us to generate documentation in a variety of formats, from plain text to HTML to UNIX man pages, and makes it read better when tools truncate it for brevity, such as when they extract the first line or sentence.\n    Comments on package declarations should provide general package documentation. These comments can be short, like the sort package\'s brief description:\n  // Package sort provides primitives for sorting slices and user-defined\n// collections.\npackage sort\n    They can also be detailed like the gob package\'s overview. That package uses another convention for packages that need large amounts of introductory documentation: the package comment is placed in its own file, doc.go, which contains only those comments and a package clause.\n    When writing package comments of any size, keep in mind that their first sentence will appear in godoc\'s package list.\n    Comments that are not adjacent to a top-level declaration are omitted from godoc\'s output, with one notable exception. Top-level comments that begin with the word \"BUG(who)” are recognized as known bugs, and included in the \"Bugs” section of the package documentation. The \"who” part should be the user name of someone who could provide more information. For example, this is a known issue from the bytes package:\n  // BUG(r): The rule Title uses for word boundaries does not handle Unicode punctuation properly.\n    Sometimes a struct field, function, type, or even a whole package becomes\n    redundant or unnecessary, but must be kept for compatibility with existing\n    programs.\n    To signal that an identifier should not be used, add a paragraph to its doc\n    comment that begins with \"Deprecated:\" followed by some information about the\n    deprecation.\n    There are a such few examples in the standard library.\n    There are a few formatting rules that Godoc uses when converting comments to HTML:\n    Subsequent lines of text are considered part of the same paragraph; you must leave a blank line to separate paragraphs.\n    Pre-formatted text must be indented relative to the surrounding comment text (see gob\'s doc.go for an example).\n    URLs will be converted to HTML links; no special markup is necessary.\n    Note that none of these rules requires you to do anything out of the ordinary.\n    In fact, the best thing about godoc\'s minimal approach is how easy it is to use. As a result, a lot of Go code, including all of the standard library, already follows the conventions.\n    Your own code can present good documentation just by having comments as described above. Any Go packages installed inside $GOROOT/src/pkg and any GOPATH work spaces will already be accessible via godoc\'s command-line and HTTP interfaces, and you can specify additional paths for indexing via the -path flag or just by running \"godoc .\" in the source directory. See the godoc documentation for more details.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/godoc-documenting-go-code','2011-03-31','2017-05-07 13:50:03.565511'),(560,'Organizing Go code','Andrew Gerrand','		Organizing Go code\n		16 August 2012\n		\n  Introduction\n    Go code is organized differently to that of other languages. This post discusses how to name and package the elements of your Go program to best serve its users.\n  Choose good names\n    The names you choose affect how you think about your code, so take care when naming your package and its exported identifiers.\n    A package\'s name provides context for its contents. For instance, the bytes package from the standard library exports the Buffer type. On its own, the name Buffer isn\'t very descriptive, but when combined with its package name its meaning becomes clear: bytes.Buffer. If the package had a less descriptive name, like util, the buffer would likely acquire the longer and clumsier name util.BytesBuffer. \n    Don\'t be shy about renaming things as you work. As you spend time with your program you will better understand how its pieces fit together and, therefore, what their names should be. There\'s no need to lock yourself into early decisions. (The gofmt command has a -r flag that provides a syntax-aware search and replace, making large-scale refactoring easier.)\n    A good name is the most important part of a software interface: the name is the first thing every client of the code will see. A well-chosen name is therefore the starting point for good documentation. Many of the following practices result organically from good naming. \n  Choose a good import path (make your package \"go get\"-able)\n    An import path is the string with which users import a package. It specifies the directory (relative to $GOROOT/src/pkg or $GOPATH/src) in which the package\'s source code resides.\n    Import paths should be globally unique, so use the path of your source repository as its base. For instance, the websocket package from the go.net sub-repository has an import path of \"golang.org/x/net/websocket\". The Go project owns the path \"github.com/golang\", so that path cannot be used by another author for a different package. Because the repository URL and import path are one and the same, the go get command can fetch and install the package automatically.\n    If you don\'t use a hosted source repository, choose some unique prefix such as a domain, company, or project name. As an example, the import path of all Google\'s internal Go code starts with the string \"google\".\n    The last element of the import path is typically the same as the package name. For instance, the import path \"net/http\" contains package http. This is not a requirement - you can make them different if you like - but you should follow the convention for predictability\'s sake: a user might be surprised that import \"foo/bar\" introduces the identifier quux into the package name space.\n    Sometimes people set GOPATH to the root of their source repository and put their packages in directories relative to the repository root, such as \"src/my/package\". On one hand, this keeps the import paths short (\"my/package\" instead of \"github.com/me/project/my/package\"), but on the other it breaks go get and forces users to re-set their GOPATH to use the package. Don\'t do this.\n  Minimize the exported interface\n    Your code is likely composed of many small pieces of useful code, and so it is tempting to expose much of that functionality in your package\'s exported interface. Resist that urge!\n    The larger the interface you provide, the more you must support. Users will quickly come to depend on every type, function, variable, and constant you export, creating an implicit contract that you must honor in perpetuity or risk breaking your users\' programs. In preparing Go 1 we carefully reviewed the standard library\'s exported interfaces and removed the parts we weren\'t ready to commit to. You should take similar care when distributing your own libraries.\n    If in doubt, leave it out!\n  What to put into a package\n    It is easy to just throw everything into a \"grab bag\" package, but this dilutes the meaning of the package name (as it must encompass a lot of functionality) and forces the users of small parts of the package to compile and link a lot of unrelated code.\n    On the other hand, it is also easy to go overboard in splitting your code into small packages, in which case you will likely becomes bogged down in interface design, rather than just getting the job done.\n    Look to the Go standard libraries as a guide. Some of its packages are large and some are small. For instance, the http package comprises 17 go source files (excluding tests) and exports 109 identifiers, and the hash package consists of one file that exports just three declarations. There is no hard and fast rule; both approaches are appropriate given their context.\n    With that said, package main is often larger than other packages. Complex commands contain a lot of code that is of little use outside the context of the executable, and often it\'s simpler to just keep it all in the one place. For instance, the go tool is more than 12000 lines spread across 34 files.\n  Document your code\n    Good documentation is an essential quality of usable and maintainable code. Read the Godoc: documenting Go code article to learn how to write good doc comments.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/organizing-go-code','2012-08-16','2017-05-07 13:50:03.603946'),(561,'Go App Engine SDK 1.5.5 released','Andrew Gerrand','		Go App Engine SDK 1.5.5 released\n		11 October 2011\n		\n    Today we released version 1.5.5 the Go App Engine SDK. You can download it from the App Engine downloads page.\n    This release includes changes and improvements to the App Engine APIs and brings the supporting Go tool chain to release.r60.2 (the current stable release). Also included in this release are the godoc, gofmt, and gofix tools from the Go tool chain. They can be found in the root directory of the SDK.\n    Some changes made in this release are backwards-incompatible, so we have incremented the SDK api_version to 3. Existing apps will require code changes when migrating to api_version 3.\n    The gofix tool that ships with the SDK has been customized with App Engine-specific modules. It can be used to automatically update Go apps to work with the latest appengine packages and the updated Go standard library. To update your apps, run:\n  /path/to/sdk/gofix /path/to/your/app\n    The SDK now includes the appengine package source code, so you can use the local godoc to read App Engine API documentation:\n  /path/to/sdk/godoc appengine/datastore Get\n    Important note: We have deprecated api_version 2. Go apps that use api_version 2 will stop working after 16 December 2011. Please update your apps to use api_version 3 before then.\n    See the release notes for a full list of changes. Please direct any questions about the new SDK to the Go App Engine discussion group.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go-app-engine-sdk-155-released','2011-10-11','2017-05-07 13:50:03.642434'),(562,'Go at Google I/O 2011: videos','Andrew Gerrand','		Go at Google I/O 2011: videos\n		23 May 2011\n		\n  Introduction\n    The Go team had a great time at Google I/O 2011. It was a pleasure to meet so many programmers who share our enthusiasm for Go, and to share our work of the past few months. For those of you that couldn’t be there in person, you can now watch videos of our two Go presentations on YouTube.\n  Writing Web Apps in Go\n    In “Writing Web Apps in Go” we announce the Go runtime for Google App Engine and walk through the development and deployment of Moustachio, the first Go App Engine app.\n    (See the presentation slides.)\n    The source code for Moustachio is available as part of the SDK along with some other examples, such as this Mandelbrot demo.\n    Most important, this talk features the debut of the plush gopher.\n    For those that didn’t get one at the conference, we hope to make him available for purchase online soon.\n  Real World Go\n    “Real World Go”, presented at I/O Bootcamp, gives a brief introduction to Go and four case studies of its use in solving real problems:\n    - Heroku with Doozer, a highly available consistent data store,\n    MROffice Dialer, a VOIP system for call centers,\n    Atlassian’s virtual machine cluster management system,\n    Camlistore, a content addressable storage system.\n    (See the [[http://golang.org/doc/talks/io2011/Real_World_Go.pdf ][presentation slides]].)\n    Thanks to everyone who attended our talks and workshops. We look forward to seeing you again soon!\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go-at-google-io-2011-videos','2011-05-23','2017-05-07 13:50:03.683804'),(563,'Share Memory By Communicating','Andrew Gerrand','		Share Memory By Communicating\n		13 July 2010\n		\n    Traditional threading models (commonly used when writing Java, C++, and Python programs, for example) require the programmer to communicate between threads using shared memory. Typically, shared data structures are protected by locks, and threads will contend over those locks to access the data. In some cases, this is made easier by the use of thread-safe data structures such as Python\'s Queue.\n    Go\'s concurrency primitives - goroutines and channels - provide an elegant and distinct means of structuring concurrent software. (These concepts have an interesting history that begins with C. A. R. Hoare\'s Communicating Sequential Processes.) Instead of explicitly using locks to mediate access to shared data, Go encourages the use of channels to pass references to data between goroutines. This approach ensures that only one goroutine has access to the data at a given time. The concept is summarized in the document Effective Go (a must-read for any Go programmer):\n    Do not communicate by sharing memory; instead, share memory by communicating.\n    Consider a program that polls a list of URLs. In a traditional threading environment, one might structure its data like so:\n  type Resource struct {\n    url        string\n    polling    bool\n    lastPolled int64\n}\ntype Resources struct {\n    data []*Resource\n    lock *sync.Mutex\n}\n    And then a Poller function (many of which would run in separate threads) might look something like this:\n  func Poller(res *Resources) {\n    for {\n        // get the least recently-polled Resource\n        // and mark it as being polled\n        res.lock.Lock()\n        var r *Resource\n        for _, v := range res.data {\n            if v.polling {\n                continue\n            }\n            if r == nil || v.lastPolled &lt; r.lastPolled {\n                r = v\n            }\n        }\n        if r != nil {\n            r.polling = true\n        }\n        res.lock.Unlock()\n        if r == nil {\n            continue\n        }\n        // poll the URL\n        // update the Resource\'s polling and lastPolled\n        res.lock.Lock()\n        r.polling = false\n        r.lastPolled = time.Nanoseconds()\n        res.lock.Unlock()\n    }\n}\n    This function is about a page long, and requires more detail to make it complete. It doesn\'t even include the URL polling logic (which, itself, would only be a few lines), nor will it gracefully handle exhausting the pool of Resources.\n    Let\'s take a look at the same functionality implemented using Go idiom. In this example, Poller is a function that receives Resources to be polled from an input channel, and sends them to an output channel when they\'re done.\n  type Resource string\nfunc Poller(in, out chan *Resource) {\n    for r := range in {\n        // poll the URL\n        // send the processed Resource to out\n        out &lt;- r\n    }\n}\n    The delicate logic from the previous example is conspicuously absent, and our Resource data structure no longer contains bookkeeping data. In fact, all that\'s left are the important parts. This should give you an inkling as to the power of these simple language features.\n    There are many omissions from the above code snippets. For a walkthrough of a complete, idiomatic Go program that uses these ides, see the Codewalk Share Memory By Communicating.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/share-memory-by-communicating','2010-07-13','2017-05-07 13:50:03.737899'),(564,'Go Concurrency Patterns: Timing out, moving on','Andrew Gerrand','		Go Concurrency Patterns: Timing out, moving on\n		23 September 2010\n		\n    Concurrent programming has its own idioms. A good example is timeouts. Although Go\'s channels do not support them directly, they are easy to implement. Say we want to receive from the channel ch, but want to wait at most one second for the value to arrive. We would start by creating a signalling channel and launching a goroutine that sleeps before sending on the channel:\n  timeout := make(chan bool, 1)\ngo func() {\n    time.Sleep(1 * time.Second)\n    timeout &lt;- true\n}()\n    We can then use a select statement to receive from either ch or timeout. If nothing arrives on ch after one second, the timeout case is selected and the attempt to read from ch is abandoned.\n  select {\ncase &lt;-ch:\n    // a read from ch has occurred\ncase &lt;-timeout:\n    // the read from ch has timed out\n}\n    The timeout channel is buffered with space for 1 value, allowing the timeout goroutine to send to the channel and then exit. The goroutine doesn\'t know (or care) whether the value is received. This means the goroutine won\'t hang around forever if the ch receive happens before the timeout is reached. The timeout channel will eventually be deallocated by the garbage collector.\n    (In this example we used time.Sleep to demonstrate the mechanics of goroutines and channels. In real programs you should use ` time.After`, a function that returns a channel and sends on that channel after the specified duration.)\n    Let\'s look at another variation of this pattern. In this example we have a program that reads from multiple replicated databases simultaneously. The program needs only one of the answers, and it should accept the answer that arrives first.\n    The function Query takes a slice of database connections and a query string. It queries each of the databases in parallel and returns the first response it receives:\n  func Query(conns []Conn, query string) Result {\n    ch := make(chan Result)\n    for _, conn := range conns {\n        go func(c Conn) {\n            select {\n            case ch &lt;- c.DoQuery(query):\n            default:\n            }\n        }(conn)\n    }\n    return &lt;-ch\n}\n    In this example, the closure does a non-blocking send, which it achieves by using the send operation in select statement with a default case. If the send cannot go through immediately the default case will be selected. Making the send non-blocking guarantees that none of the goroutines launched in the loop will hang around. However, if the result arrives before the main function has made it to the receive, the send could fail since no one is ready.\n    This problem is a textbook example of what is known as a race condition, but the fix is trivial. We just make sure to buffer the channel ch (by adding the buffer length as the second argument to make), guaranteeing that the first send has a place to put the value. This ensures the send will always succeed, and the first value to arrive will be retrieved regardless of the order of execution.\n    These two examples demonstrate the simplicity with which Go can express complex interactions between goroutines.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go-concurrency-patterns-timing-out-and','2010-09-23','2017-05-07 13:50:03.775740'),(565,'Go Slices: usage and internals','Andrew Gerrand','		Go Slices: usage and internals\n		5 January 2011\n		\n  Introduction\n    Go\'s slice type provides a convenient and efficient means of working with sequences of typed data. Slices are analogous to arrays in other languages, but have some unusual properties. This article will look at what slices are and how they are used.\n  Arrays\n    The slice type is an abstraction built on top of Go\'s array type, and so to understand slices we must first understand arrays.\n    An array type definition specifies a length and an element type. For example, the type [4]int represents an array of four integers. An array\'s size is fixed; its length is part of its type ([4]int and [5]int are distinct, incompatible types). Arrays can be indexed in the usual way, so the expression s[n] accesses the nth element, starting from zero.\n  var a [4]int\na[0] = 1\ni := a[0]\n// i == 1\n    Arrays do not need to be initialized explicitly; the zero value of an array is a ready-to-use array whose elements are themselves zeroed:\n  // a[2] == 0, the zero value of the int type\n    The in-memory representation of [4]int is just four integer values laid out sequentially:\n    Go\'s arrays are values. An array variable denotes the entire array; it is not a pointer to the first array element (as would be the case in C).  This means that when you assign or pass around an array value you will make a copy of its contents. (To avoid the copy you could pass a pointer to the array, but then that\'s a pointer to an array, not an array.) One way to think about arrays is as a sort of struct but with indexed rather than named fields: a fixed-size composite value.\n    An array literal can be specified like so:\n  b := [2]string{\"Penn\", \"Teller\"}\n    Or, you can have the compiler count the array elements for you:\n  b := [...]string{\"Penn\", \"Teller\"}\n    In both cases, the type of b is [2]string.\n  Slices\n    Arrays have their place, but they\'re a bit inflexible, so you don\'t see them too often in Go code. Slices, though, are everywhere. They build on arrays to provide great power and convenience.\n    The type specification for a slice is []T, where T is the type of the elements of the slice. Unlike an array type, a slice type has no specified length.\n    A slice literal is declared just like an array literal, except you leave out the element count:\n  letters := []string{\"a\", \"b\", \"c\", \"d\"}\n    A slice can be created with the built-in function called make, which has the signature,\n  func make([]T, len, cap) []T\n    where T stands for the element type of the slice to be created. The make function takes a type, a length, and an optional capacity. When called, make allocates an array and returns a slice that refers to that array.\n  var s []byte\ns = make([]byte, 5, 5)\n// s == []byte{0, 0, 0, 0, 0}\n    When the capacity argument is omitted, it defaults to the specified length. Here\'s a more succinct version of the same code:\n  s := make([]byte, 5)\n    The length and capacity of a slice can be inspected using the built-in len and cap functions.\n  len(s) == 5\ncap(s) == 5\n    The next two sections discuss the relationship between length and capacity.\n    The zero value of a slice is nil. The len and cap functions will both return 0 for a nil slice.\n    A slice can also be formed by \"slicing\" an existing slice or array. Slicing is done by specifying a half-open range with two indices separated by a colon. For example, the expression b[1:4] creates a slice including elements 1 through 3 of b (the indices of the resulting slice will be 0 through 2).\n  b := []byte{\'g\', \'o\', \'l\', \'a\', \'n\', \'g\'}\n// b[1:4] == []byte{\'o\', \'l\', \'a\'}, sharing the same storage as b\n    The start and end indices of a slice expression are optional; they default to zero and the slice\'s length respectively:\n  // b[:2] == []byte{\'g\', \'o\'}\n// b[2:] == []byte{\'l\', \'a\', \'n\', \'g\'}\n// b[:] == b\n    This is also the syntax to create a slice given an array:\n  x := [3]string{\"Лайка\", \"Белка\", \"Стрелка\"}\ns := x[:] // a slice referencing the storage of x\n  Slice internals\n    A slice is a descriptor of an array segment. It consists of a pointer to the array, the length of the segment, and its capacity (the maximum length of the segment).\n    Our variable s, created earlier by make([]byte, 5), is structured like this:\n    The length is the number of elements referred to by the slice. The capacity is the number of elements in the underlying array (beginning at the element referred to by the slice pointer). The distinction between length and capacity will be made clear as we walk through the next few examples.\n    As we slice s, observe the changes in the slice data structure and their relation to the underlying array:\n  s = s[2:4]\n    Slicing does not copy the slice\'s data. It creates a new slice value that points to the original array. This makes slice operations as efficient as manipulating array indices. Therefore, modifying the elements (not the slice itself) of a re-slice modifies the elements of the original slice:\n  d := []byte{\'r\', \'o\', \'a\', \'d\'}\ne := d[2:] \n// e == []byte{\'a\', \'d\'}\ne[1] = \'m\'\n// e == []byte{\'a\', \'m\'}\n// d == []byte{\'r\', \'o\', \'a\', \'m\'}\n    Earlier we sliced s to a length shorter than its capacity. We can grow s to its capacity by slicing it again:\n  s = s[:cap(s)]\n    A slice cannot be grown beyond its capacity. Attempting to do so will cause a runtime panic, just as when indexing outside the bounds of a slice or array. Similarly, slices cannot be re-sliced below zero to access earlier elements in the array.\n  Growing slices (the copy and append functions)\n    To increase the capacity of a slice one must create a new, larger slice and copy the contents of the original slice into it. This technique is how dynamic array implementations from other languages work behind the scenes. The next example doubles the capacity of s by making a new slice, t, copying the contents of s into t, and then assigning the slice value t to s:\n  t := make([]byte, len(s), (cap(s)+1)*2) // +1 in case cap(s) == 0\nfor i := range s {\n        t[i] = s[i]\n}\ns = t\n    The looping piece of this common operation is made easier by the built-in copy function. As the name suggests, copy copies data from a source slice to a destination slice. It returns the number of elements copied.\n  func copy(dst, src []T) int\n    The copy function supports copying between slices of different lengths (it will copy only up to the smaller number of elements). In addition, copy can handle source and destination slices that share the same underlying array, handling overlapping slices correctly.\n    Using copy, we can simplify the code snippet above:\n  t := make([]byte, len(s), (cap(s)+1)*2)\ncopy(t, s)\ns = t\n    A common operation is to append data to the end of a slice. This function appends byte elements to a slice of bytes, growing the slice if necessary, and returns the updated slice value:\n  func AppendByte(slice []byte, data ...byte) []byte {\n    m := len(slice)\n    n := m + len(data)\n    if n &gt; cap(slice) { // if necessary, reallocate\n        // allocate double what\'s needed, for future growth.\n        newSlice := make([]byte, (n+1)*2)\n        copy(newSlice, slice)\n        slice = newSlice\n    }\n    slice = slice[0:n]\n    copy(slice[m:n], data)\n    return slice\n}\n    One could use AppendByte like this:\n  p := []byte{2, 3, 5}\np = AppendByte(p, 7, 11, 13)\n// p == []byte{2, 3, 5, 7, 11, 13}\n    Functions like AppendByte are useful because they offer complete control over the way the slice is grown. Depending on the characteristics of the program, it may be desirable to allocate in smaller or larger chunks, or to put a ceiling on the size of a reallocation.\n    But most programs don\'t need complete control, so Go provides a built-in append function that\'s good for most purposes; it has the signature\n  func append(s []T, x ...T) []T\n    The append function appends the elements x to the end of the slice s, and grows the slice if a greater capacity is needed.\n  a := make([]int, 1)\n// a == []int{0}\na = append(a, 1, 2, 3)\n// a == []int{0, 1, 2, 3}\n    To append one slice to another, use ... to expand the second argument to a list of arguments.\n  a := []string{\"John\", \"Paul\"}\nb := []string{\"George\", \"Ringo\", \"Pete\"}\na = append(a, b...) // equivalent to \"append(a, b[0], b[1], b[2])\"\n// a == []string{\"John\", \"Paul\", \"George\", \"Ringo\", \"Pete\"}\n    Since the zero value of a slice (nil) acts like a zero-length slice, you can declare a slice variable and then append to it in a loop:\n  // Filter returns a new slice holding only\n// the elements of s that satisfy f()\nfunc Filter(s []int, fn func(int) bool) []int {\n    var p []int // == nil\n    for _, v := range s {\n        if fn(v) {\n            p = append(p, v)\n        }\n    }\n    return p\n}\n  A possible \"gotcha\"\n    As mentioned earlier, re-slicing a slice doesn\'t make a copy of the underlying array. The full array will be kept in memory until it is no longer referenced. Occasionally this can cause the program to hold all the data in memory when only a small piece of it is needed.\n    For example, this FindDigits function loads a file into memory and searches it for the first group of consecutive numeric digits, returning them as a new slice.\n  var digitRegexp = regexp.MustCompile(\"[0-9]+\")\nfunc FindDigits(filename string) []byte {\n    b, _ := ioutil.ReadFile(filename)\n    return digitRegexp.Find(b)\n}\n    This code behaves as advertised, but the returned []byte points into an array containing the entire file. Since the slice references the original array, as long as the slice is kept around the garbage collector can\'t release the array; the few useful bytes of the file keep the entire contents in memory.\n    To fix this problem one can copy the interesting data to a new slice before returning it:\n  func CopyDigits(filename string) []byte {\n    b, _ := ioutil.ReadFile(filename)\n    b = digitRegexp.Find(b)\n    c := make([]byte, len(b))\n    copy(c, b)\n    return c\n}\n    A more concise version of this function could be constructed by using append. This is left as an exercise for the reader.\n  Further Reading\n    Effective Go contains an in-depth treatment of slices and arrays, and the Go language specification defines slices and their associated helper functions.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go-slices-usage-and-internals','2011-01-05','2017-05-07 13:50:03.813379'),(566,'JSON and Go','Andrew Gerrand','		JSON and Go\n		25 January 2011\n		\n  Introduction\n    JSON (JavaScript Object Notation) is a simple data interchange format. Syntactically it resembles the objects and lists of JavaScript. It is most commonly used for communication between web back-ends and JavaScript programs running in the browser, but it is used in many other places, too. Its home page, json.org, provides a wonderfully clear and concise definition of the standard.\n    With the json package it\'s a snap to read and write JSON data from your Go programs.\n  Encoding\n    To encode JSON data we use the Marshal function.\n  func Marshal(v interface{}) ([]byte, error)\n    Given the Go data structure, Message,\n  type Message struct {\n    Name string\n    Body string\n    Time int64\n}\n    and an instance of Message\n  m := Message{\"Alice\", \"Hello\", 1294706395881547000}\n    we can marshal a JSON-encoded version of m using json.Marshal:\n  b, err := json.Marshal(m)\n    If all is well, err will be nil and b will be a []byte containing this JSON data:\n  b == []byte(`{\"Name\":\"Alice\",\"Body\":\"Hello\",\"Time\":1294706395881547000}`)\n    Only data structures that can be represented as valid JSON will be encoded:\n    JSON objects only support strings as keys; to encode a Go map type it must be of the form map[string]T (where T is any Go type supported by the json package).\n    Channel, complex, and function types cannot be encoded.\n    Cyclic data structures are not supported; they will cause Marshal to go into an infinite loop.\n    Pointers will be encoded as the values they point to (or \'null\' if the pointer is nil).\n    The json package only accesses the exported fields of struct types (those that begin with an uppercase letter). Therefore only the the exported fields of a struct will be present in the JSON output.\n  Decoding\n    To decode JSON data we use the Unmarshal function.\n  func Unmarshal(data []byte, v interface{}) error\n    We must first create a place where the decoded data will be stored\n  var m Message\n    and call json.Unmarshal, passing it a []byte of JSON data and a pointer to m\n  err := json.Unmarshal(b, &amp;m)\n    If b contains valid JSON that fits in m, after the call err will be nil and the data from b will have been stored in the struct m, as if by an assignment like:\n  m = Message{\n    Name: \"Alice\",\n    Body: \"Hello\",\n    Time: 1294706395881547000,\n}\n    How does Unmarshal identify the fields in which to store the decoded data? For a given JSON key \"Foo\", Unmarshal will look through the destination struct\'s fields to find (in order of preference):\n    An exported field with a tag of \"Foo\" (see the Go spec for more on struct tags),\n    An exported field named \"Foo\", or\n    An exported field named \"FOO\" or \"FoO\" or some other case-insensitive match of \"Foo\".\n    What happens when the structure of the JSON data doesn\'t exactly match the Go type?\n  b := []byte(`{\"Name\":\"Bob\",\"Food\":\"Pickle\"}`)\nvar m Message\nerr := json.Unmarshal(b, &amp;m)\n    Unmarshal will decode only the fields that it can find in the destination type.  In this case, only the Name field of m will be populated, and the Food field will be ignored. This behavior is particularly useful when you wish to pick only a few specific fields out of a large JSON blob. It also means that any unexported fields in the destination struct will be unaffected by Unmarshal.\n    But what if you don\'t know the structure of your JSON data beforehand?\n  Generic JSON with interface{}\n    The interface{} (empty interface) type describes an interface with zero methods.  Every Go type implements at least zero methods and therefore satisfies the empty interface.\n    The empty interface serves as a general container type:\n  var i interface{}\ni = \"a string\"\ni = 2011\ni = 2.777\n    A type assertion accesses the underlying concrete type:\n  r := i.(float64)\nfmt.Println(\"the circle\'s area\", math.Pi*r*r)\n    Or, if the underlying type is unknown, a type switch determines the type:\n  switch v := i.(type) {\ncase int:\n    fmt.Println(\"twice i is\", v*2)\ncase float64:\n    fmt.Println(\"the reciprocal of i is\", 1/v)\ncase string:\n    h := len(v) / 2\n    fmt.Println(\"i swapped by halves is\", v[h:]+v[:h])\ndefault:\n    // i isn\'t one of the types above\n}\n    The json package uses map[string]interface{} and\n    []interface{} values to store arbitrary JSON objects and arrays;\n    it will happily unmarshal any valid JSON blob into a plain\n    interface{} value.  The default concrete Go types are:\n    bool for JSON booleans,\n    float64 for JSON numbers,\n    string for JSON strings, and\n    nil for JSON null.\n  Decoding arbitrary data\n    Consider this JSON data, stored in the variable b:\n  b := []byte(`{\"Name\":\"Wednesday\",\"Age\":6,\"Parents\":[\"Gomez\",\"Morticia\"]}`)\n    Without knowing this data\'s structure, we can decode it into an interface{} value with Unmarshal:\n  var f interface{}\nerr := json.Unmarshal(b, &amp;f)\n    At this point the Go value in f would be a map whose keys are strings and whose values are themselves stored as empty interface values:\n  f = map[string]interface{}{\n    \"Name\": \"Wednesday\",\n    \"Age\":  6,\n    \"Parents\": []interface{}{\n        \"Gomez\",\n        \"Morticia\",\n    },\n}\n    To access this data we can use a type assertion to access f\'s underlying map[string]interface{}:\n  m := f.(map[string]interface{})\n    We can then iterate through the map with a range statement and use a type switch to access its values as their concrete types:\n  for k, v := range m {\n    switch vv := v.(type) {\n    case string:\n        fmt.Println(k, \"is string\", vv)\n    case int:\n        fmt.Println(k, \"is int\", vv)\n    case []interface{}:\n        fmt.Println(k, \"is an array:\")\n        for i, u := range vv {\n            fmt.Println(i, u)\n        }\n    default:\n        fmt.Println(k, \"is of a type I don\'t know how to handle\")\n    }\n}\n    In this way you can work with unknown JSON data while still enjoying the benefits of type safety.\n  Reference Types\n    Let\'s define a Go type to contain the data from the previous example:\n  type FamilyMember struct {\n    Name    string\n    Age     int\n    Parents []string\n}\n    var m FamilyMember\n    err := json.Unmarshal(b, &amp;m)\n    Unmarshaling that data into a FamilyMember value works as expected, but if we look closely we can see a remarkable thing has happened. With the var statement we allocated a FamilyMember struct, and then provided a pointer to that value to Unmarshal, but at that time the Parents field was a nil slice value. To populate the Parents field, Unmarshal allocated a new slice behind the scenes. This is typical of how Unmarshal works with the supported reference types (pointers, slices, and maps).\n    Consider unmarshaling into this data structure:\n  type Foo struct {\n    Bar *Bar\n}\n    If there were a Bar field in the JSON object, Unmarshal would allocate a new Bar and populate it. If not, Bar would be left as a nil pointer.\n    From this a useful pattern arises: if you have an application that receives a few distinct message types, you might define \"receiver\" structure like\n  type IncomingMessage struct {\n    Cmd *Command\n    Msg *Message\n}\n    and the sending party can populate the Cmd field and/or the Msg field of the top-level JSON object, depending on the type of message they want to communicate. Unmarshal, when decoding the JSON into an IncomingMessage struct, will only allocate the data structures present in the JSON data. To know which messages to process, the programmer need simply test that either Cmd or Msg is not nil.\n  Streaming Encoders and Decoders\n    The json package provides Decoder and Encoder types to support the common operation of reading and writing streams of JSON data. The NewDecoder and NewEncoder functions wrap the io.Reader and io.Writer interface types.\n  func NewDecoder(r io.Reader) *Decoder\nfunc NewEncoder(w io.Writer) *Encoder\n    Here\'s an example program that reads a series of JSON objects from standard input, removes all but the Name field from each object, and then writes the objects to standard output:\n  package main\nimport (\n    \"encoding/json\"\n    \"log\"\n    \"os\"\n)\nfunc main() {\n    dec := json.NewDecoder(os.Stdin)\n    enc := json.NewEncoder(os.Stdout)\n    for {\n        var v map[string]interface{}\n        if err := dec.Decode(&amp;v); err != nil {\n            log.Println(err)\n            return\n        }\n        for k := range v {\n            if k != \"Name\" {\n                delete(v, k)\n            }\n        }\n        if err := enc.Encode(&amp;v); err != nil {\n            log.Println(err)\n        }\n    }\n}\n    Due to the ubiquity of Readers and Writers, these Encoder and Decoder types can be used in a broad range of scenarios, such as reading and writing to HTTP connections, WebSockets, or files.\n  References\n    For more information see the json package documentation. For an example usage of json see the source files of the jsonrpc package.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/json-and-go','2011-01-25','2017-05-07 13:50:03.846774'),(567,'Go Wins 2010 Bossie Award','Andrew Gerrand','		Go Wins 2010 Bossie Award\n		6 September 2010\n		\n    The Go project has been awarded a Bossie Award for \"best open source application development software.\" The Bossie Awards have been given each year since 2007 by InfoWorld to recognize the best Open Source projects. Their citation reads:\n    Google\'s Go language tries to restore simplicity to programming. It does away with numerous constructs that have crept into all OO languages by re-imagining how to simplify and improve the conversation between a developer and the code. Go provides garbage collection, type safety, memory safety, and built-in support for concurrency and for Unicode characters. In addition, it compiles (fast!) to binaries for multiple platforms. Go is still in development (new features are being added) and it has limitations, notably poor support for Windows. But it shows a new, exciting direction in programming languages.The Go team are pleased to accept this award. It reflects not only our hard work, but also the efforts of the growing community of Go developers to whom we must extend our sincere thanks.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go-wins-2010-bossie-award','2010-09-06','2017-05-07 13:50:03.878421'),(568,'Debugging Go code (a status report)','Luuk van Dijk','		Debugging Go code (a status report)\n		2 November 2010\n		\n    When it comes to debugging, nothing beats a few strategic print statements to inspect variables or a well-placed panic to obtain a stack trace.  However, sometimes you’re missing either the patience or the source code, and in those cases a good debugger can be invaluable.  That\'s why over the past few releases we have been improving the support in Go’s gc linker (6l, 8l) for GDB, the GNU debugger.\n    In the latest release (2010-11-02), the 6l and 8l linkers emit DWARF3 debugging information when writing ELF (Linux, FreeBSD) or Mach-O (Mac OS X) binaries.  The DWARF code is rich enough to let you do the following:\n    load a Go program in GDB version 7.x,\n    list all Go, C, and assembly source files by line (parts of the Go runtime are written in C and assembly),\n    set breakpoints by line and step through the code,\n    print stack traces and inspect stack frames, and\n    find the addresses and print the contents of most variables.\n    There are still some inconveniences:\n    The emitted DWARF code is unreadable by the GDB version 6.x that ships with Mac OS X. We would gladly accept patches to make the DWARF output compatible with the standard OS X GDB, but until that’s fixed you’ll need to download, build, and install GDB 7.x to use it under OS X. The source can be found at http://sourceware.org/gdb/download/.  Due to the particulars of OS X you’ll need to install the binary on a local file system with chgrp procmod and chmod g+s.\n    Names are qualified with a package name and, as GDB doesn\'t understand Go packages, you must reference each item by its full name. For example, the variable named v in package main must be referred to as \'main.v\', in single quotes. A consequence of this is that tab completion of variable and function names does not work.\n    Lexical scoping information is somewhat obfuscated. If there are multiple variables of the same name, the nth instance will have a suffix of the form ‘#n’. We plan to fix this, but it will require some changes to the data exchanged between the compiler and linker.\n    Slice and string variables are represented as their underlying structure in the runtime library. They will look something like {data = 0x2aaaaab3e320, len = 1, cap = 1}. For slices, you must dereference the data pointer to inspect the elements.\n    Some things don\'t work yet:\n    Channel, function, interface, and map variables cannot be inspected.\n    Only Go variables are annotated with type information; the runtime\'s C variables are not.\n    Windows and ARM binaries do not contain DWARF debugging information and, as such, cannot be inspected with GDB.\n    Over the coming months we intend to address these issues, either by changing the compiler and linker or by using the Python extensions to GDB. In the meantime, we hope that Go programmers will benefit from having better access to this well-known debugging tool.\n    P.S. The DWARF information can also be read by tools other than GDB.  For example, on Linux you can use it with the sysprof system-wide profiler.\n		\n			By Luuk van Dijk\n		\n	','https://blog.golang.org/debugging-go-code-status-report','2010-11-02','2017-05-07 13:50:03.890117'),(569,'Go\'s Declaration Syntax','Rob Pike','		Go\'s Declaration Syntax\n		7 July 2010\n		\n  Introduction\n    Newcomers to Go wonder why the declaration syntax is different from the tradition established in the C family. In this post we\'ll compare the two approaches and explain why Go\'s declarations look as they do.\n  C syntax\n    First, let\'s talk about C syntax. C took an unusual and clever approach to declaration syntax. Instead of describing the types with special syntax, one writes an expression involving the item being declared, and states what type that expression will have. Thus\n  int x;\n    declares x to be an int: the expression \'x\' will have type int. In general, to figure out how to write the type of a new variable, write an expression involving that variable that evaluates to a basic type, then put the basic type on the left and the expression on the right.\n    Thus, the declarations\n  int *p;\nint a[3];\n    state that p is a pointer to int because \'*p\' has type int, and that a is an array of ints because a[3] (ignoring the particular index value, which is punned to be the size of the array) has type int.\n    What about functions? Originally, C\'s function declarations wrote the types of the arguments outside the parens, like this:\n  int main(argc, argv)\n    int argc;\n    char *argv[];\n{ /* ... */ }\n    Again, we see that main is a function because the expression main(argc, argv) returns an int. In modern notation we\'d write\n  int main(int argc, char *argv[]) { /* ... */ }\n    but the basic structure is the same.\n    This is a clever syntactic idea that works well for simple types but can get confusing fast. The famous example is declaring a function pointer. Follow the rules and you get this:\n  int (*fp)(int a, int b);\n    Here, fp is a pointer to a function because if you write the expression (*fp)(a, b) you\'ll call a function that returns int. What if one of fp\'s arguments is itself a function?\n  int (*fp)(int (*ff)(int x, int y), int b)\n    That\'s starting to get hard to read.\n    Of course, we can leave out the name of the parameters when we declare a function, so main can be declared\n  int main(int, char *[])\n    Recall that argv is declared like this,\n  char *argv[]\n    so you drop the name from the middle of its declaration to construct its type. It\'s not obvious, though, that you declare something of type char *[] by putting its name in the middle.\n    And look what happens to fp\'s declaration if you don\'t name the parameters:\n  int (*fp)(int (*)(int, int), int)\n    Not only is it not obvious where to put the name inside\n  int (*)(int, int)\n    it\'s not exactly clear that it\'s a function pointer declaration at all. And what if the return type is a function pointer?\n  int (*(*fp)(int (*)(int, int), int))(int, int)\n    It\'s hard even to see that this declaration is about fp.\n    You can construct more elaborate examples but these should illustrate some of the difficulties that C\'s declaration syntax can introduce.\n    There\'s one more point that needs to be made, though. Because type and declaration syntax are the same, it can be difficult to parse expressions with types in the middle. This is why, for instance, C casts always parenthesize the type, as in\n  (int)M_PI\n  Go syntax\n    Languages outside the C family usually use a distinct type syntax in declarations. Although it\'s a separate point, the name usually comes first, often followed by a colon. Thus our examples above become something like (in a fictional but illustrative language)\n  x: int\np: pointer to int\na: array[3] of int\n    These declarations are clear, if verbose - you just read them left to right. Go takes its cue from here, but in the interests of brevity it drops the colon and removes some of the keywords:\n  x int\np *int\na [3]int\n    There is no direct correspondence between the look of [3]int and how to use a in an expression. (We\'ll come back to pointers in the next section.) You gain clarity at the cost of a separate syntax.\n    Now consider functions. Let\'s transcribe the declaration for main as it would read in Go, although the real main function in Go takes no arguments:\n  func main(argc int, argv []string) int\n    Superficially that\'s not much different from C, other than the change from char arrays to strings, but it reads well from left to right:\n    function main takes an int and a slice of strings and returns an int.\n    Drop the parameter names and it\'s just as clear - they\'re always first so there\'s no confusion.\n  func main(int, []string) int\n    One merit of this left-to-right style is how well it works as the types become more complex. Here\'s a declaration of a function variable (analogous to a function pointer in C):\n  f func(func(int,int) int, int) int\n    Or if f returns a function:\n  f func(func(int,int) int, int) func(int, int) int\n    It still reads clearly, from left to right, and it\'s always obvious which name is being declared - the name comes first.\n    The distinction between type and expression syntax makes it easy to write and invoke closures in Go:\n  sum := func(a, b int) int { return a+b } (3, 4)\n  Pointers\n    Pointers are the exception that proves the rule. Notice that in arrays and slices, for instance, Go\'s type syntax puts the brackets on the left of the type but the expression syntax puts them on the right of the expression:\n  var a []int\nx = a[1]\n    For familiarity, Go\'s pointers use the * notation from C, but we could not bring ourselves to make a similar reversal for pointer types. Thus pointers work like this\n  var p *int\nx = *p\n    We couldn\'t say\n  var p *int\nx = p*\n    because that postfix * would conflate with multiplication. We could have used the Pascal ^, for example:\n  var p ^int\nx = p^\n    and perhaps we should have (and chosen another operator for xor), because the prefix asterisk on both types and expressions complicates things in a number of ways. For instance, although one can write\n  []int(\"hi\")\n    as a conversion, one must parenthesize the type if it starts with a *:\n  (*int)(nil)\n    Had we been willing to give up * as pointer syntax, those parentheses would be unnecessary.\n    So Go\'s pointer syntax is tied to the familiar C form, but those ties mean that we cannot break completely from using parentheses to disambiguate types and expressions in the grammar.\n    Overall, though, we believe Go\'s type syntax is easier to understand than C\'s, especially when things get complicated.\n  Notes\n    Go\'s declarations read left to right. It\'s been pointed out that C\'s read in a spiral! See  The \"Clockwise/Spiral Rule\" by David Anderson.\n		\n			By Rob Pike\n		\n	','https://blog.golang.org/gos-declaration-syntax','2010-07-07','2017-05-07 13:50:03.903894'),(570,'Real Go Projects: SmartTwitter and web.go','Michael Hoisie','		Real Go Projects: SmartTwitter and web.go\n		19 October 2010\n		\n    This week\'s article is written by Michael Hoisie. A programmer based in San Francisco, he is one of Go\'s early adopters and the author of several popular Go libraries. He describes his experiences using Go:\n    I was introduced to Go by a post on Hacker News. About an hour later I was hooked. At the time I was working at a web start-up, and had been developing internal testing apps in Python. Go offered speed, better concurrency support, and sane Unicode handling, so I was keen to port my programs to the language. At that time there wasn\'t an easy way to write web apps in Go, so I decided to build a simple web framework, web.go. It was modeled after a popular Python framework, web.py, which I had worked with previously. While working on web.go I got involved in the Go community, submitted a bunch of bug reports, and hacked on some standard library packages (mainly http and json).\n    After a few weeks I noticed that web.go was getting attention at Github. This was surprising because I\'d never really promoted the project. I think there\'s a niche for simple, fast web applications, and I think Go can fill it.\n    One weekend I decided to write a simple Facebook application: it would re-post your Twitter status updates to your Facebook profile. There is an official Twitter application to do this, but it re-posts everything, creating noise in your Facebook feed. My application allowed you to filter retweets, mentions, hashtags, replies, and more. This turned into Smart Twitter, which currently has nearly 90,000 users.\n    The entire program is written in Go, and uses Redis as its storage back-end. It is very fast and robust. It currently processes about two dozen tweets per second, and makes heavy use of Go\'s channels. It runs on a single Virtual Private Server instance with 2GB of RAM, which has no problem handling the load. Smart Twitter uses very little CPU time, and is almost entirely memory-bound as the entire database is kept in memory. At any given time there are around 10 goroutines running concurrently: one accepting HTTP connections, another reading from the Twitter Streaming API, a couple for error handling, and the rest either processing web requests or re-posting incoming tweets.\n    Smart Twitter also spawned other open-source Go projects: mustache.go, redis.go, and twitterstream.\n    I see a lot of work left to do on web.go. For instance, I\'d like to add better support for streaming connections, websockets, route filters, better support in shared hosts, and improving the documentation. I recently left the start-up to do software freelancing, and I\'m planning to use Go where possible. This means I\'ll probably use it as a back end for personal apps, as well as for clients that like working with cutting edge technology.\n    Finally, I\'d like to thank the Go team for all their effort. Go is a wonderful platform and I think it has a bright future. I hope to see the language grow around the needs of the community. There\'s a lot of interesting stuff happening in the community, and I look forward to seeing what people can hack together with the language.\n		\n			By Michael Hoisie\n		\n	','https://blog.golang.org/real-go-projects-smarttwitter-and-webgo','2010-10-19','2017-05-07 13:50:03.918883'),(571,'Gobs of data','Rob Pike','		Gobs of data\n		24 March 2011\n		\n  Introduction\n    To transmit a data structure across a network or to store it in a file, it must be encoded and then decoded again. There are many encodings available, of course: JSON, XML, Google\'s protocol buffers, and more. And now there\'s another, provided by Go\'s gob package.\n    Why define a new encoding? It\'s a lot of work and redundant at that. Why not just use one of the existing formats? Well, for one thing, we do! Go has packages supporting all the encodings just mentioned (the protocol buffer package is in a separate repository but it\'s one of the most frequently downloaded). And for many purposes, including communicating with tools and systems written in other languages, they\'re the right choice.\n    But for a Go-specific environment, such as communicating between two servers written in Go, there\'s an opportunity to build something much easier to use and possibly more efficient.\n    Gobs work with the language in a way that an externally-defined, language-independent encoding cannot. At the same time, there are lessons to be learned from the existing systems.\n  Goals\n    The gob package was designed with a number of goals in mind.\n    First, and most obvious, it had to be very easy to use. First, because Go has reflection, there is no need for a separate interface definition language or \"protocol compiler\". The data structure itself is all the package should need to figure out how to encode and decode it. On the other hand, this approach means that gobs will never work as well with other languages, but that\'s OK: gobs are unashamedly Go-centric.\n    Efficiency is also important. Textual representations, exemplified by XML and JSON, are too slow to put at the center of an efficient communications network. A binary encoding is necessary.\n    Gob streams must be self-describing. Each gob stream, read from the beginning, contains sufficient information that the entire stream can be parsed by an agent that knows nothing a priori about its contents. This property means that you will always be able to decode a gob stream stored in a file, even long after you\'ve forgotten what data it represents.\n    There were also some things to learn from our experiences with Google protocol buffers.\n  Protocol buffer misfeatures\n    Protocol buffers had a major effect on the design of gobs, but have three features that were deliberately avoided. (Leaving aside the property that protocol buffers aren\'t self-describing: if you don\'t know the data definition used to encode a protocol buffer, you might not be able to parse it.)\n    First, protocol buffers only work on the data type we call a struct in Go. You can\'t encode an integer or array at the top level, only a struct with fields inside it. That seems a pointless restriction, at least in Go. If all you want to send is an array of integers, why should you have to put it into a struct first?\n    Next, a protocol buffer definition may specify that fields T.x and T.y are required to be present whenever a value of type T is encoded or decoded.  Although such required fields may seem like a good idea, they are costly to implement because the codec must maintain a separate data structure while encoding and decoding, to be able to report when required fields are missing.  They\'re also a maintenance problem. Over time, one may want to modify the data definition to remove a required field, but that may cause existing clients of the data to crash. It\'s better not to have them in the encoding at all.  (Protocol buffers also have optional fields. But if we don\'t have required fields, all fields are optional and that\'s that. There will be more to say about optional fields a little later.)\n    The third protocol buffer misfeature is default values. If a protocol buffer omits the value for a \"defaulted\" field, then the decoded structure behaves as if the field were set to that value. This idea works nicely when you have getter and setter methods to control access to the field, but is harder to handle cleanly when the container is just a plain idiomatic struct. Required fields are also tricky to implement: where does one define the default values, what types do they have (is text UTF-8? uninterpreted bytes? how many bits in a float?) and despite the apparent simplicity, there were a number of complications in their design and implementation for protocol buffers. We decided to leave them out of gobs and fall back to Go\'s trivial but effective defaulting rule: unless you set something otherwise, it has the \"zero value\" for that type - and it doesn\'t need to be transmitted.\n    So gobs end up looking like a sort of generalized, simplified protocol buffer. How do they work?\n  Values\n    The encoded gob data isn\'t about types like int8 and uint16. Instead, somewhat analogous to constants in Go, its integer values are abstract, sizeless numbers, either signed or unsigned. When you encode an int8, its value is transmitted as an unsized, variable-length integer. When you encode an int64, its value is also transmitted as an unsized, variable-length integer. (Signed and unsigned are treated distinctly, but the same unsized-ness applies to unsigned values too.) If both have the value 7, the bits sent on the wire will be identical. When the receiver decodes that value, it puts it into the receiver\'s variable, which may be of arbitrary integer type. Thus an encoder may send a 7 that came from an int8, but the receiver may store it in an int64. This is fine: the value is an integer and as a long as it fits, everything works. (If it doesn\'t fit, an error results.) This decoupling from the size of the variable gives some flexibility to the encoding: we can expand the type of the integer variable as the software evolves, but still be able to decode old data.\n    This flexibility also applies to pointers. Before transmission, all pointers are flattened. Values of type int8, *int8, **int8, ****int8, etc. are all transmitted as an integer value, which may then be stored in int of any size, or *int, or ******int, etc. Again, this allows for flexibility.\n    Flexibility also happens because, when decoding a struct, only those fields that are sent by the encoder are stored in the destination. Given the value\n  type T struct{ X, Y, Z int } // Only exported fields are encoded and decoded.\nvar t = T{X: 7, Y: 0, Z: 8}\n    the encoding of t sends only the 7 and 8. Because it\'s zero, the value of Y isn\'t even sent; there\'s no need to send a zero value.\n    The receiver could instead decode the value into this structure:\n  type U struct{ X, Y *int8 } // Note: pointers to int8s\nvar u U\n    and acquire a value of u with only X set (to the address of an int8 variable set to 7); the Z field is ignored - where would you put it? When decoding structs, fields are matched by name and compatible type, and only fields that exist in both are affected. This simple approach finesses the \"optional field\" problem: as the type T evolves by adding fields, out of date receivers will still function with the part of the type they recognize. Thus gobs provide the important result of optional fields - extensibility - without any additional mechanism or notation.\n    From integers we can build all the other types: bytes, strings, arrays, slices, maps, even floats. Floating-point values are represented by their IEEE 754 floating-point bit pattern, stored as an integer, which works fine as long as you know their type, which we always do. By the way, that integer is sent in byte-reversed order because common values of floating-point numbers, such as small integers, have a lot of zeros at the low end that we can avoid transmitting.\n    One nice feature of gobs that Go makes possible is that they allow you to define your own encoding by having your type satisfy the GobEncoder and GobDecoder interfaces, in a manner analogous to the JSON package\'s Marshaler and Unmarshaler and also to the Stringer interface from package fmt. This facility makes it possible to represent special features, enforce constraints, or hide secrets when you transmit data. See the documentation for details.\n  Types on the wire\n    The first time you send a given type, the gob package includes in the data stream a description of that type. In fact, what happens is that the encoder is used to encode, in the standard gob encoding format, an internal struct that describes the type and gives it a unique number. (Basic types, plus the layout of the type description structure, are predefined by the software for bootstrapping.) After the type is described, it can be referenced by its type number.\n    Thus when we send our first type T, the gob encoder sends a description of T and tags it with a type number, say 127. All values, including the first, are then prefixed by that number, so a stream of T values looks like:\n  (\"define type id\" 127, definition of type T)(127, T value)(127, T value), ...\n    These type numbers make it possible to describe recursive types and send values of those types. Thus gobs can encode types such as trees:\n  type Node struct {\n    Value       int\n    Left, Right *Node\n}\n    (It\'s an exercise for the reader to discover how the zero-defaulting rule makes this work, even though gobs don\'t represent pointers.)\n    With the type information, a gob stream is fully self-describing except for the set of bootstrap types, which is a well-defined starting point.\n  Compiling a machine\n    The first time you encode a value of a given type, the gob package builds a little interpreted machine specific to that data type. It uses reflection on the type to construct that machine, but once the machine is built it does not depend on reflection. The machine uses package unsafe and some trickery to convert the data into the encoded bytes at high speed. It could use reflection and avoid unsafe, but would be significantly slower. (A similar high-speed approach is taken by the protocol buffer support for Go, whose design was influenced by the implementation of gobs.) Subsequent values of the same type use the already-compiled machine, so they can be encoded right away.\n    [Update: As of Go 1.4, package unsafe is no longer use by the gob package, with a modest performance drop.]\n    Decoding is similar but harder. When you decode a value, the gob package holds a byte slice representing a value of a given encoder-defined type to decode, plus a Go value into which to decode it. The gob package builds a machine for that pair: the gob type sent on the wire crossed with the Go type provided for decoding. Once that decoding machine is built, though, it\'s again a reflectionless engine that uses unsafe methods to get maximum speed.\n  Use\n    There\'s a lot going on under the hood, but the result is an efficient, easy-to-use encoding system for transmitting data. Here\'s a complete example showing differing encoded and decoded types. Note how easy it is to send and receive values; all you need to do is present values and variables to the gob package and it does all the work.\n  package main\nimport (\n    \"bytes\"\n    \"encoding/gob\"\n    \"fmt\"\n    \"log\"\n)\ntype P struct {\n    X, Y, Z int\n    Name    string\n}\ntype Q struct {\n    X, Y *int32\n    Name string\n}\nfunc main() {\n    // Initialize the encoder and decoder.  Normally enc and dec would be\n    // bound to network connections and the encoder and decoder would\n    // run in different processes.\n    var network bytes.Buffer        // Stand-in for a network connection\n    enc := gob.NewEncoder(&amp;network) // Will write to network.\n    dec := gob.NewDecoder(&amp;network) // Will read from network.\n    // Encode (send) the value.\n    err := enc.Encode(P{3, 4, 5, \"Pythagoras\"})\n    if err != nil {\n        log.Fatal(\"encode error:\", err)\n    }\n    // Decode (receive) the value.\n    var q Q\n    err = dec.Decode(&amp;q)\n    if err != nil {\n        log.Fatal(\"decode error:\", err)\n    }\n    fmt.Printf(\"%q: {%d,%d}\\n\", q.Name, *q.X, *q.Y)\n}\n    You can compile and run this example code in the Go Playground.\n    The rpc package builds on gobs to turn this encode/decode automation into transport for method calls across the network. That\'s a subject for another article.\n  Details\n    The gob package documentation, especially the file doc.go, expands on many of the details described here and includes a full worked example showing how the encoding represents data. If you are interested in the innards of the gob implementation, that\'s a good place to start.\n		\n			By Rob Pike\n		\n	','https://blog.golang.org/gobs-of-data','2011-03-24','2017-05-07 13:50:03.940464'),(572,'Introducing Gofix','Russ Cox','		Introducing Gofix\n		15 April 2011\n		\n    The next Go release will include significant API changes in several fundamental Go packages. Code that implements an HTTP server handler, calls net.Dial, calls os.Open, or uses the reflect package will not build unless it is updated to use the new APIs. Now that our releases are more stable and less frequent, this will be a common situation. Each of these API changes happened in a different weekly snapshot and might have been manageable on its own; together, however, they represent a significant amount of manual effort to update existing code.\n    Gofix is a new tool that reduces the amount of effort it takes to update existing code. It reads a program from a source file, looks for uses of old APIs, rewrites them to use the current API, and writes the program back to the file. Not all API changes preserve all the functionality of an old API, so gofix cannot always do a perfect job. When gofix cannot rewrite a use of an old API, it prints a warning giving the file name and line number of the use, so that a developer can examine and rewrite the code. Gofix takes care of the easy, repetitive, tedious changes, so that a developer can focus on the ones that truly merit attention.\n    Each time we make a significant API change we’ll add code to gofix to take care of the conversion, as much as mechanically possible.  When you update to a new Go release and your code no longer builds, just run gofix on your source directory.\n    You can extend gofix to support changes to your own APIs.  The gofix program is a simple driver around plugins called fixes that each handle a particular API change.  Right now, writing a new fix requires doing some scanning and rewriting of the go/ast syntax tree, usually in proportion to how complex the API changes are.  If you want to explore, the netdialFix, osopenFix, httpserverFix, and reflectFix are all illustrative examples, in increasing order of complexity.\n    We write Go code too, of course, and our code is just as affected by these API changes as yours. Typically, we write the gofix support at the same time as the API change and then use gofix to rewrite the uses in the main source tree. We use gofix to update other Go code bases and our personal projects. We even use gofix to update Google’s internal source tree when it is time to build against a new Go release.\n    As an example, gofix can rewrite code like this snippet from fmt/print.go:\n  switch f := value.(type) {\ncase *reflect.BoolValue:\n    p.fmtBool(f.Get(), verb, field)\ncase *reflect.IntValue:\n    p.fmtInt64(f.Get(), verb, field)\n// ...\ncase reflect.ArrayOrSliceValue:\n    // Byte slices are special.\n    if f.Type().(reflect.ArrayOrSliceType).Elem().Kind() == reflect.Uint8 {\n        // ...\n    }   \n// ...\n}\n    to adapt it to the new reflect API:\n  switch f := value; f.Kind() {\ncase reflect.Bool:\n    p.fmtBool(f.Bool(), verb, field)\ncase reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64:\n    p.fmtInt64(f.Int(), verb, field)\n// ...\ncase reflect.Array, reflect.Slice:\n    // Byte slices are special.\n    if f.Type().Elem().Kind() == reflect.Uint8 {\n        // ...\n    }   \n// ...\n}\n    Nearly every line above changed in some small way. The changes involved in the rewrite are extensive but nearly entirely mechanical, just the kind of thing that computers are great at doing.  \n    Gofix is possible because Go has support in its standard libraries for parsing Go source files into syntax trees and also for printing those syntax trees back to Go source code. Importantly, the Go printing library prints a program in the official format (typically enforced via the gofmt tool), allowing gofix to make mechanical changes to Go programs without causing spurious formatting changes. In fact, one of the key motivations for creating gofmt—perhaps second only to avoiding debates about where a particular brace belongs—was to simplify the creation of tools that rewrite Go programs, as gofix does.\n    Gofix has already made itself indispensable. In particular, the recent reflect changes would have been unpalatable without automated conversion, and the reflect API badly needed to be redone. Gofix gives us the ability to fix mistakes or completely rethink package APIs without worrying about the cost of converting existing code. We hope you find gofix as useful and convenient as we have.\n		\n			By Russ Cox\n		\n	','https://blog.golang.org/introducing-gofix','2011-04-15','2017-05-07 13:50:03.981951'),(573,'Two Go Talks: \"Lexical Scanning in Go\" and \"Cuddle: an App Engine Demo\"','Andrew Gerrand','		Two Go Talks: \"Lexical Scanning in Go\" and \"Cuddle: an App Engine Demo\"\n		1 September 2011\n		\n    On Tuesday night Rob Pike and Andrew Gerrand each presented at the Sydney Google Technology User Group.\n    Rob\'s talk, \"Lexical Scanning in Go\", discusses the design of  a particularly interesting and idiomatic piece of Go code, the lexer component of the new template package.\n    The slides are available here. The new template package is available as exp/template in Go release r59. In a future release it will replace the old template package.\n    Andrew\'s talk, \"Cuddle: an App Engine Demo\", describes the construction of a simple real-time chat application that uses App Engine\'s Datastore, Channel, and Memcache APIs. It also includes a question and answer session that covers Go for App Engine and Go more generally.\n    The slides are available here. The code is available at the cuddle Google Code project.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/two-go-talks-lexical-scanning-in-go-and','2011-09-01','2017-05-07 13:50:04.019755'),(574,'Go and Google App Engine','David Symonds, Nigel Tao and Andrew Gerrand','		Go and Google App Engine\n		10 May 2011\n		\n    Google’s App Engine provides a reliable, scalable, easy way to build and deploy applications for the web. Over a hundred thousand apps are hosted at appspot.com and custom domains using the App Engine infrastructure. Originally written for Python apps, in 2009 the system added a Java runtime. And today, at Google I/O, we’re thrilled to announce that Go will be next. It’s marked as an experimental App Engine feature for now, because it’s early days, but both the App Engine and Go teams are very excited about this milestone.\n    By early days, we mean that it’s still rolling out. As of today, the App Engine SDK for Go is available for download, and we will soon enable deployment of Go apps into the App Engine hosting infrastructure. Today, through the SDK, you’ll be able to write web apps, learn about the APIs (and the language, if it’s new to you), and run your web app locally.  Once full deployment is enabled, it’ll be easy to push your app to Google’s cloud.\n    One of the cool but less obvious things about this news is that it provides a very easy way to play with Go. You don’t even need to have Go installed beforehand because the SDK is fully self-contained. Just download the SDK, unzip it, and start coding. Moreover, the SDK’s “dev app server” means you don’t even need to run the compiler yourself; everything is delightfully automatic.\n    What you’ll find in the SDK is many of the standard App Engine APIs, custom designed in good Go style, including Datastore, Blobstore, URL Fetch, Mail, Users, and so on. More APIs will be added as the environment develops. The runtime provides the full Go language and almost all the standard libraries, except for a few things that don’t make sense in the App Engine environment. For instance, there is no unsafe package and the syscall package is trimmed. (The implementation uses an expanded version of the setup in the Go Playground on golang.org.)\n    Also, although goroutines and channels are present, when a Go app runs on App Engine only one thread is run in a given instance.  That is, all goroutines run in a single operating system thread, so there is no CPU parallelism available for a given client request.  We expect this restriction will be lifted at some point.\n    Despite these minor restrictions, it’s the real language: Code is deployed in source form and compiled in the cloud using the 64-bit x86 compiler (6g), making it the first true compiled language that runs on App Engine. Go on App Engine makes it possible to deploy efficient, CPU-intensive web applications.\n    If you want to know more, read the documentation (start with “Getting Started”). The libraries and SDK are open source, hosted at http://code.google.com/p/appengine-go/.  We’ve created a new google-appengine-go mailing list; feel free to contact us there with App Engine-specific questions. The issue tracker for App Engine is the place for reporting issues related to the new Go SDK.\n    The Go App Engine SDK is available for Linux and Mac OS X (10.5 or greater); we hope a Windows version will also be available soon.\n    We’d like to offer our thanks for all the help and enthusiasm we received from Google’s App Engine team in making this happen.\n		\n			By David Symonds, Nigel Tao and Andrew Gerrand\n		\n	','https://blog.golang.org/go-and-google-app-engine','2011-05-10','2017-05-07 13:50:04.050498'),(575,'Go for App Engine is now generally available','Andrew Gerrand','		Go for App Engine is now generally available\n		21 July 2011\n		\n    The Go and App Engine teams are excited to announce that the Go runtime for App Engine is now generally available. This means you can take that Go app you\'ve been working on (or meaning to work on) and deploy it to App Engine right now with the new 1.5.2 SDK.\n    Since we announced the Go runtime at Google I/O we have continued to improve and extend Go support for the App Engine APIs and have added the Channels API. The Go Datastore API now supports transactions and ancestor queries, too. See the Go App Engine documentation for all the details.\n    For those who have been using the Go SDK already, please note that the 1.5.2 release introduces api_version 2. This is because the new SDK is based on Go release.r58.1 (the current stable version of Go) and is not backwards compatible with the previous release. Existing apps may require changes as per the r58 release notes. Once you\'ve updated your code, you should redeploy your app with the line api_version: 2 in its app.yaml file. Apps written against api_version 1 will stop working after the 18th of August.\n    Finally, we owe a huge thanks to our trusted testers and their many bug reports. Their help was invaluable in reaching this important milestone.\n    The fastest way to get started with Go on App Engine is with the Getting Started guide.\n    Note that the Go runtime is still considered experimental; it is not as well-supported as the Python and Java runtimes.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go-for-app-engine-is-now-generally','2011-07-21','2017-05-07 13:50:04.080971'),(576,'JSON-RPC: a tale of interfaces','Andrew Gerrand','		JSON-RPC: a tale of interfaces\n		27 April 2010\n		\n    Here we present an example where Go\'s interfaces made it easy to refactor some existing code to make it more flexible and extensible. Originally, the standard library\'s RPC package used a custom wire format called gob. For a particular application, we wanted to use JSON as an alternate wire format.\n    We first defined a pair of interfaces to describe the functionality of the existing wire format, one for the client, and one for the server (depicted below).\n  type ServerCodec interface {\n ReadRequestHeader(*Request) error\n ReadRequestBody(interface{}) error\n WriteResponse(*Response, interface{}) error\n Close() error\n}\n    On the server side, we then changed two internal function signatures to accept the ServerCodec interface instead of our existing gob.Encoder. Here\'s one of them:\n  func sendResponse(sending *sync.Mutex, req *Request,\n reply interface{}, enc *gob.Encoder, errmsg string)\n    became\n  func sendResponse(sending *sync.Mutex, req *Request,\n  reply interface{}, enc ServerCodec, errmsg string)\n    We then wrote a trivial gobServerCodec wrapper to reproduce the original functionality. From there it is simple to build a jsonServerCodec.\n    After some similar changes to the client side, this was the full extent of the work we needed to do on the RPC package. This whole exercise took about 20 minutes! After tidying up and testing the new code, the final changeset was submitted.\n    In an inheritance-oriented language like Java or C++, the obvious path would be to generalize the RPC class, and create JsonRPC and GobRPC subclasses. However, this approach becomes tricky if you want to make a further generalization orthogonal to that hierarchy. (For example, if you were to implement an alternate RPC standard). In our Go package, we took a route that is both conceptually simpler and requires less code be written or changed.\n    A vital quality for any codebase is maintainability. As needs change, it is essential to adapt your code easily and cleanly, lest it become unwieldy to work with. We believe Go\'s lightweight, composition-oriented type system provides a means of structuring code that scales.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/json-rpc-tale-of-interfaces','2010-04-27','2017-05-07 13:50:04.110366'),(577,'A GIF decoder: an exercise in Go interfaces','Rob Pike','		A GIF decoder: an exercise in Go interfaces\n		25 May 2011\n		\n  Introduction\n    At the Google I/O conference in San Francisco on May 10, 2011, we announced that the Go language is now available on Google App Engine.  Go is the first language to be made available on App Engine that compiles directly to machine code, which makes it a good choice for CPU-intensive tasks such as image manipulation.\n    In that vein, we demonstrated a program called Moustachio that makes it easy to improve a picture such as this one:\n    by adding a moustache and sharing the result:\n    All the graphical processing, including rendering the antialiased moustache, is done by a Go program running on App Engine. (The source is available at the appengine-go project.)\n    Although most images on the web—at least those likely to be moustachioed—are JPEGs, there are countless other formats floating around, and it seemed reasonable for Moustachio to accept uploaded images in a few of them. JPEG and PNG decoders already existed in the Go image library, but the venerable GIF format was not represented, so we decided to write a GIF decoder in time for the announcement. That decoder contains a few pieces that demonstrate how Go\'s interfaces make some problems easier to solve. The rest of this blog post describes a couple of instances.\n  The GIF format\n    First, a quick tour of the GIF format.  A GIF image file is paletted, that is, each pixel value is an index into a fixed color map that is included in the file. The GIF format dates from a time when there were usually no more than 8 bits per pixel on the display, and a color map was used to convert the limited set of values into the RGB (red, green, blue) triples needed to light the screen. (This is in contrast to a JPEG, for example, which has no color map because the encoding represents the distinct color signals separately.)\n    A GIF image can contain anywhere from 1 to 8 bits per pixel, inclusive, but 8 bits per pixel is the most common.\n    Simplifying somewhat, a GIF file contains a header defining the pixel depth and image dimensions, a color map (256 RGB triples for an 8-bit image), and then the pixel data.  The pixel data is stored as a one-dimensional bit stream, compressed using the LZW algorithm, which is quite effective for computer-generated graphics although not so good for photographic imagery. The compressed data is then broken into length-delimited blocks with a one-byte count (0-255) followed by that many bytes:\n  Deblocking the pixel data\n    To decode GIF pixel data in Go, we can use the LZW decompressor from the compress/lzw package. It has a NewReader function that returns an object that, as the documentation says, \"satisfies reads by decompressing the data read from r\":\n  func NewReader(r io.Reader, order Order, litWidth int) io.ReadCloser\n    Here order defines the bit-packing order and litWidth is the word size in bits, which for a GIF file corresponds to the pixel depth, typically 8.\n    But we can\'t just give NewReader the input file as its first argument because the decompressor needs a stream of bytes but the GIF data is a stream of blocks that must be unpacked. To address this problem, we can wrap the input io.Reader with some code to deblock it, and make that code again implement Reader. In other words, we put the deblocking code into the Read method of a new type, which we call blockReader.\n    Here\'s the data structure for a blockReader.\n  type blockReader struct {\n   r     reader    // Input source; implements io.Reader and io.ByteReader.\n   slice []byte    // Buffer of unread data.\n   tmp   [256]byte // Storage for slice.\n}\n    The reader, r, will be the source of the image data, perhaps a file or HTTP connection.  The slice and tmp fields will be used to manage the deblocking. Here\'s the Read method in its entirety. It\'s a nice example of the use of slices and arrays in Go.\n  1  func (b *blockReader) Read(p []byte) (int, os.Error) {\n2      if len(p) == 0 {\n3          return 0, nil\n4      }\n5      if len(b.slice) == 0 {\n6          blockLen, err := b.r.ReadByte()\n7          if err != nil {\n8              return 0, err\n9          }\n10          if blockLen == 0 {\n11              return 0, os.EOF\n12          }\n13          b.slice = b.tmp[0:blockLen]\n14          if _, err = io.ReadFull(b.r, b.slice); err != nil {\n15              return 0, err\n16          }\n17      }\n18      n := copy(p, b.slice)\n19      b.slice = b.slice[n:]\n20      return n, nil\n21  }\n    Lines 2-4 are just a sanity check: if there\'s no place to put data, return zero.  That should never happen, but it\'s good to be safe.\n    Line 5 asks if there\'s data left over from a previous call by checking the length of b.slice.  If there isn\'t, the slice will have length zero and we need to read the next block from r.\n    A GIF block starts with a byte count, read on line 6.  If the count is zero, GIF defines this to be a terminating block, so we return EOF on line 11.\n    Now we know we should read blockLen bytes, so we point b.slice to the first blockLen bytes of b.tmp and then use the helper function io.ReadFull to read that many bytes.  That function will return an error if it can\'t read exactly that many bytes, which should never happen.  Otherwise we have blockLen bytes ready to read.\n    Lines 18-19 copy the data from b.slice to the caller\'s buffer. We are implementing Read, not ReadFull, so we are allowed to return fewer than the requested number of bytes.  That makes it easy: we just copy the data from b.slice to the caller\'s buffer (p), and the return value from copy is the number of bytes transferred.  Then we reslice b.slice to drop the first n bytes, ready for the next call.\n    It\'s a nice technique in Go programming to couple a slice (b.slice) to an array (b.tmp).  In this case, it means blockReader type\'s Read method never does any allocations. It also means we don\'t need to keep a count around (it\'s implicit in the slice length), and the built-in copy function guarantees we never copy more than we should. (For more about slices, see this post from the Go Blog.)\n    Given the blockReader type, we can unblock the image data stream just by wrapping the input reader, say a file, like this:\n  deblockingReader := &amp;blockReader{r: imageFile}\n    This wrapping turns a block-delimited GIF image stream into a simple stream of bytes accessible by calls to the Read method of the blockReader.\n  Connecting the pieces\n    With blockReader implemented and the LZW compressor available from the library, we have all the pieces we need to decode the image data stream.  We stitch them together with this thunderclap, straight from the code:\n  lzwr := lzw.NewReader(&amp;blockReader{r: d.r}, lzw.LSB, int(litWidth))\nif _, err = io.ReadFull(lzwr, m.Pix); err != nil {\n   break\n}\n    That\'s it.\n    The first line creates a blockReader and passes it to lzw.NewReader to create a decompressor.  Here d.r is the io.Reader holding the image data, lzw.LSB defines the byte order in the LZW decompressor, and litWidth is the pixel depth.\n    Given the decompressor, the second line calls io.ReadFull to decompress the data and store it in the image, m.Pix. When ReadFull returns, the image data is decompressed and stored in the image, m, ready to be displayed.\n    This code worked first time. Really.\n    We could avoid the temporary variable lzwr by placing the NewReader call into the argument list for ReadFull, just as we built the blockReader inside the call to NewReader, but that might be packing too much into a single line of code.\n  Conclusion\n    Go\'s interfaces make it easy to construct software by assembling piece parts like this to restructure data.  In this example, we implemented GIF decoding by chaining together a deblocker and a decompressor using the io.Reader interface, analogous to a type-safe Unix pipeline. Also, we wrote the deblocker as an (implicit) implementation of a Reader interface, which then required no extra declaration or boilerplate to fit it into the processing pipeline. It\'s hard to implement this decoder so compactly yet cleanly and safely in most languages, but the interface mechanism plus a few conventions make it almost natural in Go.\n    That deserves another picture, a GIF this time:\n    The GIF format is defined at http://www.w3.org/Graphics/GIF/spec-gif89a.txt.\n		\n			By Rob Pike\n		\n	','https://blog.golang.org/gif-decoder-exercise-in-go-interfaces','2011-05-25','2017-05-07 13:50:04.154021'),(578,'Go: What\'s New in March 2010','Andrew Gerrand','		Go: What\'s New in March 2010\n		18 March 2010\n		\n    Welcome to the official Go Blog. We, the Go team, hope to use this blog to keep the world up-to-date on the development of the Go programming language and the growing ecosystem of libraries and applications surrounding it.\n    It\'s been a few months since we launched (November last year), so let\'s talk about what\'s been happening in Go World since then.\n    The core team at Google has continued to develop the language, compilers, packages, tools, and documentation.  The compilers now produce code that is in some cases between 2x and an order of magnitude faster than at release. We have put together some graphs of a selection of Benchmarks, and the the Build Status page tracks the reliability of each changeset submitted to the repository.\n    We have made syntax changes to make the language more concise, regular, and flexible. Semicolons have been almost entirely removed from the language.  The ...T syntax makes it simpler to handle an arbitrary number of typed function parameters.  The syntax x[lo:] is now shorthand for x[lo:len(x)]. Go also now natively supports complex numbers. See the release notes for more. \n    Godoc now provides better support for third-party libraries, and a new tool - goinstall - has been released to make it easy to install them. Additionally, we\'ve started working on a package tracking system to make it easier to find what you need. You can view the beginnings of this on the Packages page.\n    More than 40,000 lines of code have been added to the standard library, including many entirely new packages, a sizable portion written by external contributors.\n    Speaking of third parties, since launch a vibrant community has flourished on our mailing list and irc channel (#go-nuts on freenode). We have officially added more than 50 people to the project. Their contributions range from bug fixes and documentation corrections to core packages and support for additional operating systems (Go is now supported under FreeBSD, and a Windows port is underway). We regard these community contributions our greatest success so far.\n    We\'ve received some good reviews, too.  This recent article in PC World summarized the enthusiasm surrounding the project.  Several bloggers have begun documenting their experiences in the language (see here, here, and here for example)  The general reaction of our users has been very positive; one first-timer remarked \"I came away extremely impressed. Go walks an elegant line between simplicity and power.\"\n    As to the future: we have listened to the myriad voices telling us what they need, and are now focused on getting Go ready for the prime time. We are improving the garbage collector, runtime scheduler, tools, and standard libraries, as well as exploring new language features. 2010 will be an exciting year for Go, and we look forward to collaborating with the community to make it a successful one.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/go-whats-new-in-march-2010','2010-03-18','2017-05-07 13:50:04.194894'),(579,'New Talk and Tutorials','Andrew Gerrand','		New Talk and Tutorials\n		5 May 2010\n		\n    Rob Pike recently gave a talk at Stanford\'s Computer Systems Colloquium (EE380). Titled Another Go at Language Design, the presentation gives an overview of the itches Go was built to scratch, and how Go addresses those problems. You can view a video stream of the talk, and download the slides.\n    Last week\'s release included a code lab, Writing Web Applications, that details the construction of a simple wiki program. It is a practical introduction to some fundamental Go concepts, and the first of a series of Go code labs.\n    Lastly, we are often asked \"How do Go packages work?\" It\'s easier to show than to explain, so I put together a Go Packages screen cast that demonstrates the process of writing, building, installing, and redistributing Go packages. I hope to post more of these covering a variety of Go programming topics to the gocoding YouTube channel in the near future.\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/new-talk-and-tutorials','2010-05-05','2017-05-07 13:50:04.217250'),(580,'Upcoming Google I/O Go Events','Andrew Gerrand','		Upcoming Google I/O Go Events\n		12 May 2010\n		\n    Google I/O 2010 is happening next week at the Moscone Centre in San Francisco. Those of you with tickets will be able to catch some of the Go team both at I/O and at Bootcamp. In reverse-chronological order:\n    Rob Pike and Russ Cox will be presenting a Go Programming talk on Thursday at 10.15am. This session takes a detailed look at how Go differs from other languages in a practical sense. Through a series of examples, they will demonstrate various features of Go and the ways in which they affect program design.\n    Several members of the Go team will be at the Go cube during Office Hours on Wednesday between 12pm and 2:30pm. Come by to have your Go questions answered by the experts.\n    At Bootcamp on Tuesday at 4.15pm, Andrew Gerrand will be giving an introductory talk about Go. The session will give an overview of the problems that motivated us to build a new language, and the ways in which Go addresses those problems.\n    If you\'re coming to I/O, we look forward to seeing you there!\n		\n			By Andrew Gerrand\n		\n	','https://blog.golang.org/upcoming-google-io-go-events','2010-05-12','2017-05-07 13:50:04.232449');
/*!40000 ALTER TABLE `SpiderDoc_golangblog` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2017-05-08  3:41:18
